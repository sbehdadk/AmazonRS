{
 "cells": [
  {
   "source": [
    "# Few short learning with Siamese Networks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device mapping:\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8) #0.333\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True, gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x7fd014095910>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from tensorflow.compat.v1.keras import backend as K\n",
    "import tensorflow as tf\n",
    "K.clear_session()\n",
    "tf.compat.v1.reset_default_graph()\n",
    "K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras and other Deep Learning dependencies\n",
    "from keras.models import Sequential\n",
    "import time\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate, GlobalMaxPool2D\n",
    "from keras.models import Model\n",
    "import seaborn as sns\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D, GlobalMaxPool3D, GlobalMaxPool2D, GlobalMaxPooling2D, GlobalMaxPooling3D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.optimizers import *\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "K.set_image_data_format('channels_last')\n",
    "import cv2\n",
    "import os\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#from fr_utils import *\n",
    "#from inception_blocks_v2 import *\n",
    "import numpy.random as rng\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 7245686603538246416\n, name: \"/device:GPU:0\"\ndevice_type: \"GPU\"\nmemory_limit: 9370284851\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 17157890842422095920\nphysical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n]\n"
     ]
    }
   ],
   "source": [
    "# Check whether GPU is being or not\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  1\n2.4.0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'3.8.5 (default, Sep  4 2020, 07:30:14) \\n[GCC 7.3.0]'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "#check the GPU again\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.test.is_built_with_cuda()\n",
    "print(tf.version.VERSION)\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"data_path = os.path.join('/home/sina/Desktop')\\ntrain_folder = os.path.join(data_path,'images_background')\\nvalpath = os.path.join(data_path,'images_evaluation')\""
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "'''data_path = os.path.join('/home/sina/Desktop')\n",
    "train_folder = os.path.join(data_path,'images_background')\n",
    "valpath = os.path.join(data_path,'images_evaluation')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('/home/sina/Desktop/omniglot')\n",
    "train_folder = os.path.join(data_path,'images_background')\n",
    "valpath = os.path.join(data_path,'images_evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_class_name = 'character'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'img = cv2.imread(\\'/home/sina/Desktop/images_evaluation/TelephoneAccessories/HandsetCords/HandsetCords_B0002B0IB8.png\\')\\nprint(\"Each image in the data set has a same of {0}\".format(img.shape))\\nflattened_img = img.flatten()\\n\\nprint(\"The number of features in any image from the data set are: {0}\".format(flattened_img.shape[0]))'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "'''img = cv2.imread('/home/sina/Desktop/images_evaluation/TelephoneAccessories/HandsetCords/HandsetCords_B0002B0IB8.png')\n",
    "print(\"Each image in the data set has a same of {0}\".format(img.shape))\n",
    "flattened_img = img.flatten()\n",
    "\n",
    "print(\"The number of features in any image from the data set are: {0}\".format(flattened_img.shape[0]))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_class_names(base_class_name):\n",
    "    classes = []\n",
    "    for i in range(1,21):\n",
    "        if i < 10:\n",
    "            classes.append(\"{0}0{1}\".format(base_class_name, i))\n",
    "        else:\n",
    "            classes.append(\"{0}{1}\".format(base_class_name, i))\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = gen_class_names(base_class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_hot_encoding(classes):\n",
    "    encoder = LabelBinarizer()\n",
    "    transfomed_labels = encoder.fit_transform(classes)\n",
    "    return transfomed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = generate_one_hot_encoding(classes)"
   ]
  },
  {
   "source": [
    "# Siamese Networks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shape, dtype=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Plot of weights initialized, with mean of 0.0 and standard deviation of 0.01')"
      ]
     },
     "metadata": {},
     "execution_count": 15
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"263.63625pt\" version=\"1.1\" viewBox=\"0 0 462.51 263.63625\" width=\"462.51pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-01-24T03:35:15.193922</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 263.63625 \nL 462.51 263.63625 \nL 462.51 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 63.855 239.758125 \nL 398.655 239.758125 \nL 398.655 22.318125 \nL 63.855 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 106.325851 239.758125 \nL 115.935786 239.758125 \nL 115.935786 237.988162 \nL 106.325851 237.988162 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 115.935786 239.758125 \nL 125.54572 239.758125 \nL 125.54572 239.758125 \nL 115.935786 239.758125 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 125.54572 239.758125 \nL 135.155655 239.758125 \nL 135.155655 239.758125 \nL 125.54572 239.758125 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 135.155655 239.758125 \nL 144.765589 239.758125 \nL 144.765589 236.218198 \nL 135.155655 236.218198 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 144.765589 239.758125 \nL 154.375524 239.758125 \nL 154.375524 234.448235 \nL 144.765589 234.448235 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 154.375524 239.758125 \nL 163.985458 239.758125 \nL 163.985458 216.748601 \nL 154.375524 216.748601 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 163.985458 239.758125 \nL 173.595393 239.758125 \nL 173.595393 199.048967 \nL 163.985458 199.048967 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 173.595393 239.758125 \nL 183.205327 239.758125 \nL 183.205327 202.588894 \nL 173.595393 202.588894 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 183.205327 239.758125 \nL 192.815262 239.758125 \nL 192.815262 145.950066 \nL 183.205327 145.950066 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 192.815262 239.758125 \nL 202.425196 239.758125 \nL 202.425196 156.569847 \nL 192.815262 156.569847 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 202.425196 239.758125 \nL 212.035131 239.758125 \nL 212.035131 108.780836 \nL 202.425196 108.780836 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 212.035131 239.758125 \nL 221.645065 239.758125 \nL 221.645065 110.550799 \nL 212.035131 110.550799 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 221.645065 239.758125 \nL 231.255 239.758125 \nL 231.255 60.991825 \nL 221.645065 60.991825 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 231.255 239.758125 \nL 240.864935 239.758125 \nL 240.864935 32.672411 \nL 231.255 32.672411 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 240.864935 239.758125 \nL 250.474869 239.758125 \nL 250.474869 64.531751 \nL 240.864935 64.531751 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 250.474869 239.758125 \nL 260.084804 239.758125 \nL 260.084804 59.221861 \nL 250.474869 59.221861 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 260.084804 239.758125 \nL 269.694738 239.758125 \nL 269.694738 99.931019 \nL 260.084804 99.931019 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 269.694738 239.758125 \nL 279.304673 239.758125 \nL 279.304673 114.090726 \nL 269.694738 114.090726 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 279.304673 239.758125 \nL 288.914607 239.758125 \nL 288.914607 160.109773 \nL 279.304673 160.109773 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 288.914607 239.758125 \nL 298.524542 239.758125 \nL 298.524542 188.429187 \nL 288.914607 188.429187 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 298.524542 239.758125 \nL 308.134476 239.758125 \nL 308.134476 197.279004 \nL 298.524542 197.279004 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 308.134476 239.758125 \nL 317.744411 239.758125 \nL 317.744411 227.368381 \nL 308.134476 227.368381 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 317.744411 239.758125 \nL 327.354345 239.758125 \nL 327.354345 222.058491 \nL 317.744411 222.058491 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 327.354345 239.758125 \nL 336.96428 239.758125 \nL 336.96428 236.218198 \nL 327.354345 236.218198 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 336.96428 239.758125 \nL 346.574214 239.758125 \nL 346.574214 236.218198 \nL 336.96428 236.218198 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 346.574214 239.758125 \nL 356.184149 239.758125 \nL 356.184149 236.218198 \nL 346.574214 236.218198 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"me0b7d7dfc9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"90.977425\" xlink:href=\"#me0b7d7dfc9\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −0.04 -->\n      <g transform=\"translate(75.654769 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"164.549282\" xlink:href=\"#me0b7d7dfc9\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −0.02 -->\n      <g transform=\"translate(149.226625 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"242.822266\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"238.121138\" xlink:href=\"#me0b7d7dfc9\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.00 -->\n      <g transform=\"translate(226.988326 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"311.692995\" xlink:href=\"#me0b7d7dfc9\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.02 -->\n      <g transform=\"translate(300.560182 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"385.264851\" xlink:href=\"#me0b7d7dfc9\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.04 -->\n      <g transform=\"translate(374.132039 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m0a1c8ecf85\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"63.855\" xlink:href=\"#m0a1c8ecf85\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0 -->\n      <g transform=\"translate(50.4925 243.557344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"63.855\" xlink:href=\"#m0a1c8ecf85\" y=\"193.519705\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 10 -->\n      <g transform=\"translate(44.13 197.318924)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"63.855\" xlink:href=\"#m0a1c8ecf85\" y=\"147.281285\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 20 -->\n      <g transform=\"translate(44.13 151.080503)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"63.855\" xlink:href=\"#m0a1c8ecf85\" y=\"101.042864\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 30 -->\n      <g transform=\"translate(44.13 104.842083)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"63.855\" xlink:href=\"#m0a1c8ecf85\" y=\"54.804444\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 40 -->\n      <g transform=\"translate(44.13 58.603663)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_11\">\n     <!-- Density -->\n     <g transform=\"translate(38.050312 150.046719)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 19.671875 64.796875 \nL 19.671875 8.109375 \nL 31.59375 8.109375 \nQ 46.6875 8.109375 53.6875 14.9375 \nQ 60.6875 21.78125 60.6875 36.53125 \nQ 60.6875 51.171875 53.6875 57.984375 \nQ 46.6875 64.796875 31.59375 64.796875 \nz\nM 9.8125 72.90625 \nL 30.078125 72.90625 \nQ 51.265625 72.90625 61.171875 64.09375 \nQ 71.09375 55.28125 71.09375 36.53125 \nQ 71.09375 17.671875 61.125 8.828125 \nQ 51.171875 0 30.078125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-68\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"77.001953\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"138.525391\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"201.904297\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"254.003906\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"281.787109\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"320.996094\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_11\">\n    <path clip-path=\"url(#p09bb6809f8)\" d=\"M 79.073182 239.749827 \nL 89.77944 239.615927 \nL 98.956233 239.220357 \nL 105.074095 239.015191 \nL 111.191958 239.087197 \nL 120.368751 239.291198 \nL 124.957147 239.11073 \nL 129.545544 238.646062 \nL 132.604475 238.165254 \nL 135.663406 237.510449 \nL 138.722337 236.603179 \nL 141.781268 235.322528 \nL 143.310733 234.498942 \nL 144.840199 233.532143 \nL 146.369664 232.410558 \nL 147.89913 231.127829 \nL 149.428595 229.683918 \nL 152.487526 226.34652 \nL 155.546457 222.527564 \nL 160.134854 216.319016 \nL 167.782181 205.651807 \nL 170.841112 200.972381 \nL 173.900043 195.610659 \nL 176.958974 189.337792 \nL 180.017905 182.203231 \nL 187.665233 163.357867 \nL 190.724164 156.476354 \nL 198.371492 139.892968 \nL 201.430423 132.643755 \nL 209.07775 113.955995 \nL 216.725078 96.064402 \nL 219.784009 88.207239 \nL 225.901871 71.628724 \nL 227.431336 67.8402 \nL 228.960802 64.421106 \nL 230.490267 61.462398 \nL 232.019733 59.025045 \nL 233.549198 57.135801 \nL 235.078664 55.788261 \nL 236.608129 54.948639 \nL 238.137595 54.564935 \nL 239.66706 54.577694 \nL 241.196526 54.930444 \nL 242.725991 55.578105 \nL 244.255457 56.492159 \nL 245.784922 57.662001 \nL 247.314388 59.092593 \nL 248.843853 60.799185 \nL 250.373319 62.800291 \nL 251.902784 65.110375 \nL 253.43225 67.733657 \nL 254.961715 70.660177 \nL 258.020646 77.309318 \nL 261.079577 84.729097 \nL 265.667974 96.557388 \nL 270.25637 108.886675 \nL 273.315302 117.631582 \nL 276.374233 127.043451 \nL 279.433164 137.148923 \nL 288.609957 168.301715 \nL 291.668888 177.437142 \nL 294.727819 185.521516 \nL 297.78675 192.634935 \nL 300.845681 198.966118 \nL 303.904612 204.66306 \nL 306.963543 209.74592 \nL 310.022474 214.132869 \nL 311.551939 216.039297 \nL 313.081405 217.753156 \nL 314.61087 219.286109 \nL 317.669801 221.905738 \nL 320.728732 224.151764 \nL 331.434991 231.552958 \nL 334.493922 233.330252 \nL 337.552853 234.682802 \nL 340.611784 235.578059 \nL 343.670715 236.106052 \nL 349.788577 236.711829 \nL 354.376974 237.267952 \nL 368.142163 239.249997 \nL 372.73056 239.556978 \nL 378.848422 239.716568 \nL 383.436818 239.748381 \nL 383.436818 239.748381 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path d=\"M 63.855 239.758125 \nL 63.855 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path d=\"M 398.655 239.758125 \nL 398.655 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path d=\"M 63.855 239.758125 \nL 398.655 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_32\">\n    <path d=\"M 63.855 22.318125 \nL 398.655 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_12\">\n    <!-- Plot of weights initialized, with mean of 0.0 and standard deviation of 0.01 -->\n    <g transform=\"translate(7.2 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      <path d=\"M 4.203125 54.6875 \nL 13.1875 54.6875 \nL 24.421875 12.015625 \nL 35.59375 54.6875 \nL 46.1875 54.6875 \nL 57.421875 12.015625 \nL 68.609375 54.6875 \nL 77.59375 54.6875 \nL 63.28125 0 \nL 52.6875 0 \nL 40.921875 44.828125 \nL 29.109375 0 \nL 18.5 0 \nz\n\" id=\"DejaVuSans-119\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 5.515625 54.6875 \nL 48.1875 54.6875 \nL 48.1875 46.484375 \nL 14.40625 7.171875 \nL 48.1875 7.171875 \nL 48.1875 0 \nL 4.296875 0 \nL 4.296875 8.203125 \nL 38.09375 47.515625 \nL 5.515625 47.515625 \nz\n\" id=\"DejaVuSans-122\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 11.71875 12.40625 \nL 22.015625 12.40625 \nL 22.015625 4 \nL 14.015625 -11.625 \nL 7.71875 -11.625 \nL 11.71875 4 \nz\n\" id=\"DejaVuSans-44\"/>\n      <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-80\"/>\n     <use x=\"60.302734\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"88.085938\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"149.267578\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"188.476562\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"220.263672\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"281.445312\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"316.650391\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"348.4375\" xlink:href=\"#DejaVuSans-119\"/>\n     <use x=\"430.224609\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"491.748047\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"519.53125\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"583.007812\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"646.386719\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"685.595703\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"737.695312\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"769.482422\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"797.265625\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"860.644531\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"888.427734\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"927.636719\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"955.419922\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1016.699219\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"1044.482422\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1072.265625\" xlink:href=\"#DejaVuSans-122\"/>\n     <use x=\"1124.755859\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1186.279297\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"1249.755859\" xlink:href=\"#DejaVuSans-44\"/>\n     <use x=\"1281.542969\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1313.330078\" xlink:href=\"#DejaVuSans-119\"/>\n     <use x=\"1395.117188\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1422.900391\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1462.109375\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"1525.488281\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1557.275391\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"1654.6875\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1716.210938\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1777.490234\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1840.869141\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1872.65625\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1933.837891\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"1969.042969\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2000.830078\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"2064.453125\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"2096.240234\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"2159.863281\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2191.650391\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2252.929688\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"2316.308594\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"2379.785156\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2411.572266\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2463.671875\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"2502.880859\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2564.160156\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"2627.539062\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"2691.015625\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2752.294922\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"2791.658203\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"2855.134766\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2886.921875\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"2950.398438\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"3011.921875\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"3071.101562\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"3098.884766\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"3160.164062\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"3199.373047\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"3227.15625\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"3288.337891\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"3351.716797\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"3383.503906\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"3444.685547\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"3479.890625\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"3511.677734\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"3575.300781\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"3607.087891\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"3670.710938\" xlink:href=\"#DejaVuSans-49\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p09bb6809f8\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"63.855\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdAAAAEICAYAAADvMKVCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAA4iUlEQVR4nO3dd3gc5bX48e9Z9V6sYsuSLNu4YAPumI6BECCEcgMBQglJSEgguTch4Sak3JtGEpJ7kxvSIeVHB9N7AsRg04x7792Wm2QVq7fd8/tjRmQRkrValdldnc/z6NGWKWfenZkz7zvvzIiqYowxxpi+8XkdgDHGGBONLIEaY4wxYbAEaowxxoTBEqgxxhgTBkugxhhjTBgsgRpjjDFh6FcCFZGFIvL5gQqml3ndIiKHRaRBREYM0jw2iMi8EIfdLSIfGYQYzhSRLQMxrIiUuuUVF8K05olIedD7kMsiVCJyn4jcOcDT/LuI3DiU84w0IlIoIm+KSL2I/NLreMIVqb/VQG/rIqIiclyIw/5ARB4agHn+SUT+qx/jN4jIuP7G0cd5pojICyJyVESeGMp5h6rXBOquPM1uAR52V/L0vsxERMrclSY+nCBFJAH4FfBRVU1X1apwptMbVZ2qqgv7O52uyaiPMbylqpPCGbbrhq6qe93y8ocRx4CUxWBT1YtU9X4AEfmMiLztdUweuBk4AmSq6je6fimOn4tIlfv3cxGRniYmIteKyB4RaRSRZ0UkdzCD76++JKThSlW/pKo/DmXY7ipG7n5k5+BE16MrgUJghKp+srsBROQ2ETkkInUi8jcRSeppYiJynohsFpEmEXlDRMYEfXeViLzrfrcw1ABDrYFeoqrpwExgNvC9UGcwQAqBZGDDEM/XmGgwBtioPd8V5WbgcmAacBJwCfDF7gYUkanAPcANONtdE/CHAY53WAi3wmDeNwbYqqod3X0pIhcAdwDnucOOA37Yw7B5wNPAfwG5wHJgftAg1cCvgbv6FKGqHvMP2A18JOj9/wAvuq8XAp93X/twEuseoAJ4AMhyv9sLKNDg/p3azXyS3AU44P792v1sItAYNP7r3Yx7P/AN9/Vod9gvu+/Hu4Xjc99/HFgN1ALvAid1t6xAijvdGmAT8E2gvMuwtwNrgaM4P0YykAY0A4Gg5S0CTsb50eqAw8CveijveaHMp+uwwIPuPJvdeX4TKHPLIt4d5rPustQDO4Ev9jLfzrKoDVqWzt+iLITynAGsdOc3H3gMuDOEdW6sO73O3+zPQEXQ9w8CXwteB4HjgRbA78ZZ635/H/B74CU3jiXA+B7m21lenwX2ub/9l4A5bvnXAr/rMs7n3DKtAV4BxgR9d7c7nTpgBXBm0Hc/AB7H2U7qcQ4OZx+jTE4DlrnrwDLgtKDlawfa3OX+SDfjvgvcHPT+JuC9HubzU+CRoPfj3Wln9DB82MvYl/UDOA5Y5C7/EWC++/mb7m/W6C7/1UAO8CJQ6f4uLwLFQdNaCPwYeMed96tAXtD3N+Dsx6qA7/LBbeFkYLG7LhwEfgckBo2rwJeBbcAu97P/dIc94K4vChx3jHV/kRvXa+70Hwr6/hT396wF1gDz3M+vBpZ3mdZtwPNB68md7useywf4Cc421OKW5++Clus493WW+5tWuuX0Pf61rX4GeBv4X3fau4CLjrFeH+/+HrXu+nGp+/kPcda7djeOm7oZ9xHgp0HvzwMO9TCfm4F3g9537qcndxnu88DC3vZR7w8fws4seOUpcRfyx8E7r6AdyXaco4B0nGz/YJcdU/wx5vMj4D2gAMh3V5IfhzK+O+8X3NfXAjv41wb2OeC5oA22ApgLxAE3usuX1M2y3oWzIucAxTg70K4JZilOcszF2Yl+yf1uXvCw7meLgRvc1+nAKT0sy7xw58OHD3Y+UG7AxTg7RAHOxqldzAxlWkGf/xRnp5VwrPIEEnE2rtvcYa/E2Rh6TaDufPYCs9zXW3AS/vFB383oZh38DPB2l+nch7MjPBmIBx4GHuthnp3l9Secg6GP4uxInsVZL0e7y3u2O/xlOOv88e60v8cHN9LrgRHud98ADvGvg58fuNP+mFt2P6PnpJaLszO6wZ3Wp9z3I7ruHHsY/ygwN+j9bKC+h2GfA77V5bOGzt+im+HDWsa+rh/AozjJzOf+NmcEffeBhOTGcwWQCmQATwDPBn2/EGcfMRHnQHkhcJf73RR3ec/CWY9/BXTwr/3CLJwkFu+uL5twD+aCYnnN/c1SgAtxDphPwNlpP9I13m72E79y530WTiJ9yP1uNM66/DG3HM533+e7y1oPTAia1jLgmq7rSIjl8/kucQUn0Afc9STDLYOtuAkOZxtsB77g/ua34Bw4SDfLmoCz/XzHXR/OdZdhUtD681B35eR+vwa4Ouh9nhvniG6GvRv4Y5fP1gNXdPmsTwk01CbcZ0WkFufIYhHOTrSr63BqVTtVtQH4NnBNH5oxrgN+pKoVqlqJcwRyQ4jjLgLOEBEfzkr3C+B097uz3e/BOQq5R1WXqKpfnXNnrTgbRFdX4Rzd1KhqOfCbbob5jaoeUNVq4AVg+jFibAeOE5E8VW1Q1fdCXLa+zqdHqvqSqu5QxyKcI+8zQx1fRK7GOUC5QlXbOXZ5noKzgfxaVdtV9UmcDTpUi4CzRWSk+/5J9/1YIBNn4wnVM6q6VJ2moIfpvfx+rKotqvoqTs3mUXe93A+8hXPgAE7t9Gequsmd9k+B6Z3nVlT1IVWtUtUOVf0lzk4x+Pz226r6sjrnqB/EaWLtzsXANlV90J3Wo8BmnKbYUKTjJNFOR4H0Hs6Ddh22c/iM7ibcj2Xs6/rRjtNMV+T+Nj2e63bjeUpVm1S1HqdWdXaXwf6fqm5V1WacWvJ09/MrcVrY3lTVVpwmv0DQtFeo6nvu8u7Gae7uOu2fqWq1O+2r3HmtV9VGnKTQLREpxWnt+C9VbVXVN3G2907XAy+75RlQ1ddwWrU+pqpNOEntU+60JgCTgefDLJ+eYowDrgG+rar1bhn8kg/uq/eo6p/d3/x+YBTO6YCuTsFZ3+5S1TZVfR2nNvypUGKh+/Uaul9X+7RehyrUBHq5qmar6hhVvdVdMboqwjmi7LQH5yitu4LrTnfjF4UyoqruwNnRTcdJCC8CB0RkEh9MoGOAb4hIbecfTq26u/kU4TRNddrXzTCHgl434fxIPbkJ54h3s4gsE5GP97ZcYc6nRyJykYi8JyLV7rJ/DOeoLZRxZ+A0J/2be4ADxy7PImC/uod1ruDftzeLcGrFZ+HUeBfi/JZnA2+paqDHMT+sr+V3OOh1czfvO8cfA9wdtOzVOLX70QAicruIbHJ7EdbiNH0Fl3fXuJJ7OODsum3gvh/dy3J0asA56OiUCTR0+W16GrZz+PruJtyPZezr+vFNnLJd6vYQ/1xPA4pIqojc43aEqsNZf7Llg73Re1onPrDdu0nv/U6LIjJRRF7s7LiCc9DUdRsK3ld03Y8caxmLgBp3nt0NPwb4ZJft7QycBAVO7bYz+VyLU6ts6jqTEMunJ3k4Bz5d99XB6+L7ZRs0/+62uSJgX5dtub/rNXS/rvZpvQ7VQF4HegDnB+5UitP0cRinWh3O+Af6MP9FOEePiW5NYRFOk2IOzjk6cFbkn7gHA51/qe4RfVcHcZpuO5X0IZYPLa+qblPVT+E0Bf4ceFJE0vowzbDm28ntnfYUzrmJQlXNBl7G2Skdk4gU4DRjfllVVwV9dazyPAiM7lLLKe3DsizCORia575+G6dVIfiAqKtQ1rOBtA/nPHLw8qeo6rsicibOTv8qIMct76OEUN7d6LptgFOW+0McfwMfrN1Oo+cOeR8YVpxLF5Jwmuk+oJ/L2Kf1Q1UPqeoXVLUIpwPUH47R8/YbOLXguaqaiXMQRh/ien9bF5FUnCbPTn/Eqf1PcKf9nW6mG7wefmB6HHsbOAjkdNkvBA+/D+e0WPD6lqaqnR1fXgPyRWQ6TiJ9pIf59FY+x9qOjvCv1oDgGENdF4MdAErclsNwptXden1Yu79Ko+t6nYZzOqtfHVMHMoE+CtwmImPFuczlpzjnITtwTjYHcM6PHmv874lIvttj6r+Bvlz/tAj4Cs7RFDg1lq/gNCF1XsbxZ+BLIjLX7dqfJiIXi0h31fjHgW+LSI6IjHanFarDwAgRyer8QESuF5F892ir1v24L7WoUOfbUxkn4uwIK4EOEbkI5xzfMbm1hSdxzkU83uXrY5XnYpwDqP8QkQQR+QTOecjgaav0cK2pqm7Dqe1dDyxS1c7OV1fQcwI9DBSLSGJvyzVA/oSzjkwFEJEsEensbp+Bs/yVQLyI/DcfPgIO1cvARHEuL4l3m9Kn4LS0hOIB4OsiMlpEinB2oPf1MOzDwCXiXGOchtM34Wm3qa+r/ixjr+tHMBH5pIh0HtDW4OzkO7efrut9Bs66UyvOJTjfDzEmcNb1j4vIGe569CM+uJ/MwOkw1SAik3HO8R3L48BnRGSKm4x7jEVV9+A0yf5QRBJF5Aw+2Ez/EM5vc4GIxIlIsjiXzBW747fjnM/8H5xzsK/1MKveyqfH/Yi7L30c+ImIZIhzuuLr9G1f3WkJTu3/m+46MA9neR8LcfwHgJvcss3G6YNwXw/DPgOcICJXiEgyTn5Zq6qbwWmadj+PB3xu2Sb0FsBAJtC/4ZzjeBOn51UL8O/wfjX+J8A7btNDd+cc78RZedYC63B65/XloupFOCtGZwJ9G+ckeed7VHU5zsnt3+FshNtxTnp350dAubss/8TZsFpDCcT9UR4FdrrLW4TTmWCDiDTgnNC+poem8P74Gc5BSK2I3N4lpnrgP3BW/hqcJp4PnR/pRjFOTfBr4lwL3PlXeqzyVNU24BPu+2qcXoJPd05UREpwmk/WHWPei4AqVd0X9F5w1o3uvI5zRHlIRI6EsGz9oqrP4LQmPOY2ha0HLnK/fgX4B07NbQ/O9tDdaYBQ5lOF09v5GzjNid8EPq6qoS7jPTjn0ta5Mb7kfga8f5H8me68NuCc230Yp8NUBnBrD9MNexl7Wz+6MQdY4m4/zwNf1X9dl/gD4H53vb8Kpwd/Ck5t6T03xpC4y/9lnNrbQZz1Ovia7ttxtp16nAPI+V2n0WV6f3fjeR1n+3i9lxCuxemUV42T2B4ImtY+nI5r38E5aNmH08M3eD/+CPAR4Ant4fIPei+fu4ErRaRGRLrr+/HvOKfMduLsZx/B2f/3ibsOXIKzzRzBuVzq051JLYTx/4HT3+UNnI6Fewg6GBCnqf86d9hKnIPvn+D8pnNxzuV2ugHnoOKPOPu7Zpzf95ik+9MgpisRuQUn6YV0st0cm4hcD0xV1W97HYsxxoTDEmgPRGQUTjPGYmACzlH771T1117GZYwxJjLYnTJ6lojTzNV5Uf9j2B1ZjDHGuKwGaowxxoTBHmdmjDHGhMGacIG8vDwtKyvzOgxjjIkaK1asOKKq+V7H4SVLoEBZWRnLly/3OgxjjIkaItKXO4vFJGvCNcYYY8JgCdQYY4wJgyVQY4wxJgyWQI0xxpgwWAI1xhhjwmAJ1BhjjAmDJVBjjDEmDJZAjTHGmDBYAjXGGGPCYHciMiaCPbJkb0jDXTu3dJAjMcZ0ZTVQY4wxJgyWQI0xxpgwWAI1xhhjwmAJ1BhjjAmDJVBjjDEmDJZAjTHGmDBYAjXGGGPCYAnUGGOMCYMlUGOMMSYMlkCNMcaYMFgCNcYYY8JgCdQYY4wJgyVQY4wxJgyWQI0xxpgwWAI1xhhjwmAJ1BhjjAmDJVBjjDEmDJZAjTHGmDBEfQIVkTgRWSUiL7rvx4rIEhHZLiLzRSTR6xiNMcbEnqhPoMBXgU1B738O/J+qHgfUADd5EpUxg6i5zU9VQytHm9tRVa/DMWZYivc6gP4QkWLgYuAnwNdFRIBzgWvdQe4HfgD80ZMAjRlAAVXW7KvlnR1HOFjbQmfaTEuMY8WeGm44dQwzSnM8jdGY4SSqEyjwa+CbQIb7fgRQq6od7vtyYHR3I4rIzcDNAKWlpYMbpTH91NDawUPv7WFvdRMjM5M5d3IBuWmJtHYE2FfdxKsbD/P0qv2cP6WQ//74FEpyU70O2ZiYF7UJVEQ+DlSo6goRmdfX8VX1XuBegNmzZ1sbmIlYNU1t/PXtXdQ1t3PFzGJmlGbjE3n/+1PGjeDS6UU8sHg3v12wnYvufov/ufIkLjpxlIdRGxP7ovkc6OnApSKyG3gMp+n2biBbRDoPDIqB/d6EZ0z/tXUEeHDxHpraOvj8GWOZNSbnA8mzU3pSPLfOO47Xvn4W4wvSueXhlfxmwTY7P2rMIIraBKqq31bVYlUtA64BXlfV64A3gCvdwW4EnvMoRGP6RVV5amU5h+tauGZOKaUj0nodpzgnlSe+eCqfmDmaX722lZ/9fbMlUWMGSdQ24R7Dt4DHROROYBXwV4/jMSYsr2w4xLr9R/nolEImFmb0PoIrMd7H/145jbTEeO59cyfZqQncOu84HlmyN6Txr51rfQKMCUVMJFBVXQgsdF/vBE72Mh5j+qu+pZ3vP7+BUVnJnDkhv8/j+3zCDy+dytHmdn7xjy0U51inImMGWkwkUGOiTW+1wX+sP0RFXSu3zBtPnO/D5zxD4fMJ//vJaRw82swdT63l5jPHUZCZHNa0jDEfFrXnQI2JVfUt7SzeeYRpJdn9rjkmxvv47admkpIQxyNL99LuDwxQlMYYS6DGRJiFWyvxB5TzJhcMyPRGZiXzy6umUVHfyuubKwZkmsYYS6DGRJT6lnaW7apmZmkOI9KTBmy68yYVMHtMDm9uraS8pmnApmvMcGbnQI2JIO/trMYfUM7qY8ehUHrYfuzEUWw5XM/zaw7wpbPHd3s9qTEmdFYDNSZCtPsDLNlVxeSRGeRlDFzts1NyQhwXTh1JeU0zq/bWDvj0jRluLIEaEyFW7a2lqc3PGWFcthKqaSXZlOSk8MqGQ7R2+AdtPsYMB5ZAjYkAqsqSXVWMykqmbMTgXbPpE+HiE0fR0NrB4h1VgzYfY4YDS6DGRIDymmYOHm3h5LG5yCCfmywdkcbkkRm8ua2S5jarhRoTLkugxkSApbuqSYzzMa04e0jmd/6UQlraA7y9vXJI5mdMLLIEaozHWtr9rN1fy7SSLJIT4oZknqOyUpgyKpP3dlbbuVBjwmQJ1BiPrd9/lHa/MntM7pDO96yJ+TS3+1m+u2ZI52tMrLAEaozHVu6tJS89ieKclCGdb2luKmUjUnln+xH8AXvkmTF9ZQnUGA9VN7axu6qRmaXZg955qDtnTsintrmddftrh3zexkQ7S6DGeGjVvhoEmF6S7cn8J43MID8jibe2HbEHbxvTR5ZAjfGIqrJqby1j89PITk30JAafCGdNyOPg0Ra2VzR4EoMx0coSqDEe2VvdRHVjGzNLcjyNY1pxNhnJ8byz44incRgTbSyBGuORlXtrSIgTpo7O9DSO+Dgfc8py2Xa4gerGNk9jMSaaWAI1xgPt/gDr9h/lhKIskuKH5trPY5lTlosILN1lt/czJlSWQI3xwKaDdbS0B5hR6m3zbaeslASOH5XJ8j01tLTbjRWMCYUlUGM8sGpvLVkpCYzLT/M6lPfNHTuCpjY/L6876HUoxkQFS6DGDLGK+ha2VdQzvSQ7oh5qPS4/jbz0RB56b4/XoRgTFSyBGjPEnl99gIDCjNJsr0P5AJ8Ic8eOYOXeWtbvP+p1OMZEPEugxgyxJ1eUU5yTQkFGstehfMjM0hySE3w8vGSv16EYE/EsgRozhDYeqGPzofqI6TzUVUpiHBefWMQLaw7Q1NbhdTjGRDRLoMYMoadXlpMQJ0wbneV1KD26ek4JDa0dvLTWOhMZcyyWQI0ZIh3+AM+uPsC5kwtITYr3OpwezSnLYVxeGo8v3+d1KMZENEugxgyRhVsqOdLQyhUzi70O5ZhEhKvmlLBsd43dH9eYY7AEaswQeWzZPvLSkzhncoHXofTqEzNHE+cTnrBaqDE9sgRqzBCoqGvhjS0VXDmrmIS4yN/sCjKSOXdyAU+tLKfdH/A6HGMiUuRvycbEgCdXluMPKFfPKfE6lJBdPbuEIw1tvL65wutQjIlIlkCNGWSqyuPL9jF3bC5j8yLn1n29mTcpn4KMJOYvs2ZcY7pjCdSYQbZkVzW7q5qiqvYJzmPOrpxVzMItFRw62uJ1OMZEHEugxgyy+cv2kZEcz0UnjPI6lD67anYJAYWnVpZ7HYoxEccSqDGD6GhzOy+vO8jl00eTkuj9cz/7qiwvjbljc3l8+T4CAfU6HGMiiiVQYwbRE8v30doRiLrm22DXnFzCnqomluyq9joUYyKKJVBjBok/oDyweA9zynI4IYJv3debC6eOIj0p3ppxjekiahOoiCSLyFIRWSMiG0Tkh+7nY0VkiYhsF5H5IpLodaxmeFq4pYK91U3ceFqZ16H0i3OD+VG8vO4gja12g3ljOkVtAgVagXNVdRowHbhQRE4Bfg78n6oeB9QAN3kXohnO7nt3NyMzk7lg6kivQ+m3K2YV09Tm5x/rD3kdijERI3LvaN0LVVWg80adCe6fAucC17qf3w/8APjjUMdnhrftFQ28te0It390YlTceSjYI908C1RVyU1L5PcLt9Pa4dyZ6Nq5pUMdmjERJbq27C5EJE5EVgMVwGvADqBWVTvbmcqB0T2Me7OILBeR5ZWVlUMSrxk+Hli8m8Q4H9ecHBtJRkSYUZrNrspGapvavA7HmIgQ1QlUVf2qOh0oBk4GJvdh3HtVdbaqzs7Pzx+sEM0wVNfSzlMryrlkWhF56UlehzNgZpbkoMCqfbVeh2JMRIjqBNpJVWuBN4BTgWwR6WyaLgb2exWXGZ6eXF5OY5ufz0R556GuctISGZuXxso9NThnUIwZ3qI2gYpIvohku69TgPOBTTiJ9Ep3sBuB5zwJ0AxLgYDywOLdzCzN5sTi6L10pSczS3Ooamxjb3WT16EY47moTaDAKOANEVkLLANeU9UXgW8BXxeR7cAI4K8exmiGmUVbK9ld1cRnTh/rdSiD4oSiTBLihJV7a70OxRjPRXMv3LXAjG4+34lzPtSYIXffu7spyEjiohOi/9KV7iQlxHFCURZry2tpafeTnBB9tyc0ZqBEcw3UmIiyo7KBRVsruf6UMVF36UpfzByTQ2tHgFc3HvY6FGM8FbtbuTFD7MHFe0iM8/GpGLl0pSdj89LITkngqRV2az8zvEVtE64xQ627Gwx0amn38+jSvUwpyiQ/I3YuXemOT4TpJdm8tf0IRxpaY+pSHWP6wmqgxgyAlXtraO0IcNr4EV6HMiROKsnGH1BeXnfQ61CM8YzVQI3pp4Aq7+2soiQnheKc1GPWVGPFyMxkJo/M4LnVB/j0qWVeh2OMJ6wGakw/ba9o4EhDG6cOk9pnp0umFbFiTw377JpQM0xZAjWmnxbvqCIjKT6qn/kZjkunFQHwwtoDHkdijDesCdeYfqhubGPr4XrOmVxAvG94HY++te0IpbmpPPDuHrJTun/srj2xxcSyiNjiReRpEblYRCIiHmNCtXRXNSIwpyzX61A8Ma04i0N1LRyqa/E6FGOGXKQkrD/gPMNzm4jcJSKTvA7ImN50+AOs2FPN5JGZZKUkeB2OJ04YnYVPYK09ocUMQxGRQFX1n6p6HTAT2A38U0TeFZHPisjw3DOZiLfhQB2NbX7mjh2etU+AjOQExuens6a81p7QYoadiEigACIyAvgM8HlgFXA3TkJ9zcOwjOnRkl1V5KYlMr4g3etQPHXi6Cxqmto5cNSacc3wEhEJVESeAd4CUoFLVPVSVZ2vqv8ODO+9k4lIh+pa2F3VxMllufhEvA7HU8ePysQnsGH/Ua9DMWZIRUQCBf6sqlNU9WeqehBARJIAVHW2t6EZ82FLd1UT7xNmjcnxOhTPpSXFU5aXxvoDddaMa4aVSEmgd3bz2eIhj8KYELR2+Fm1t4YTRmeRlmRXggGcUJTFkYZWKupbvQ7FmCHjaQIVkZEiMgtIEZEZIjLT/ZuH05xrTMRZu+8orR2BYd15qKupRZkIsP6ANeOa4cPrw+cLcDoOFQO/Cvq8HviOFwEZcyyqypJdVYzMTKY0147xOmUkJ1A6IpUN++s4b3Kh1+EYMyQ8TaCqej9wv4hcoapPeRmLMaEor2nmwNEWLp1WhAzzzkNdnVCUxUvrDtojzsyw4XUT7vXuyzIR+XrXPy9jM6Y7S3ZVkxjvY0ZJttehRJypRZmA9cY1w4fXnYjS3P/pQEY3f8ZEjKa2DtaW1zK9JJukhDivw4k42amJFOeksP5AndehGDMkvG7Cvcf9/0Mv4zAmFCv31tIRUOs8dAxTi7J4ZcMhjja3D9vbG5rhw+saKAAi8gsRyRSRBBFZICKVQc27xnhOVVm6q4rS3FRGZaV4HU7EmjzSaTjafMhqoSb2RUQCBT6qqnXAx3HuhXsc8J+eRmRMkMU7qjjS0Ga1z14UZCSRm5bI5oP1XodizKCLlATa2ZR8MfCEqlovBBNRHlqyh9TEuGH30Oy+EhGOH5nBjsoG2joCXodjzKCKlAT6oohsBmYBC0QkH7A7U5uIUFHXwqsbDjOrNIeEuEjZZCLX5FGZdASU7RVWCzWxLSL2Bqp6B3AaMFtV24FG4DJvozLG8diyfXQElJOt+TYkZSPSSE7wscmacU2M8/pORMEm41wPGhzTA14FYwxAuz/Aw0v2cNbEfEbYzQFCEucTJhZmsPlQHf6AEuezG06Y2BQRNVAReRD4X+AMYI77Z09hMZ57dcNhDte1cuOpY7wOJaocPzKTxjY/q/fVeh2KMYMmUmqgs4Epas9CMhHm/sW7KclNYd6kAuYv2+d1OFFjYmEGPoEFmw7bI99MzIqIGiiwHhjpdRDGBNt0sI6lu6q54ZQx1gzZRymJcZSNSOOfmw57HYoxgyZSaqB5wEYRWQq8/0BBVb3Uu5DMcPfA4j0kxfu4anaJ16FEpcmjMnl53UH2VTdRYk+uMTEoUhLoD7wOwJhgR5vaeXbVfi6fPprs1ESvw4lKk0dm8PK6g7y+uYIbTyvzOhxjBlxENOGq6iKcOxAluK+XASs9DcoMa0+s2Edzu58brPNQ2PLSkxibl8brmyu8DsWYQRERCVREvgA8CdzjfjQaeNazgMyw1u4P8Le3d3FyWa7deaifzplUwOKdVTS1dXgdijEDLiISKPBl4HSgDkBVtwEFnkZkhq0X1x7gwNEWvnj2OK9DiXrnHV9AW0eAd7ZXeR2KMQMuUhJoq6q2db5xb6Zgl7SYIaeq3LNoJxML0zlnkh3D9decslzSk+KtGdfEpEhJoItE5DtAioicDzwBvOBxTGYYWrS1ks2H6rn5rPH47NKVfkuM93HmhDze2FyBXeZtYk2kJNA7gEpgHfBF4GXge8caQURKROQNEdkoIhtE5Kvu57ki8pqIbHP/21XcJmT3LNrJyMxkLp1W5HUoMeOcyQUcqmth40F7RqiJLRGRQFU1gNNp6FZVvVJV/xzCXYk6gG+o6hTgFODLIjIFJxkvUNUJwAL3vTG9WrOvlsU7q7jpjLEkxkfEphETOpvC37BmXBNjPL0OVEQE+D7wFdxkLiJ+4Leq+qNjjauqB4GD7ut6EdmE03v3MmCeO9j9wELgW4MQvokCjyzZG9Jw184t5d43d5KRHM81J9uNEwZSfkYS04qzWLC5gq+cO8HrcIwZMF4fZt+G0/t2jqrmqmouMBc4XURuC3UiIlIGzACWAIVucgU4BBT2MM7NIrJcRJZXVlb2ZxlMDNh9pJG/rz/I9aeMISM5wetwYs45kwtYva+WqobW3gc2Jkp4nUBvAD6lqrs6P1DVncD1wKdDmYCIpANPAV9T1Q+cZHGbgbttClbVe1V1tqrOzs/PDzd+EyP+/NZO4n0+Pmt3zBkU500uRBUWbrGDVRM7vE6gCap6pOuHqloJ9FoNEJEEnOT5sKo+7X58WERGud+PAuzEizmm+pZ2nlhRzidmjqYgM9nrcGLS1KJM8jOSeH2LbY4mdnidQNvC/K7z/OlfgU2q+qugr54HbnRf3wg8168ITcx7b2cV7f4AXzjLbpwwWHw+4dxJBby5tZJ2f8DrcIwZEF4n0GkiUtfNXz1wYi/jno7TBHyuiKx2/z4G3AWcLyLbgI+4743pVmuHn/d2VnP+8YWMz0/3OpyYds7kAupbOli+u8brUIwZEJ72wlXVuH6M+zbQ05Xu54U7XTO8LN9dQ3O7ny+ePd7rUGLeGRPySIgT3thSwanjR3gdjjH95nUN1BjP+APKO9uPMGZEKrPG2P02Blt6UjynjBvBAnvItokRlkDNsLVufy21ze2cPcF6YQ+VcyYVsKOykb1VTV6HYky/WQI1w5Kq8ubWIxRkJDFxZIbX4Qwb5x3v3JXo9c1WCzXRzxKoGZa2VzRwqK6FMyfk4RO7afxQGTMijXH5aSyw2/qZGOBpJyJjvPL29iNkJMUzrTgbCP2Wf6b/zp1UwAOL99DY2kFaku2CTPSyGqgZdg7XtbCtooFTxo8gPs42gaF27vEFtPkDvL39Q/dQMSaq2N7DDDvvbD9CvE84uSzX61CGpTlluWQkxdvTWUzUs/YTM6w0tHawel8tM0tzrPlwCPTUND4mL42X1h3kxNFZiAjXzi0d4siM6T+rgZphZcnOKjoCymnH2YX8XppcmEF9SwcHjrZ4HYoxYbMEaoaNdn+A93ZWMakwg4IMu2m8lyaOzECALYfqeh3WmEhlCdQMG+v2H6Wxzc/px+V5Hcqwl54UT3FOCpsP1XsdijFhswRqho2lu6rJS09ifH6a16EYYNLIDPbXNNPQ2uF1KMaExRKoGRYOHm1mb3UTJ4/NRezGCRFh8shMFNhqtVATpSyBmmFh6a5q4n3CzNJsr0MxrlFZyWQmx7PZzoOaKGUJ1MS81nY/q/bVcuLoLFIT7dKVSCEiTCzMYFtFA20d9pBtE30sgZqYt6b8KG0dAeaOtRsnRJrjR2XS2uH0jjYm2lgCNTFNVVm6q4qRmcmU5KZ6HY7p4riCdBLjfby87qDXoRjTZ5ZATUwrr2nmwNEW6zwUoRLifEwZlck/Nhyi3W/NuCa6WAI1MW3p7moS43xML8n2OhTTgxOKsqhtaufdHdaMa6KLJVATs442t7O2vJZpJVkkJ8R5HY7pwYTCdNKT4nl5rTXjmuhiCdTErGdWltPuV04us/veRrKEOB/nTym0ZlwTdSyBmpikqjy8ZC/FOSmMzknxOhzTi4tPHMXR5nbesWeEmihiCdTEpGW7a9hW0WDP/IwSZ07MIyMpnpesGddEEUugJiY9vGQPGcnxnFSc7XUoJgRJ8XGcP6WQVzcetpsqmKhhCdTEnKqGVv6+7hBXzCwmMd5W8Whx8UlOM+6bWyu9DsWYkNjexcScJ1eU0+YPcO3cUq9DMX1w1sR8RqQl8uSKcq9DMSYklkBNTAkElEeX7mVOWQ4TCzO8Dsf0QUKcj8tnjGbB5sNUN7Z5HY4xvbIEamLKuzuq2F3VxHVzx3gdignDJ2cX0+5Xnl213+tQjOmVJVATUx5esoec1AQuPGGk16GYMEwemcmJo7N4wppxTRSwBGpixsGjzby68TCfnF1idx6KYp+cXcymg3Ws33/U61CMOSZLoCZmPPTeHgKq3HCKNd9Gs0unFZEY57PORCbiWQI1MaGl3c+jS/fxkeML7bFlUS47NZHzpxby3Or9dk2oiWiWQE1MeGHNAaob2/jsaWVeh2IGwCdnFVPT1M4rGw55HYoxPbIEaqKeqnLfu7uZWJjOqePtxvGx4KwJ+YwZkcr97+72OhRjemQJ1ES95Xtq2HCgjs+cNtYemh0jfD7h06eWsXxPDevKrTORiUzxXgdgTH/d9+5uslISuHxGkdehmDA9smTvhz4TIDHOx/efX8+Vs0rszlIm4lgN1ES1vVVN/GP9Ia6ZU0Jqoh0PxpLkhDhmjslmTflRGlo7vA7HmA+J2gQqIn8TkQoRWR/0Wa6IvCYi29z/OV7GaAbfHxZuJ84n3HTGWK9DMYPglHEj8AeUpbuqvQ7FmA+J2gQK3Adc2OWzO4AFqjoBWOC+NzFqf20zT60s55o5JRRkJnsdjhkEBRnJTChIZ8muKtr9dkmLiSxRm0BV9U2g62HpZcD97uv7gcuHMiYztP60cAcAXzp7vMeRmMF02vg86ls6eHHtAa9DMeYDojaB9qBQVTsfaX8IKPQyGDN4Dte1MH/5Pq6cVUxRdorX4ZhBNKEwncLMJP7wxg4CAfU6HGPeF2sJ9H2qqkCPW5uI3Cwiy0VkeWWlPcA32tyzaCf+gHLL2cd5HYoZZD4R5k0sYFtFA69utBsrmMgRawn0sIiMAnD/V/Q0oKreq6qzVXV2fn7+kAVo+u/Q0RYeWbqHy6YXUTrCbts3HJxYnMXYvDR+98Z2nGNjY7wXawn0eeBG9/WNwHMexmIGyS/+sZlAAL523kSvQzFDxCfCLWePZ/3+OhZutRYjExmi9sI5EXkUmAfkiUg58H3gLuBxEbkJ2ANc5V2EZjCs3lfL06v2c8u88Vb7HGZaOvxkpyTw/ec2cOCs5h7vOmU3XDBDJWoTqKp+qoevzhvSQMyQUVV+9MIG8tKTuHWe9bwdbuJ9Ps6amM/zaw6w80gj4/PTvQ7JDHNRm0BN7Orutm4Aa8prWbm3lk/MGE1GcsIQR2UiwawxOSzcUsGrGw7xpbPH272Pjadi7RyoiVFtHQFeWX+IUVnJzBxjN5garhLifHzk+EL21TSz8WCd1+GYYc4SqIkKr208RG1zOx8/qQif1TqGtRmlOeSnJ/HKhsP47bpQ4yFLoCbi7a1q5N0dVcwdm8vYvDSvwzEei/MJF0wt5EhDKyv31ngdjhnG7ByoiWgd/gBPrdpPZkoCF0wd+f7nPZ0nNcPD8aMyKc1NZcGmw0wrziYx3uoCZujZWmci2utbKqisb+XfZowmOSHO63BMhBARLpg6krqWDt7ZccTrcMwwZQnURKzymibe3FrJzNJsJhZmeB2OiTBj89KYWpTJwi0V1DS1eR2OGYYsgZqI1NrhZ/6yfWQkJ3DxiUVeh2Mi1MUnjgLgpbUHexnSmIFnCdREpBfXHKS6sY2rZpeQkmhNt6Z72amJnDupgI0H69hyqN7rcMwwYwnURJy15bWs2FvDvEn51uvW9Or0CXnkpSfxwtoD9tBtM6QsgZqIUl7TxLOr91OSk8K5k+1xrqZ38T4fl0wbRXVjG4vsRvNmCFkCNRHDH1Bum78aVbh6TilxPrthggnNhIIMppdks3BLBav31XodjhkmLIGaiPH7N7azbHcNl04rIjct0etwTJS55KQiMpITuG3+apraOrwOxwwDlkBNRFixp4a7F2zjsulFTC/J9jocE4VSEuP45Kxidlc18tOXN3kdjhkGLIEaz9W3tPO1+asYlZXMjy8/wZ6wYcI2Lj+dz58xlofe28vrmw97HY6JcZZAjef++7kN7K9p5u5rppNpjykz/XT7BZM4flQmt81fw56qRq/DMTHMEqjx1LOr9vPMqv189byJzBqT63U4JgYkxcdxz/WzALj5gRXUt7R7HJGJVZZAjWd2H2nku8+sY/aYHL58znivwzExpHREKr+/diY7Khu45aGVtHXY9aFm4NnTWMyQCX6CSoc/wJ/e3EFA4dzJBTy+vNzDyEwsOmNCHnddcRK3P7GGrz++ml9fPZ34OKszmIFjCdR44h8bDnGgtoXr544hO9UuWTGD48pZxVQ3tvLTlzcjIvzqqmkkWBI1A8QSqBlymw7W8e6OKk4dP4IpRZleh2Ni3M1njSegcNffN1Pb1Mbvr5tpndXMgLBDMTOkapvaeHJFOUXZyVwU9IBsYwbTl84ezy+uPInFO6q44g/vsq+6yeuQTAywBGqGjD+gzF+2D78q18wptfNRZkhdNbuEB246mcN1LfzbH97hXXsQt+kna8I1Q+bVjYfYU93EVbNLyEtP8jocMwydNj6Pp289nZsfXM51f17CvEkFnDu5oMf7Ll87t3SIIzTRxKoAZkg8t3o/b207wtyxuXarPuOp4wrSefHfz2BGaQ5vbKngL2/vpKapzeuwTBSyBGoG3YYDR/nWU2sZMyKVi08a5XU4xpCaGM+Vs4q5anYxB4+28JsF21i5pwZV9To0E0UsgZpBVd3YxhcfXEF2SiLXnlxKvM9WORM5ppfk8B/nTmBUVjJPrizn4SV7aWi1J7mY0NjezAyaprYObrp/GRX1rfzphllk2KUDJgLlpiXy+TPHceHUkWw5XM/dC7ax+WCd12GZKGCdiMygaPcHuOWhlazZV8sfrpvF9JJsNh6wnZIZfMF3vAqVT4SzJuYzoTCdJ5aX88B7e5g9JodLpxeRnmS7SdM9q4GaARcIKLc/sYZFWyv56b+dyIUn2PWeJjqMykrh1nnjOWtCPiv21HDR3W+ybHe112GZCGUJ1Awof0D57rPreG71Af7zgklcc7JdBmCiS3ycjwtPGMkXzhwHwFX3LOauv2+mtcPvcWQm0lgCNQOmtcPP1+av5tGl+/jKOcdx6zx7woqJXmV5afz9q2dx9ewS/rRoB5f97h022blRE8QSqBkQRxpaue7PS3hhzQHuuGgyt18wCZHuL043JlqkJ8Vz1xUn8ZdPz+ZIQyuX/PZt7nxxI3X2jFGDdSIyA+DdHUe4bf5qapva+d21M/j4SUVeh2TMgPrIlEJeKT2L/3llC399ZxfPrNrPNz46iStnFZMYb/WQ4cp+eRO2upZ2/uvZ9Vz3lyWkJcXz9K2nWfI0MWtEehJ3XXESz3/5DMbmpfGdZ9Zx+s9f57cLtlHdaHcyGo7E7rwBs2fP1uXLl3sdRtRoaO3gsaV7+cPCHdQ2tXHjaWWU5qaSFB/ndWjGDKie7oWrqry17Qh/fXsXi7ZWkhAnzB07gnMmF3D2xHzG5aXh6+H+urFCRFao6myv4/CSNeGakG2vqOfBxXt4auV+Glo7OOO4PO64aDInjM4K69o7Y6KVuNeNnjUxn22H63liRTmvb67gxy9u5MdASkIcEwvTmVCYwaisZAoykynISKLQ/Z+fkWQP9o4BVgPFaqA9aWn388tXt7LlUB2bD9VTUd9KnE84cXQWp4wbQWluqtchGhNRqhvb2FHZwOG6Fg7XtVBR30pDSwdd97ICpCfHk52SQHZqIjmpCeSkJTIiLYnPnVFGUVZKxNdgrQYaozVQEbkQuBuIA/6iqnd5HFJECwSUIw2t7KhsZHtlA1sP1bOmvJZNB+to9ys+gbF5acwpy2VaSbbdmcWYHuSmJZKblvuBz/wBpbGtg/rmDupb2qlr6aCupZ2jTe3UNLexv7aZjQfq8LuVmb+9s4vEeB9jclMpy0tjbF4aZSPSKMtLZWxeGoUZyRGfXIeLmNsTikgc8HvgfKAcWCYiz6vqxsGedyCg+FXxB9w/Vecz96+53U9Tm/PX3Oanqa2jy2cdtLQHaGn309rxr/+tHX4CCgdqmxERfOLcekwAn0+I8wlx4v73CdNKskmMExLifMTH+RCgud1PS7s733Y/VQ2tVNS3UlHXSkV9C+3+fx0jpyXGcWJxFjedMY6GlnbG5aeTnGDnN40JR5xPyExOIDM5AUjpdpiAKnXN7VQ1tjE+P53dVY3sOtLI7iONLNpaSVtH4P1hkxN8jMl1EmpZXhqjMpPJSk0gK8X5y0xOICk+jsR4H4nxPhLixHkd57NLywZYzCVQ4GRgu6ruBBCRx4DLgAFPoBf835vsqmp8P0EOBJ9AckIcSfG+9/8nxcch4jQPqTobm+L8d5I2+AMB/AElEIBFWyu7nbaIc24mJSGO3LRECjOTmTsujcLMZEZmJjMuP43jCtIZmZn8/oZm5zaNGXw+EbJTE8lOTfxQxyV/QDl4tJndR5rYXeUk1d1VjWyvaOCNzZW0+QM9TLW7+Tjz8okg4uwTNv7wQqvRhikWE+hoYF/Q+3JgbteBRORm4Gb3bYOIbBmC2ADygCNDNK9oZWUUGiun0ERVOV03xPOLuxMIr4zGDHgwUSYWE2hIVPVe4N6hnq+ILB/uJ957Y2UUGiun0Fg59c7KKDyx2I96P1AS9L7Y/cwYY4wZMLGYQJcBE0RkrIgkAtcAz3sckzHGmBgTc024qtohIl8BXsG5jOVvqrrB47CCDXmzcRSyMgqNlVNorJx6Z2UUBruRgjHGGBOGWGzCNcYYYwadJVBjjDEmDJZAB4GI5IrIayKyzf2f08NwN7rDbBORG7v5/nkRWT/4EQ+9/pSRiKSKyEsisllENohIzN2qUUQuFJEtIrJdRO7o5vskEZnvfr9ERMqCvvu2+/kWEblgSAMfQuGWkYicLyIrRGSd+//cIQ9+CPVnXXK/LxWRBhG5fciCjhaqan8D/Af8ArjDfX0H8PNuhskFdrr/c9zXOUHffwJ4BFjv9fJEWhkBqcA57jCJwFvARV4v0wCWTRywAxjnLt8aYEqXYW4F/uS+vgaY776e4g6fBIx1pxPn9TJFWBnNAIrc1ycA+71enkgsp6DvnwSeAG73enki7c9qoIPjMuB+9/X9wOXdDHMB8JqqVqtqDfAacCGAiKQDXwfuHPxQPRN2Galqk6q+AaCqbcBKnOt9Y8X7t6N0l6/zdpTBgsvvSeA8ce6/eBnwmKq2quouYLs7vVgTdhmp6ipVPeB+vgFIEZGkIYl66PVnXUJELgd24ZST6cIS6OAoVNWD7utDQGE3w3R3y8HR7usfA78EmgYtQu/1t4wAEJFs4BJgwSDE6JVelzt4GFXtAI4CI0IcNxb0p4yCXQGsVNXWQYrTa2GXk3sg/y3gh0MQZ1SKuetAh4qI/BMY2c1X3w1+o6oqIiFfKyQi04Hxqnpb13MR0Wawyiho+vHAo8Bv1H14gDGhEpGpwM+Bj3odS4T6AfB/qtpgT3HpniXQMKnqR3r6TkQOi8goVT0oIqOAim4G2w/MC3pfDCwETgVmi8hunN+nQEQWquo8oswgllGne4Ftqvrr/kcbUUK5HWXnMOXugUQWUBXiuLGgP2WEiBQDzwCfVtUdgx+uZ/pTTnOBK0XkF0A2EBCRFlX93aBHHSWsCXdwPA909qq9EXium2FeAT4qIjluD9SPAq+o6h9VtUhVy4AzgK3RmDxDEHYZAYjInTgb+tcGP9QhF8rtKIPL70rgdXV6fDwPXOP2rBwLTACWDlHcQynsMnKb/V/C6cT2zlAF7JGwy0lVz1TVMndf9Gvgp5Y8u/C6F1Ms/uGcZ1kAbAP+CeS6n88G/hI03OdwOnlsBz7bzXTKiN1euGGXEc5RtAKbgNXu3+e9XqYBLp+PAVtxelB+1/3sR8Cl7utknJ6R23ES5Ligcb/rjreFGOqdPFBlBHwPaAxad1YDBV4vT6SVU5dp/ADrhfuhP7uVnzHGGBMGa8I1xhhjwmAJ1BhjjAmDJVBjjDEmDJZAjTHGmDBYAjXGGGPCYAnUGGOMCYMlUGOMMSYM/x8zKJDXgHtDJQAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Intialize bias with mean 0.0 and standard deviation of 10^-2\n",
    "weights = initialize_weights((1000,1))\n",
    "sns.distplot(weights)\n",
    "plt.title(\"Plot of weights initialized, with mean of 0.0 and standard deviation of 0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bias(shape, dtype=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Plot of biases initialized, with mean of 0.0 and standard deviation of 0.01')"
      ]
     },
     "metadata": {},
     "execution_count": 17
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"263.63625pt\" version=\"1.1\" viewBox=\"0 0 453.991875 263.63625\" width=\"453.991875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-01-24T03:35:15.482056</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 263.63625 \nL 453.991875 263.63625 \nL 453.991875 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 59.595937 239.758125 \nL 394.395938 239.758125 \nL 394.395938 22.318125 \nL 59.595937 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 103.128424 239.758125 \nL 113.037825 239.758125 \nL 113.037825 238.101439 \nL 103.128424 238.101439 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 113.037825 239.758125 \nL 122.947226 239.758125 \nL 122.947226 239.758125 \nL 113.037825 239.758125 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 122.947226 239.758125 \nL 132.856627 239.758125 \nL 132.856627 238.101439 \nL 122.947226 238.101439 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 132.856627 239.758125 \nL 142.766028 239.758125 \nL 142.766028 231.474696 \nL 132.856627 231.474696 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 142.766028 239.758125 \nL 152.67543 239.758125 \nL 152.67543 228.161325 \nL 142.766028 228.161325 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 152.67543 239.758125 \nL 162.584831 239.758125 \nL 162.584831 221.534582 \nL 152.67543 221.534582 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 162.584831 239.758125 \nL 172.494232 239.758125 \nL 172.494232 224.847954 \nL 162.584831 224.847954 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 172.494232 239.758125 \nL 182.403633 239.758125 \nL 182.403633 211.594468 \nL 172.494232 211.594468 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 182.403633 239.758125 \nL 192.313034 239.758125 \nL 192.313034 188.400868 \nL 182.403633 188.400868 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 192.313034 239.758125 \nL 202.222435 239.758125 \nL 202.222435 163.550582 \nL 192.313034 163.550582 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_13\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 202.222435 239.758125 \nL 212.131836 239.758125 \nL 212.131836 150.297096 \nL 202.222435 150.297096 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 212.131836 239.758125 \nL 222.041237 239.758125 \nL 222.041237 82.372982 \nL 212.131836 82.372982 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 222.041237 239.758125 \nL 231.950638 239.758125 \nL 231.950638 88.999725 \nL 222.041237 88.999725 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 231.950638 239.758125 \nL 241.860039 239.758125 \nL 241.860039 80.716296 \nL 231.950638 80.716296 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_17\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 241.860039 239.758125 \nL 251.76944 239.758125 \nL 251.76944 32.672411 \nL 241.860039 32.672411 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 251.76944 239.758125 \nL 261.678841 239.758125 \nL 261.678841 84.029668 \nL 251.76944 84.029668 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 261.678841 239.758125 \nL 271.588242 239.758125 \nL 271.588242 74.089554 \nL 261.678841 74.089554 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 271.588242 239.758125 \nL 281.497643 239.758125 \nL 281.497643 142.013668 \nL 271.588242 142.013668 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 281.497643 239.758125 \nL 291.407044 239.758125 \nL 291.407044 148.640411 \nL 281.497643 148.640411 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 291.407044 239.758125 \nL 301.316445 239.758125 \nL 301.316445 168.520639 \nL 291.407044 168.520639 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 301.316445 239.758125 \nL 311.225847 239.758125 \nL 311.225847 201.654354 \nL 301.316445 201.654354 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 311.225847 239.758125 \nL 321.135248 239.758125 \nL 321.135248 203.311039 \nL 311.225847 203.311039 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 321.135248 239.758125 \nL 331.044649 239.758125 \nL 331.044649 228.161325 \nL 321.135248 228.161325 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 331.044649 239.758125 \nL 340.95405 239.758125 \nL 340.95405 228.161325 \nL 331.044649 228.161325 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 340.95405 239.758125 \nL 350.863451 239.758125 \nL 350.863451 238.101439 \nL 340.95405 238.101439 \nz\n\" style=\"fill:#1f77b4;opacity:0.4;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mdd2cd8fc6c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"90.3624\" xlink:href=\"#mdd2cd8fc6c\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.46 -->\n      <g transform=\"translate(79.229588 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"128.300685\" xlink:href=\"#mdd2cd8fc6c\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.47 -->\n      <g transform=\"translate(117.167872 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"166.238969\" xlink:href=\"#mdd2cd8fc6c\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.48 -->\n      <g transform=\"translate(155.106157 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"204.177254\" xlink:href=\"#mdd2cd8fc6c\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.49 -->\n      <g transform=\"translate(193.044441 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-57\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"242.115538\" xlink:href=\"#mdd2cd8fc6c\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.50 -->\n      <g transform=\"translate(230.982726 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"280.053823\" xlink:href=\"#mdd2cd8fc6c\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0.51 -->\n      <g transform=\"translate(268.92101 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"317.992107\" xlink:href=\"#mdd2cd8fc6c\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.52 -->\n      <g transform=\"translate(306.859295 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"355.930392\" xlink:href=\"#mdd2cd8fc6c\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.53 -->\n      <g transform=\"translate(344.797579 254.356563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"393.868676\" xlink:href=\"#mdd2cd8fc6c\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.54 -->\n      <g transform=\"translate(382.735864 254.356563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m1f0785a8c3\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"59.595937\" xlink:href=\"#m1f0785a8c3\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0 -->\n      <g transform=\"translate(46.233437 243.557344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"59.595937\" xlink:href=\"#m1f0785a8c3\" y=\"196.485838\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 10 -->\n      <g transform=\"translate(39.870937 200.285057)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"59.595937\" xlink:href=\"#m1f0785a8c3\" y=\"153.213552\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 20 -->\n      <g transform=\"translate(39.870937 157.012771)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"59.595937\" xlink:href=\"#m1f0785a8c3\" y=\"109.941265\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 30 -->\n      <g transform=\"translate(39.870937 113.740484)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"59.595937\" xlink:href=\"#m1f0785a8c3\" y=\"66.668979\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 40 -->\n      <g transform=\"translate(39.870937 70.468198)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"59.595937\" xlink:href=\"#m1f0785a8c3\" y=\"23.396692\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 50 -->\n      <g transform=\"translate(39.870937 27.195911)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- Density -->\n     <g transform=\"translate(33.79125 150.046719)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 19.671875 64.796875 \nL 19.671875 8.109375 \nL 31.59375 8.109375 \nQ 46.6875 8.109375 53.6875 14.9375 \nQ 60.6875 21.78125 60.6875 36.53125 \nQ 60.6875 51.171875 53.6875 57.984375 \nQ 46.6875 64.796875 31.59375 64.796875 \nz\nM 9.8125 72.90625 \nL 30.078125 72.90625 \nQ 51.265625 72.90625 61.171875 64.09375 \nQ 71.09375 55.28125 71.09375 36.53125 \nQ 71.09375 17.671875 61.125 8.828125 \nQ 51.171875 0 30.078125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-68\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-68\"/>\n      <use x=\"77.001953\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"138.525391\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"201.904297\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"254.003906\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"281.787109\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"320.996094\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p6d6bb2a402)\" d=\"M 74.814119 239.750416 \nL 85.520378 239.636343 \nL 94.697171 239.291372 \nL 102.344498 239.04957 \nL 117.639154 238.867147 \nL 120.698085 238.519586 \nL 123.757016 237.97132 \nL 126.815947 237.208206 \nL 129.874878 236.222376 \nL 132.933809 235.007267 \nL 135.99274 233.56213 \nL 139.051671 231.908207 \nL 148.228464 226.597058 \nL 151.287395 225.189276 \nL 154.346326 224.135069 \nL 157.405257 223.374213 \nL 161.993653 222.316328 \nL 163.523119 221.836503 \nL 165.052584 221.228503 \nL 166.58205 220.458298 \nL 168.111515 219.497347 \nL 169.640981 218.323302 \nL 171.170446 216.920057 \nL 172.699912 215.277349 \nL 174.229377 213.390178 \nL 175.758843 211.258263 \nL 177.288308 208.885678 \nL 178.817774 206.280667 \nL 181.876705 200.426454 \nL 184.935636 193.836363 \nL 187.994567 186.682926 \nL 192.582964 175.202304 \nL 197.17136 162.8872 \nL 200.230291 154.055777 \nL 203.289222 144.635064 \nL 212.466015 115.519644 \nL 213.995481 111.300305 \nL 215.524946 107.459907 \nL 217.054412 104.033297 \nL 218.583877 101.024999 \nL 220.113343 98.408353 \nL 221.642808 96.128372 \nL 224.701739 92.255091 \nL 229.290136 86.775717 \nL 230.819601 84.722277 \nL 233.878532 80.054152 \nL 241.52586 67.1224 \nL 243.055325 65.087074 \nL 244.584791 63.501769 \nL 246.114256 62.454744 \nL 247.643722 62.003847 \nL 249.173187 62.171825 \nL 250.702653 62.946459 \nL 252.232118 64.2856 \nL 253.761584 66.126281 \nL 255.291049 68.396262 \nL 256.820515 71.025841 \nL 258.34998 73.957707 \nL 261.408911 80.592578 \nL 264.467842 88.201208 \nL 267.526773 96.807136 \nL 270.585705 106.287046 \nL 276.703567 125.938686 \nL 279.762498 134.862301 \nL 282.821429 142.700619 \nL 285.88036 149.589952 \nL 296.586618 172.323828 \nL 301.175015 182.375102 \nL 304.233946 188.535912 \nL 307.292877 194.04897 \nL 310.351808 199.023474 \nL 316.46967 208.343845 \nL 321.058066 215.093344 \nL 324.116997 219.177203 \nL 327.175928 222.700831 \nL 330.234859 225.624791 \nL 333.29379 228.054724 \nL 336.352721 230.151933 \nL 339.411652 232.036602 \nL 342.470583 233.744982 \nL 345.529514 235.249156 \nL 348.588445 236.506382 \nL 351.647377 237.49841 \nL 354.706308 238.242119 \nL 357.765239 238.777539 \nL 362.353635 239.288546 \nL 366.942032 239.560855 \nL 374.589359 239.726567 \nL 379.177756 239.750125 \nL 379.177756 239.750125 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_28\">\n    <path d=\"M 59.595937 239.758125 \nL 59.595937 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path d=\"M 394.395938 239.758125 \nL 394.395938 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path d=\"M 59.595938 239.758125 \nL 394.395938 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path d=\"M 59.595938 22.318125 \nL 394.395938 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_17\">\n    <!-- Plot of biases initialized, with mean of 0.0 and standard deviation of 0.01 -->\n    <g transform=\"translate(7.2 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 5.515625 54.6875 \nL 48.1875 54.6875 \nL 48.1875 46.484375 \nL 14.40625 7.171875 \nL 48.1875 7.171875 \nL 48.1875 0 \nL 4.296875 0 \nL 4.296875 8.203125 \nL 38.09375 47.515625 \nL 5.515625 47.515625 \nz\n\" id=\"DejaVuSans-122\"/>\n      <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      <path d=\"M 11.71875 12.40625 \nL 22.015625 12.40625 \nL 22.015625 4 \nL 14.015625 -11.625 \nL 7.71875 -11.625 \nL 11.71875 4 \nz\n\" id=\"DejaVuSans-44\"/>\n      <path d=\"M 4.203125 54.6875 \nL 13.1875 54.6875 \nL 24.421875 12.015625 \nL 35.59375 54.6875 \nL 46.1875 54.6875 \nL 57.421875 12.015625 \nL 68.609375 54.6875 \nL 77.59375 54.6875 \nL 63.28125 0 \nL 52.6875 0 \nL 40.921875 44.828125 \nL 29.109375 0 \nL 18.5 0 \nz\n\" id=\"DejaVuSans-119\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-80\"/>\n     <use x=\"60.302734\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"88.085938\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"149.267578\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"188.476562\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"220.263672\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"281.445312\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"316.650391\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"348.4375\" xlink:href=\"#DejaVuSans-98\"/>\n     <use x=\"411.914062\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"439.697266\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"500.976562\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"553.076172\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"614.599609\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"666.699219\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"698.486328\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"726.269531\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"789.648438\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"817.431641\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"856.640625\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"884.423828\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"945.703125\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"973.486328\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1001.269531\" xlink:href=\"#DejaVuSans-122\"/>\n     <use x=\"1053.759766\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1115.283203\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"1178.759766\" xlink:href=\"#DejaVuSans-44\"/>\n     <use x=\"1210.546875\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1242.333984\" xlink:href=\"#DejaVuSans-119\"/>\n     <use x=\"1324.121094\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1351.904297\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1391.113281\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"1454.492188\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1486.279297\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"1583.691406\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1645.214844\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1706.494141\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1769.873047\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1801.660156\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1862.841797\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"1898.046875\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1929.833984\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"1993.457031\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"2025.244141\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"2088.867188\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2120.654297\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2181.933594\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"2245.3125\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"2308.789062\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2340.576172\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2392.675781\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"2431.884766\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2493.164062\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"2556.542969\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"2620.019531\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2681.298828\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"2720.662109\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"2784.138672\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2815.925781\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"2879.402344\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"2940.925781\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"3000.105469\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"3027.888672\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"3089.167969\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"3128.376953\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"3156.160156\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"3217.341797\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"3280.720703\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"3312.507812\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"3373.689453\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"3408.894531\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"3440.681641\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"3504.304688\" xlink:href=\"#DejaVuSans-46\"/>\n     <use x=\"3536.091797\" xlink:href=\"#DejaVuSans-48\"/>\n     <use x=\"3599.714844\" xlink:href=\"#DejaVuSans-49\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6d6bb2a402\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"59.595937\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Intialize bias with mean 0.5 and standard deviation of 10^-2\n",
    "bias = initialize_bias((1000,1))\n",
    "sns.distplot(bias)\n",
    "plt.title(\"Plot of biases initialized, with mean of 0.0 and standard deviation of 0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D,Activation, Dropout, GlobalAveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.applications import resnet50, vgg16, vgg19, xception, densenet, inception_v3, mobilenet, nasnet, inception_resnet_v2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.compat.v1.keras.applications import MobileNetV2\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger, EarlyStopping\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras import optimizers\n",
    "\n",
    "def get_siamese_model(input_shape):\n",
    "    \"\"\"\n",
    "        Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
    "                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation='relu',\n",
    "                     kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
    "    \n",
    "    #model.add(GlobalAveragePooling2D())\n",
    "    #model.add(Activation('sigmoid', name='predictions'))\n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D,Activation, Dropout, GlobalAveragePooling2D\\nfrom keras.models import Model, Sequential\\nfrom keras.applications import resnet50, vgg16, vgg19, xception, densenet, inception_v3, mobilenet, nasnet, inception_resnet_v2\\nfrom keras.preprocessing.image import ImageDataGenerator\\nfrom tensorflow.compat.v1.keras.applications import MobileNetV2\\nimport tensorflow as tf\\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger, EarlyStopping\\nfrom keras.applications.resnet50 import preprocess_input\\nfrom keras import optimizers\\n\\ndef get_siamese_model(input_shape):\\n\\n    # Two inputs one each - left and right image\\n    #left_input = Input((224,224,3))\\n    #right_input = Input((224,224,3))\\n    left_input = Input(input_shape)\\n    right_input = Input(input_shape)\\n    #Import Resnetarchitecture from keras application and initializing each layer with pretrained imagenet weights.\\n    Please note that it’s usually better to intialize the layers with imagenet initializations than random. While training I will be updating the weights for each layer in each epoch. we don’t want to confuse this activity with transfer learning as I am not freezing any layer but initilializing each layer with imagenet weights\\n\\n    convnet = MobileNetV2(weights=\\'imagenet\\', include_top=False, input_shape=input_shape)\\n    \\n    \\n    convnet.trainable = False\\n\\n    # Add the final fully connected layers\\n    x = convnet.output\\n    #x = convnet(inputs)\\n    #x = GlobalAveragePooling2D()(x)\\n    x = Flatten()(x)\\n    x = Dropout(0.5)(x)\\n    x = Dense(1024, activation=\"relu\")(x)\\n\\n    preds = Dense(2, activation=\\'sigmoid\\')(x) # Apply sigmoid\\n    convnet = Model(inputs=convnet.input, outputs=preds)\\n    #Applying above model for both the left and right images\\n    encoded_l = convnet(left_input)\\n    encoded_r = convnet(right_input)\\n    # Euclidian Distance between the two images or encodings through the Resnet-50 architecture\\n    Euc_layer = Lambda(lambda tensor:K.abs(tensor[0] - tensor[1]))\\n    # use and add the distance function\\n    Euc_distance = Euc_layer([encoded_l, encoded_r])\\n    #identify the prediction\\n    prediction = Dense(1,activation=\\'sigmoid\\')(Euc_distance)\\n\\n    #Define the network with the left and right inputs and the ouput prediction\\n    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\\n    #define the optimizer. Here I have used SGD with nesterov momentum\\n    for layer in convnet.layers[:100]:\\n\\t    layer.trainable = False\\n    for layer in convnet.layers[100:]:\\n        layer.trainable=True\\n    #convnet.trainable = False\\n    \\n    #optim = optimizers.SGD(lr=0.001, decay=.01, momentum=0.9, nesterov=True)\\n    #compile the network using binary cross entropy loss and the above optimizer\\n    #model.summary()\\n    #model.compile(loss=\"binary_crossentropy\",optimizer=optim,metrics=[\"accuracy\"])\\n\\n    return siamese_net'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "'''from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D,Activation, Dropout, GlobalAveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.applications import resnet50, vgg16, vgg19, xception, densenet, inception_v3, mobilenet, nasnet, inception_resnet_v2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.compat.v1.keras.applications import MobileNetV2\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger, EarlyStopping\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras import optimizers\n",
    "\n",
    "def get_siamese_model(input_shape):\n",
    "\n",
    "    # Two inputs one each - left and right image\n",
    "    #left_input = Input((224,224,3))\n",
    "    #right_input = Input((224,224,3))\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    #Import Resnetarchitecture from keras application and initializing each layer with pretrained imagenet weights.\n",
    "    Please note that it’s usually better to intialize the layers with imagenet initializations than random. While training I will be updating the weights for each layer in each epoch. we don’t want to confuse this activity with transfer learning as I am not freezing any layer but initilializing each layer with imagenet weights\n",
    "\n",
    "    convnet = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    \n",
    "    \n",
    "    convnet.trainable = False\n",
    "\n",
    "    # Add the final fully connected layers\n",
    "    x = convnet.output\n",
    "    #x = convnet(inputs)\n",
    "    #x = GlobalAveragePooling2D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "\n",
    "    preds = Dense(2, activation='sigmoid')(x) # Apply sigmoid\n",
    "    convnet = Model(inputs=convnet.input, outputs=preds)\n",
    "    #Applying above model for both the left and right images\n",
    "    encoded_l = convnet(left_input)\n",
    "    encoded_r = convnet(right_input)\n",
    "    # Euclidian Distance between the two images or encodings through the Resnet-50 architecture\n",
    "    Euc_layer = Lambda(lambda tensor:K.abs(tensor[0] - tensor[1]))\n",
    "    # use and add the distance function\n",
    "    Euc_distance = Euc_layer([encoded_l, encoded_r])\n",
    "    #identify the prediction\n",
    "    prediction = Dense(1,activation='sigmoid')(Euc_distance)\n",
    "\n",
    "    #Define the network with the left and right inputs and the ouput prediction\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    #define the optimizer. Here I have used SGD with nesterov momentum\n",
    "    for layer in convnet.layers[:100]:\n",
    "\t    layer.trainable = False\n",
    "    for layer in convnet.layers[100:]:\n",
    "        layer.trainable=True\n",
    "    #convnet.trainable = False\n",
    "    \n",
    "    #optim = optimizers.SGD(lr=0.001, decay=.01, momentum=0.9, nesterov=True)\n",
    "    #compile the network using binary cross entropy loss and the above optimizer\n",
    "    #model.summary()\n",
    "    #model.compile(loss=\"binary_crossentropy\",optimizer=optim,metrics=[\"accuracy\"])\n",
    "\n",
    "    return siamese_net'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 105, 105, 1) 0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 105, 105, 1) 0                                            \n__________________________________________________________________________________________________\nsequential (Sequential)         (None, 4096)         38947648    input_1[0][0]                    \n                                                                 input_2[0][0]                    \n__________________________________________________________________________________________________\nlambda (Lambda)                 (None, 4096)         0           sequential[0][0]                 \n                                                                 sequential[1][0]                 \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 1)            4097        lambda[0][0]                     \n==================================================================================================\nTotal params: 38,951,745\nTrainable params: 38,951,745\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model((105, 105, 1))\n",
    "#model = get_siamese_model((150,150,3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optim = SGD(lr=0.00005, decay=0.01, momentum=0.9, nesterov=True)\n",
    "optim = Adam(lr=0.00006)\n",
    "\n",
    "#compile the network using binary cross entropy loss and the above optimizer\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optim, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing function and loss function\n",
    "The training was initially done using Stochastic gradient descent(as described in the paper) with a learning rate 0.0005(choosen randomly between 0.01 and 0.00001) initially, due to which the convergence of model was very slow, after 3000 iterations the validation decreased from .47 to .43, where when the training was done using Adam's algorithm to optimize the training process even though with a smaller learning rate of 0.00006, the decrease in the validation loss was much faster as compared to simple Stochastic Gradient Descent.\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Image Verification and Recognition\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(path):\n",
    "    \"\"\"\n",
    "        Plot all 20 samples of a particular character of a language\n",
    "    \"\"\"\n",
    "    f, axarr = plt.subplots(5,4, figsize=(10,10))\n",
    "    images_list = []\n",
    "    for image in os.listdir(path):\n",
    "        image_path = os.path.join(path, image)\n",
    "        img = cv2.imread(image_path)\n",
    "        images_list.append(img)\n",
    "    for i in range(5):\n",
    "        for j in range(4):\n",
    "            axarr[i,j].imshow(images_list.pop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'plot_images(os.path.join(data_path, \\'images_evaluation/VehicleAudio&VideoInstallation/InterconnectCables\\'))\\nprint(\"Underwater Photography, 20 samples of the Housing category.\")'"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "'''plot_images(os.path.join(data_path, 'images_evaluation/VehicleAudio&VideoInstallation/InterconnectCables'))\n",
    "print(\"Underwater Photography, 20 samples of the Housing category.\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Arcadian language, 20 samples of the third character.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 720x720 with 20 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"578.031038pt\" version=\"1.1\" viewBox=\"0 0 576.434291 578.031038\" width=\"576.434291pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-01-24T03:35:17.812932</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 578.031038 \nL 576.434291 578.031038 \nL 576.434291 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 104.277051 \nL 127.011638 104.277051 \nL 127.011638 10.552913 \nL 33.2875 10.552913 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p7672219432)\">\n    <image height=\"94\" id=\"image4412f87ee1\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"33.2875\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAFVUlEQVR4nO2cSywzURSAz0y1FY8UaSwIC0FYSCxULLDAAisSCzuPhAgrBEkjEhthR4JFxYLEwiOxIbGwZVMSsZA0JB4Lj8RrCE2j0/Mv/vyN/qU67rSn07lfMou51TPHNzNn7r2dGQEREThRR6ROQK9w8URw8URw8URw8URw8URw8URw8URw8URw8URw8URw8URw8URw8UQksHy5oaEBLi8vmRLY2tqCvLw8phiaBBXg8/lwenoa09PTMT09HUVRRABgWiwWCz49PSlJIy5QJH5jYwMFQWCW/f9iMBhQkqRI/Y8xiaIaj3931G9OrJDIsgzZ2dmqx41lwq7xr6+vcHJy8uVnVVVVijd8dnYGNzc3ir8XN4R7ajidzqAS0dzcjO3t7SjLsuJTraenJyBWSkqK4hhahqlXMzk5CYWFhcw7X48w9eMFQVArD93BJB75nSG/JmaO+Le3N6isrFQtXqxDdsTPzMxAbW1tQKzr62vwer0sKWkGsrkak8kEGRkZYDAY/G3n5+dQV1dHlVJUIS01a2trkJ+fzxRDq5BfXPXaM+LTwkTETK9Gb5CXGr2OBchLjV7PGnLxeoXXeCLIa7xeiblSI0kSnJ6eUqcRcWJO/NHREUxMTFCnEXHIa7xeyxWT+KGhIXh/f2dKYGpqCpaWlvw7saioCLq6uphiagEBwzzkDg4OwGazBbU/Pz+DxWJhSgIRYW9vDwAALBYLlJSUMMXTAmGL93g8sLi4CH19fQHtaojXI2GXGrPZDJmZmZHMRVfEXK9GL3DxRHDxRHDxRHDxRHDxRHDxRHDxRHDxRHDxRHDxRHDxRDA9EULNzs4OOJ1O/7rJZIKRkRHCjMJH0+K3t7dhdnbWv56UlKQZ8bzUEMEsvrS0FHw+nxq56Apm8RcXF5CTkwOyLEd9BwiCAKIo+pfPDznEOorEG41GSExMDGq/vr4Go9EINTU1zD9+K2FmZgY+Pj78y/Pzc9S2zYzSB2MdDgcWFRVhUlLSl+8laGlpwfv7e5Uew41fFIv/x/Dw8LfyOzs78ebmRs08445fi0dEHBsbQ5PJ9KX81dVVtXKMS5j68ePj42C1WuHl5QVGR0dZQumOsO+rCYUkSZCWlhbQZrPZYGVlBQoKCljDxyURG0A5nU54enqKVHjNo4r41NRU2N/fD2pvaGjQ9ztpQqCKeFEUoaKiAnZ2dgLaHx8f+aj2G1QrNYIgQEJC8LX69vZWt7dihyLis5NlZWXgdru/HPH+4/DwEGRZ/jGOKMbPnJ6q4jMzM6GsrAwODg5C/p3L5YKjoyP/emdn549TDcvLy5CamgpNTU0qZBoDqD0wcDgcQYMpt9vt/9zlcmFjY+OvXpOYnJyMExMTuL6+rnbaUSdqP4RIkgSDg4NweXkJu7u7v4rx9vYGdrsdcnNzwev1Qmtrq8pZRo+oiK+vrwev1+t/6oOVq6srGBoaArPZDM3NzarEjDaqjFw/s7CwAN3d3Yq/53K5vn2Yrbq6Gm5vb4ParVarf8TsdDqDRs8xjdq1y+PxYH9//4/1WhRF3NzcREmSUJIk9Pl838Z8eXnBh4eHkK/X1dpUtOriEf++/LmtrQ0NBkOQoISEBLTb7SjLckjZX8W8u7vDlJQULv4nbDZbgByz2Yy9vb1MMU9OTjArK0vz4iN6cS0vL4fj42PweDxgMpmgo6MD5ubmmGIWFxfD+vo6jI2NBbQbjUamuFEn0ns2KysLRVHEwcHBSG9KU6jeq/mf+fl5cLvdMDAwwF+z8omIi+d8TfzMOmkMLp4ILp4ILp4ILp4ILp4ILp4ILp4ILp4ILp4ILp6IP5eQrVQdI6K6AAAAAElFTkSuQmCC\" y=\"-10.277051\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m5544d91069\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.733805\" xlink:href=\"#m5544d91069\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(30.552555 118.875489)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"78.364347\" xlink:href=\"#m5544d91069\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <g transform=\"translate(72.001847 118.875489)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"122.994889\" xlink:href=\"#m5544d91069\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <g transform=\"translate(113.451139 118.875489)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_4\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"md0590c5d67\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#md0590c5d67\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#md0590c5d67\" y=\"55.629761\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 59.428979)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#md0590c5d67\" y=\"100.260302\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 104.059521)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 104.277051 \nL 33.2875 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 127.011638 104.277051 \nL 127.011638 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 104.277051 \nL 127.011638 104.277051 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 10.552913 \nL 127.011638 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 178.852717 104.277051 \nL 272.576855 104.277051 \nL 272.576855 10.552913 \nL 178.852717 10.552913 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p57b2f2dac4)\">\n    <image height=\"94\" id=\"imaged858c61f10\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"178.852717\" xlink:href=\"data:image/png;base64,\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\" y=\"-10.277051\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.299023\" xlink:href=\"#m5544d91069\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(176.117773 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"223.929565\" xlink:href=\"#m5544d91069\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 50 -->\n      <g transform=\"translate(217.567065 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.560107\" xlink:href=\"#m5544d91069\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 100 -->\n      <g transform=\"translate(259.016357 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#md0590c5d67\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0 -->\n      <g transform=\"translate(165.490217 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#md0590c5d67\" y=\"55.629761\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 50 -->\n      <g transform=\"translate(159.127717 59.428979)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#md0590c5d67\" y=\"100.260302\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 100 -->\n      <g transform=\"translate(152.765217 104.059521)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 178.852717 104.277051 \nL 178.852717 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 272.576855 104.277051 \nL 272.576855 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 178.852717 104.277051 \nL 272.576855 104.277051 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 178.852717 10.552913 \nL 272.576855 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g id=\"patch_12\">\n    <path d=\"M 324.417935 104.277051 \nL 418.142073 104.277051 \nL 418.142073 10.552913 \nL 324.417935 10.552913 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pa920e3822b)\">\n    <image height=\"94\" id=\"imaged74de8274b\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"324.417935\" xlink:href=\"data:image/png;base64,\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\" y=\"-10.277051\"/>\n   </g>\n   <g id=\"matplotlib.axis_5\">\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.86424\" xlink:href=\"#m5544d91069\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0 -->\n      <g transform=\"translate(321.68299 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.494782\" xlink:href=\"#m5544d91069\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 50 -->\n      <g transform=\"translate(363.132282 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"414.125324\" xlink:href=\"#m5544d91069\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 100 -->\n      <g transform=\"translate(404.581574 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_6\">\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#md0590c5d67\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0 -->\n      <g transform=\"translate(311.055435 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#md0590c5d67\" y=\"55.629761\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 50 -->\n      <g transform=\"translate(304.692935 59.428979)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#md0590c5d67\" y=\"100.260302\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 100 -->\n      <g transform=\"translate(298.330435 104.059521)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 324.417935 104.277051 \nL 324.417935 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 418.142073 104.277051 \nL 418.142073 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 324.417935 104.277051 \nL 418.142073 104.277051 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 324.417935 10.552913 \nL 418.142073 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_4\">\n   <g id=\"patch_17\">\n    <path d=\"M 469.983152 104.277051 \nL 563.70729 104.277051 \nL 563.70729 10.552913 \nL 469.983152 10.552913 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pdaf1a56068)\">\n    <image height=\"94\" id=\"image4d6a5204ba\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"469.983152\" xlink:href=\"data:image/png;base64,\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\" y=\"-10.277051\"/>\n   </g>\n   <g id=\"matplotlib.axis_7\">\n    <g id=\"xtick_10\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"470.429458\" xlink:href=\"#m5544d91069\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 0 -->\n      <g transform=\"translate(467.248208 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"515.059999\" xlink:href=\"#m5544d91069\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 50 -->\n      <g transform=\"translate(508.697499 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"559.690541\" xlink:href=\"#m5544d91069\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 100 -->\n      <g transform=\"translate(550.146791 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_8\">\n    <g id=\"ytick_10\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#md0590c5d67\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 0 -->\n      <g transform=\"translate(456.620652 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#md0590c5d67\" y=\"55.629761\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 50 -->\n      <g transform=\"translate(450.258152 59.428979)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#md0590c5d67\" y=\"100.260302\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 100 -->\n      <g transform=\"translate(443.895652 104.059521)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 469.983152 104.277051 \nL 469.983152 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path d=\"M 563.70729 104.277051 \nL 563.70729 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path d=\"M 469.983152 104.277051 \nL 563.70729 104.277051 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path d=\"M 469.983152 10.552913 \nL 563.70729 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_5\">\n   <g id=\"patch_22\">\n    <path d=\"M 33.2875 216.746017 \nL 127.011638 216.746017 \nL 127.011638 123.021879 \nL 33.2875 123.021879 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p29b92e3b0c)\">\n    <image height=\"94\" id=\"image850b884f17\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"33.2875\" xlink:href=\"data:image/png;base64,\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\" y=\"-122.746017\"/>\n   </g>\n   <g id=\"matplotlib.axis_9\">\n    <g id=\"xtick_13\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.733805\" xlink:href=\"#m5544d91069\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 0 -->\n      <g transform=\"translate(30.552555 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"78.364347\" xlink:href=\"#m5544d91069\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 50 -->\n      <g transform=\"translate(72.001847 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"122.994889\" xlink:href=\"#m5544d91069\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_27\">\n      <!-- 100 -->\n      <g transform=\"translate(113.451139 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_10\">\n    <g id=\"ytick_13\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#md0590c5d67\" y=\"123.468184\"/>\n      </g>\n     </g>\n     <g id=\"text_28\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 127.267403)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#md0590c5d67\" y=\"168.098726\"/>\n      </g>\n     </g>\n     <g id=\"text_29\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 171.897945)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#md0590c5d67\" y=\"212.729268\"/>\n      </g>\n     </g>\n     <g id=\"text_30\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 216.528487)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_23\">\n    <path d=\"M 33.2875 216.746017 \nL 33.2875 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path d=\"M 127.011638 216.746017 \nL 127.011638 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path d=\"M 33.2875 216.746017 \nL 127.011638 216.746017 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path d=\"M 33.2875 123.021879 \nL 127.011638 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_6\">\n   <g id=\"patch_27\">\n    <path d=\"M 178.852717 216.746017 \nL 272.576855 216.746017 \nL 272.576855 123.021879 \nL 178.852717 123.021879 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p84e82fc35c)\">\n    <image height=\"94\" id=\"image3e91287ed1\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"178.852717\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAFnUlEQVR4nO2cSywrXxjAv76piCCRiBvx6AKZjUi8yoZrgb0E8YjHQkIkhAgSFljYIOKxuHHFCgsWQm5uci+xspIQC2IjRDSkaKlqm/a7ixv+xtTf447zTTm/ZBZzpj3fN7+ZOWfmzGlViIjAYY6aOoHPChdPBBdPBBdPBBdPBBdPBBdPBBdPBBdPBBdPBBdPBBdPBBdPBBdPBBdPBBdPBBdPBBdPBBdPBBdPBBdPBBdPBBdPBBdPxIcRj4hgs9nAaDRCXl4eeDwe8Hq91Gk9yYcRb7VaITIyEpxOJ6yuroJer4e+vj7qtJ7kQ4jf29sDk8kkOcMtFgtcXFwQZfUMGOBsbGxgbGwsAoDfpaWlBa1WK3WaEgJW/ObmJg4PD2NycvKT0u+W5uZmtNls1CmLCDjxR0dH2NjYiDk5Oc8Kf7hUVVWhy+WiTv+egBHv8XiwqKjoWeFRUVE4ODjod5vD4aDejXsCRnxqauqzZ3VwcDDu7e3h9fU1DgwMSLYLgoA+n496VxAxAMT7fD4UBOFZ6fv7+6JO9Pb2Ftva2iSfM5lMhHvzH4q9nXS73eBwOCArKwt2dnZE2zQaDRiNRjAajbC8vAwejwcSExMhIiLi/jMGgwHCwsJAo9GIvquY20vqI++Pq6srbGxs9Htm63Q6rK2tfXFd5eXlou+Hh4fj8fHxO2b/MhQn/vLyEjs7O/1K12q1WFlZ+ar6ent7MTQ0VNLWU6Mo8Xa7Hbu6uvxKV6lU2NDQ8KZ6s7OzRXV9+fIFf//+LXP2r0Mx4m9vb7G5ufnJzrO7u/vNdyTfvn3DsLAwUX3FxcUy78HrUIR4n8+HpaWlfoX39PTgzMwMejyef4rxeFghJiYGZ2ZmZNqD10MuvqysDM1ms1/pvb29aLfbZYnjbzynqalJlrrfAqn4iooK1Ol0fqW3trbi1dWVbLEODw8xKChIFCM0NBS/f/8uW4zXQCLe6/ViU1MTajQav9IrKyvfZVzl8vJSEmtkZET2OC9B+4+PAW9iaGgIRkdHJeVqtRqKiopgenoaVCqV7HFDQkIkZQ6HA9xuN+j1etnj/R/Mn1ztdjtYLBZJeUpKChQUFMDS0tK7SL8jKSlJtN7Z2Qk/f/58t3hPwvoSW1hYkFzuBQUF6PV6mcQ/OzuTxF9aWmIS+yFMz/jT01NYWVmRlM/NzYFaTTNslJubC3FxcewDszrCNpsNy8rK/Ham5+fnrNLAm5sbbG9vJ+9cVYhs/q/GYrFAdHS0pHxsbAzq6+tBp9OxSAMA/vYza2trAAAgCAIkJCQwi30PqyN8cnIiOdMnJibQ6XSySkFRMGlY3W43CIIgKY+Pj4egoCAWKSgOZj2a1WoVret0OslLis8E2RuokZER+Pr1K1V4chT76u+jw8UTwUT84OCgaD0jIwPS0tJYhFYsTMQ/nrVrNpshPT2dRWjFwpsaIrh4Irh4Irh4IkjEn5+fK2cqHRFMxOfn54vWp6enYXZ2lkVoxcLknevi4iIYDAYWoV6E1WqF+fl5UVlJSQlERkayS4LFEKjL5ZIMCefn5+Pu7i6L8BK2t7cl+WxtbTHNgUlTo9VqYWpqSlT269cvODg4YBFekTARr1arobCwkEWogIHfThJBMqGJGkEQwOVyicq0WrYqPqV4lUrFfObYY3hTQwSp+PX1dbDb7ZQpkEHa1AwMDIDT6YSoqChob28nm01GAfmEpjtqampkm6xqMpmgo6NDlrreDVZPai6XCycnJ5/9obAci9lsZrVbb4bZta3X66G6uhrGx8dZhVQ0TBtVg8EAdXV10N/fD2q1+n75jDBr4x+Cf38CdL8eHx8PNptNtvozMzPhx48fstX3HpCI5/AHKDK4eCK4eCK4eCK4eCK4eCK4eCK4eCK4eCK4eCK4eCK4eCK4eCK4eCK4eCL+AJ9c4R0CM7grAAAAAElFTkSuQmCC\" y=\"-122.746017\"/>\n   </g>\n   <g id=\"matplotlib.axis_11\">\n    <g id=\"xtick_16\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.299023\" xlink:href=\"#m5544d91069\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_31\">\n      <!-- 0 -->\n      <g transform=\"translate(176.117773 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"223.929565\" xlink:href=\"#m5544d91069\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_32\">\n      <!-- 50 -->\n      <g transform=\"translate(217.567065 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.560107\" xlink:href=\"#m5544d91069\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_33\">\n      <!-- 100 -->\n      <g transform=\"translate(259.016357 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_12\">\n    <g id=\"ytick_16\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#md0590c5d67\" y=\"123.468184\"/>\n      </g>\n     </g>\n     <g id=\"text_34\">\n      <!-- 0 -->\n      <g transform=\"translate(165.490217 127.267403)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#md0590c5d67\" y=\"168.098726\"/>\n      </g>\n     </g>\n     <g id=\"text_35\">\n      <!-- 50 -->\n      <g transform=\"translate(159.127717 171.897945)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#md0590c5d67\" y=\"212.729268\"/>\n      </g>\n     </g>\n     <g id=\"text_36\">\n      <!-- 100 -->\n      <g transform=\"translate(152.765217 216.528487)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_28\">\n    <path d=\"M 178.852717 216.746017 \nL 178.852717 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path d=\"M 272.576855 216.746017 \nL 272.576855 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path d=\"M 178.852717 216.746017 \nL 272.576855 216.746017 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path d=\"M 178.852717 123.021879 \nL 272.576855 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_7\">\n   <g id=\"patch_32\">\n    <path d=\"M 324.417935 216.746017 \nL 418.142073 216.746017 \nL 418.142073 123.021879 \nL 324.417935 123.021879 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p36ec68eb19)\">\n    <image height=\"94\" id=\"image5cf69e0c06\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"324.417935\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAEN0lEQVR4nO2bP0gCfRjHv3eEliFlJRRBDoFndNiWUblIQ5BEc1MuETUVRFsONbVUS2ONLW221iKCS0RoBA0V/YECixoSG+z3DpFkZ+/ra3f3HN3zAYf7oc/z8OHxud95niSEEGBMR6YuwK6weCJYPBEsnggWTwSLJ4LFE8HiiWDxRLB4Ilg8ESyeCBZPBIsngsUTweKJYPFEsHgiWDwRLJ4IFk9EHXUBX3l4eMDCwoLucePxOPx+v+5xf4Nklb935PN5DA8P4/j4WPfYqqrC7Xbj8PAQ9fX1usevBUuIF0LA5/Ph5ubG0Dw+nw+Xl5eQJMnQPNVgCfEdHR24v783JVdnZyeurq5QV0c7ZS0h3uPx4Pn5uXTsdDrhdrt/HTefzyOfz2vWA4EA0uk0mpqafp2jZoQFaG5uFgBKr6mpKV3ibm1tiWAwKFwuV1l8ACIcDovb21td8tTCnxb/yeLiomhoaNDIj0aj4uLiQtdc1WKLffza2hqWlpY0c31/fx+JRIKkJnLxq6urFeew3sTjcWxsbGjW9/b2kM1mDc+vgeR79gVFUcq+/qqqipOTE0NyFYtFsbu7qxk5iUTCkHz/BnnHf99Tt7a2IhgMGpJLlmWMjIxo1ufm5nB+fm5Izh9rMTWbBWhpaUEymSxbu76+RqFQMLUO24mXZbni/r1QKECYeEljO/EA4HA44PV6y9ZCoRByuZxpNdhSvKIo2N7eJq3BluKtALl4M+fqV/x+P0ZHR0vHMzMzcLlcpuUnvxFC9ROt3+/H+vo6JicnAQDRaBSNjY2m5ScXT9XxwMevlIFAgCQ3+aixwk0JCsjFU3Y8JeTiueMZUyEXz6OGCB41RHDHE8EdTwR3PGMqLJ4IcvE844ngGc+YCrl4HjVE2HXUkN8I0avjhRCIRCIoFoultVgshlgspkt8vSEXryfJZLJMfCaTgcfjwcTEBF1RP0A+aozk5eUFr6+v1GVU5E+LX15eLt3Mthp/atQoioL393cIISBJErxer2V3TeTiv+9qnp6ekM1moarq/4ojSRJOT0/1LM1QyEfN947MZDKYnp7G0dERUUXmQP7U387ODmZnZzV/kx4aGkIoFKr4GVVVLbtNrBZy8cDHs0jj4+NVX0y1t7djYGCgdLy5uYmuri6jyjMES4gXQiCVSiEcDtf0+Z6eHs2j8mNjY1hZWdGjPEMgP7kCH3N+cHAQ6XS6rJOr5ezsTLPW29urR2mGQX5y/USWZfT39+Pg4ABOp7Piy+FwUJepG5bo+E8kSUIkEvnxeaRcLoe+vr6ytcfHR7y9vZlRnq5YSvx/0dbWhru7u7K1+fl5pFIpzXu7u7vNKqsmLHFytSOWmfF2g8UTweKJYPFEsHgiWDwRLJ4IFk8EiyeCxRPB4olg8USweCJYPBEsnggWTwSLJ4LFE8HiiWDxRLB4Ilg8Ef8AW04KenqFAiUAAAAASUVORK5CYII=\" y=\"-122.746017\"/>\n   </g>\n   <g id=\"matplotlib.axis_13\">\n    <g id=\"xtick_19\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.86424\" xlink:href=\"#m5544d91069\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_37\">\n      <!-- 0 -->\n      <g transform=\"translate(321.68299 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_20\">\n     <g id=\"line2d_38\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.494782\" xlink:href=\"#m5544d91069\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_38\">\n      <!-- 50 -->\n      <g transform=\"translate(363.132282 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_21\">\n     <g id=\"line2d_39\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"414.125324\" xlink:href=\"#m5544d91069\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_39\">\n      <!-- 100 -->\n      <g transform=\"translate(404.581574 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_14\">\n    <g id=\"ytick_19\">\n     <g id=\"line2d_40\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#md0590c5d67\" y=\"123.468184\"/>\n      </g>\n     </g>\n     <g id=\"text_40\">\n      <!-- 0 -->\n      <g transform=\"translate(311.055435 127.267403)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_41\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#md0590c5d67\" y=\"168.098726\"/>\n      </g>\n     </g>\n     <g id=\"text_41\">\n      <!-- 50 -->\n      <g transform=\"translate(304.692935 171.897945)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_42\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#md0590c5d67\" y=\"212.729268\"/>\n      </g>\n     </g>\n     <g id=\"text_42\">\n      <!-- 100 -->\n      <g transform=\"translate(298.330435 216.528487)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 324.417935 216.746017 \nL 324.417935 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 418.142073 216.746017 \nL 418.142073 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 324.417935 216.746017 \nL 418.142073 216.746017 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 324.417935 123.021879 \nL 418.142073 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_8\">\n   <g id=\"patch_37\">\n    <path d=\"M 469.983152 216.746017 \nL 563.70729 216.746017 \nL 563.70729 123.021879 \nL 469.983152 123.021879 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p192844acd5)\">\n    <image height=\"94\" id=\"image5d051386c5\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"469.983152\" xlink:href=\"data:image/png;base64,\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\" y=\"-122.746017\"/>\n   </g>\n   <g id=\"matplotlib.axis_15\">\n    <g id=\"xtick_22\">\n     <g id=\"line2d_43\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"470.429458\" xlink:href=\"#m5544d91069\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_43\">\n      <!-- 0 -->\n      <g transform=\"translate(467.248208 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_23\">\n     <g id=\"line2d_44\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"515.059999\" xlink:href=\"#m5544d91069\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_44\">\n      <!-- 50 -->\n      <g transform=\"translate(508.697499 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_24\">\n     <g id=\"line2d_45\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"559.690541\" xlink:href=\"#m5544d91069\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_45\">\n      <!-- 100 -->\n      <g transform=\"translate(550.146791 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_16\">\n    <g id=\"ytick_22\">\n     <g id=\"line2d_46\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#md0590c5d67\" y=\"123.468184\"/>\n      </g>\n     </g>\n     <g id=\"text_46\">\n      <!-- 0 -->\n      <g transform=\"translate(456.620652 127.267403)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_47\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#md0590c5d67\" y=\"168.098726\"/>\n      </g>\n     </g>\n     <g id=\"text_47\">\n      <!-- 50 -->\n      <g transform=\"translate(450.258152 171.897945)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_24\">\n     <g id=\"line2d_48\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#md0590c5d67\" y=\"212.729268\"/>\n      </g>\n     </g>\n     <g id=\"text_48\">\n      <!-- 100 -->\n      <g transform=\"translate(443.895652 216.528487)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_38\">\n    <path d=\"M 469.983152 216.746017 \nL 469.983152 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_39\">\n    <path d=\"M 563.70729 216.746017 \nL 563.70729 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_40\">\n    <path d=\"M 469.983152 216.746017 \nL 563.70729 216.746017 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_41\">\n    <path d=\"M 469.983152 123.021879 \nL 563.70729 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_9\">\n   <g id=\"patch_42\">\n    <path d=\"M 33.2875 329.214982 \nL 127.011638 329.214982 \nL 127.011638 235.490844 \nL 33.2875 235.490844 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pb68fe24773)\">\n    <image height=\"94\" id=\"image37d0c2e99c\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"33.2875\" xlink:href=\"data:image/png;base64,\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\" y=\"-235.214982\"/>\n   </g>\n   <g id=\"matplotlib.axis_17\">\n    <g id=\"xtick_25\">\n     <g id=\"line2d_49\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.733805\" xlink:href=\"#m5544d91069\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_49\">\n      <!-- 0 -->\n      <g transform=\"translate(30.552555 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_26\">\n     <g id=\"line2d_50\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"78.364347\" xlink:href=\"#m5544d91069\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_50\">\n      <!-- 50 -->\n      <g transform=\"translate(72.001847 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_27\">\n     <g id=\"line2d_51\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"122.994889\" xlink:href=\"#m5544d91069\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_51\">\n      <!-- 100 -->\n      <g transform=\"translate(113.451139 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_18\">\n    <g id=\"ytick_25\">\n     <g id=\"line2d_52\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#md0590c5d67\" y=\"235.93715\"/>\n      </g>\n     </g>\n     <g id=\"text_52\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 239.736369)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_26\">\n     <g id=\"line2d_53\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#md0590c5d67\" y=\"280.567692\"/>\n      </g>\n     </g>\n     <g id=\"text_53\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 284.36691)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_27\">\n     <g id=\"line2d_54\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#md0590c5d67\" y=\"325.198234\"/>\n      </g>\n     </g>\n     <g id=\"text_54\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 328.997452)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_43\">\n    <path d=\"M 33.2875 329.214982 \nL 33.2875 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_44\">\n    <path d=\"M 127.011638 329.214982 \nL 127.011638 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_45\">\n    <path d=\"M 33.2875 329.214982 \nL 127.011638 329.214982 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_46\">\n    <path d=\"M 33.2875 235.490844 \nL 127.011638 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_10\">\n   <g id=\"patch_47\">\n    <path d=\"M 178.852717 329.214982 \nL 272.576855 329.214982 \nL 272.576855 235.490844 \nL 178.852717 235.490844 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p14a02e5372)\">\n    <image height=\"94\" id=\"image10a1a848e5\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"178.852717\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAFJklEQVR4nO2dMUgbbRjH/1e1miAhNQ6pmFRrOzhU0BqLiGjEoa1QCtJBuzq4iXZy6qDgqIIUBB0sFnGxtFBdHQQxIII4pCBSbJtFiEkwubSaezp83yeml7TGS/Kc+Z4f3HB3uff+7+/evHe5e48oREQQ8s4N7gD/V0Q8EyKeCRHPhIhnQsQzIeKZEPFMiHgmRDwTIp4JEc+EiGdCxDMh4pkQ8UyIeCZEPBMinoli7gCXRdM0vH37NuPt2tvbUVdXl4NExjC9+NnZWXz79g2JRAITExMZbev1evHw4cMcJTMImZTl5WXq7+8nh8NBADKe2traaHd3l7saaTFli//06RNGRkbw/fv3K5dRW1uLBw8eZDFVluE+8r+zublJFRUVV2rlFyer1Uoul4tWVla4q5QSU13VEBGi0SiCwaBuXXFxMZxOJ6LRqG5K1bJjsRi+fv2KSCQCTdPyET8zuI/8fyQSCfL5fLqWa7fbqbKykmKxGGmalnJbTdPo/v37aVv/xsZGnmvzd0zR4okIW1tbaGlp0a1bW1vD0dERLBYLFEVJub2iKPj8+TMaGhpSrt/f38fPnz+zmtkw3EeeiGh1dZUURdG11KamJvL7/ZcuR1VV6urqStnqv3z5ksMaZA67+IWFBbJYLDpRra2ttL29nXF5BwcHIv5vzMzMkN1u10l69OgR+Xy+K5V5XcSz9vEfP35EKBTSLa+vr4fH48l/oDxiipPrRZqbmzE+Ps4dI/dwfdVevXpFJSUlSd3BnTt3KBwOGyo3XVdjt9spGo2mvSTNN2wtPhqN4vT0NGlZeXk5bDaboXJramqwvr6uWx4KhWCz2eB0Og2Vny1M09WUl5djb2/PcDmKosBqtcLhcOjWJRIJ3L592/A+soFpxGcTj8eDxcVFuFyupOVtbW3Y2dlhSpWMKe9OZoPHjx9jcnIS79+/P182Pz+f9tdvvilY8QDQ29uL3t5e7hgpKciu5jog4pkQ8UyIeCZEPBMingkRz4SIZ0LEMyHimRDxTIh4JkQ8EyKeiYK+LXxZ/H4/jo+Pz+dLS0vR1NSU251yPewdHBxMehhdVlZGHz58YMnS09OTlMXtdud8n2xdzZMnT3D37t3z+Xg8juHhYa44eYdN/LNnz3Dv3j2u3bMjJ1cmTHVyPTw8RF9fH5aWlvK637m5OcRisfP54uLca2EV//sT/7OzMwQCAWiahhs38vdlZBnklPPT9x/QNI08Ho9ubPzLly9JVVXOaDmHfXw8EaUcqj00NGR4HKWZMUUf7/V6kwYeAcD09DSsVis6OjouXU5RURG6u7uzHS83cB95on+6nIGBAcOvWN68eZPevHlj2lcsL2IK8UREP378oNHRUcPyAVBVVRW9e/eOu0p/xDTiiYhOTk5obGwsK/JdLhc9f/6c1tfXuauVElOJJyKKRCL0+vXrrMjHvy87NDY2UiAQ4K5aEgqR+f6qQlVVhMPhjLerrq5GIpFIua6yshKHh4ewWCxG42UFU4q/CkSEcDgMp9MJTdN0b5sAgMViQTAYRFlZGUPCZArmXo2iKLDb7YjH49jY2MCtW7d0n1FVFdXV1Qzp9BSM+Iu0tLRgcXERVVVV3FHSUpDiAeDp06eYmpqC2+3mjpKSghUPAC9evEB7ezt3jJSY4pZBLhkYGEBnZ+f5vBlOrEABXdVcNwq6qzEzIp4JEc+EiGdCxDMh4pkQ8UyIeCZEPBMingkRz4SIZ0LEMyHimRDxTIh4JkQ8EyKeCRHPxC9VshiPq4xUjQAAAABJRU5ErkJggg==\" y=\"-235.214982\"/>\n   </g>\n   <g id=\"matplotlib.axis_19\">\n    <g id=\"xtick_28\">\n     <g id=\"line2d_55\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.299023\" xlink:href=\"#m5544d91069\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_55\">\n      <!-- 0 -->\n      <g transform=\"translate(176.117773 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_29\">\n     <g id=\"line2d_56\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"223.929565\" xlink:href=\"#m5544d91069\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_56\">\n      <!-- 50 -->\n      <g transform=\"translate(217.567065 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_30\">\n     <g id=\"line2d_57\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.560107\" xlink:href=\"#m5544d91069\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_57\">\n      <!-- 100 -->\n      <g transform=\"translate(259.016357 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_20\">\n    <g id=\"ytick_28\">\n     <g id=\"line2d_58\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#md0590c5d67\" y=\"235.93715\"/>\n      </g>\n     </g>\n     <g id=\"text_58\">\n      <!-- 0 -->\n      <g transform=\"translate(165.490217 239.736369)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_29\">\n     <g id=\"line2d_59\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#md0590c5d67\" y=\"280.567692\"/>\n      </g>\n     </g>\n     <g id=\"text_59\">\n      <!-- 50 -->\n      <g transform=\"translate(159.127717 284.36691)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_30\">\n     <g id=\"line2d_60\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#md0590c5d67\" y=\"325.198234\"/>\n      </g>\n     </g>\n     <g id=\"text_60\">\n      <!-- 100 -->\n      <g transform=\"translate(152.765217 328.997452)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_48\">\n    <path d=\"M 178.852717 329.214982 \nL 178.852717 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_49\">\n    <path d=\"M 272.576855 329.214982 \nL 272.576855 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_50\">\n    <path d=\"M 178.852717 329.214982 \nL 272.576855 329.214982 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_51\">\n    <path d=\"M 178.852717 235.490844 \nL 272.576855 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_11\">\n   <g id=\"patch_52\">\n    <path d=\"M 324.417935 329.214982 \nL 418.142073 329.214982 \nL 418.142073 235.490844 \nL 324.417935 235.490844 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p4f6c731ded)\">\n    <image height=\"94\" id=\"image1c42728209\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"324.417935\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAEUElEQVR4nO3aP0hqfxjH8c9RSftnB4qWcChKbCgJCmopIoQKC1oaaqiwKKgpLg1Nzi1tEeEUDW01NNRQRESJUwT2DySphrAlTLIEPXf6yc/bn2vqOc/X6/OChr4cz3l8Y8eTHklRFAVMczrqAQoVhyfC4YlweCIcngiHJ8LhiXB4IhyeCIcnwuGJcHgiHJ4IhyfC4YlweCIcngiHJ8LhiXB4IhyeCIcnwuGJcHgiHJ4IhydioB4g12KxGC4vL9PevqamBlVVVSpO9Ll/Jvze3h5isRhCoRAmJyfTftzU1BQGBgbQ1dUFs9ms4oSppHy8aTUQCGBnZydlbXFxEa+vrxnvc2FhAW63G8XFxdmOlx4lz9zf3yv9/f0KgJz/hEIhzZ5H3pxqhoaG8P7+jpeXFxwfH1OPkzWhw//69Qu7u7sAgIuLCyhpnhVbWlqwvr7+7TZHR0eYnZ3NesZMCRNeURQEg0G0trYm1yKRCGKxWFqPlyQJj4+P0Ov1MBgMqKio+Hb7YDCYzbhZEyJ8IpGAyWRCIpFAPB7/6/Z6vR5GoxFLS0uYnp5OWZckSc1Rc0aI8A8PD4jH40gkEl9uY7FYklEdDgc8Ho9W46lCiPB2u/3L6E1NTSgvL8fBwQGMRqPGk6lHiPB/qqurQ0dHBwDA7Xajvr6eeCIVaHbh+g1ZllOup8fHx1U/5vX1tdLb25s85szMjBKJRFQ/7n+EfMVrwWq1Ynl5GSMjIwAAp9OJ0tJSzY5fsOEBwGazwWazkRybPxYmwuGJcHgiQoZ/fn7G09MT9RiqEiK8w+FI+X17extra2tE02hDiPCbm5vQ6YQYRTOF9WwFwuGJcHgiHJ4IhyfC4YkU9IdkP+H3+xEOh1PWZFlGY2NjRvvj8Gnwer2YmJjA1dVVynpzczM8Hg/a2tp+vE8+1aRhdXX1Q3QAOD8/x9zcHLxe74/3yeGz5PP5cHJy8uPHCRve4/Fgf3+fegzVCBFekiScnZ2lrAWDwbz4hHJ0dBQul+vnD9Ts292/CIfDiiRJKV96m0wmxefzUY+mjI2NfXqTa09PjxKNRjPapzBXNWVlZbi5uUFDQ0Ny7e3tLa07y9RWUlICWZY/rMuyDJPJlNE+hbo/PhAIfLiH5vT0FO3t7UQTqUeIc3whEiq82WzG4OBgytrW1hYikQjRROoR6lQDAIeHh+ju7k5Zu7u7g8ViIZpIHcK8uabr9vYW8/PzOdufy+WC0+nM2f7SlRfh+/r6UFRUBACIRqOf/vueqT//urSSF+H9fj/1CDkn1JsrAHR2dv7zt3YAAr7idTodDIavxzIYDKiurs7Z8bS8Q/j/hAv/GbvdnjzHW61WbGxsEE+UPSHD19bWYnh4OPn7ysoKKisrCSfKPeGu4wuFcG+uhYLDE+HwRDg8EQ5PhMMT4fBEODwRDk+EwxPh8EQ4PBEOT4TDE+HwRDg8EQ5PhMMT4fBEODwRDk/kN8yv1zXrujIRAAAAAElFTkSuQmCC\" y=\"-235.214982\"/>\n   </g>\n   <g id=\"matplotlib.axis_21\">\n    <g id=\"xtick_31\">\n     <g id=\"line2d_61\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.86424\" xlink:href=\"#m5544d91069\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_61\">\n      <!-- 0 -->\n      <g transform=\"translate(321.68299 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_32\">\n     <g id=\"line2d_62\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.494782\" xlink:href=\"#m5544d91069\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_62\">\n      <!-- 50 -->\n      <g transform=\"translate(363.132282 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_33\">\n     <g id=\"line2d_63\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"414.125324\" xlink:href=\"#m5544d91069\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_63\">\n      <!-- 100 -->\n      <g transform=\"translate(404.581574 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_22\">\n    <g id=\"ytick_31\">\n     <g id=\"line2d_64\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#md0590c5d67\" y=\"235.93715\"/>\n      </g>\n     </g>\n     <g id=\"text_64\">\n      <!-- 0 -->\n      <g transform=\"translate(311.055435 239.736369)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_32\">\n     <g id=\"line2d_65\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#md0590c5d67\" y=\"280.567692\"/>\n      </g>\n     </g>\n     <g id=\"text_65\">\n      <!-- 50 -->\n      <g transform=\"translate(304.692935 284.36691)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_33\">\n     <g id=\"line2d_66\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#md0590c5d67\" y=\"325.198234\"/>\n      </g>\n     </g>\n     <g id=\"text_66\">\n      <!-- 100 -->\n      <g transform=\"translate(298.330435 328.997452)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_53\">\n    <path d=\"M 324.417935 329.214982 \nL 324.417935 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_54\">\n    <path d=\"M 418.142073 329.214982 \nL 418.142073 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_55\">\n    <path d=\"M 324.417935 329.214982 \nL 418.142073 329.214982 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_56\">\n    <path d=\"M 324.417935 235.490844 \nL 418.142073 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_12\">\n   <g id=\"patch_57\">\n    <path d=\"M 469.983152 329.214982 \nL 563.70729 329.214982 \nL 563.70729 235.490844 \nL 469.983152 235.490844 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pa8cf3aa859)\">\n    <image height=\"94\" id=\"image10c4226794\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"469.983152\" xlink:href=\"data:image/png;base64,\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\" y=\"-235.214982\"/>\n   </g>\n   <g id=\"matplotlib.axis_23\">\n    <g id=\"xtick_34\">\n     <g id=\"line2d_67\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"470.429458\" xlink:href=\"#m5544d91069\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_67\">\n      <!-- 0 -->\n      <g transform=\"translate(467.248208 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_35\">\n     <g id=\"line2d_68\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"515.059999\" xlink:href=\"#m5544d91069\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_68\">\n      <!-- 50 -->\n      <g transform=\"translate(508.697499 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_36\">\n     <g id=\"line2d_69\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"559.690541\" xlink:href=\"#m5544d91069\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_69\">\n      <!-- 100 -->\n      <g transform=\"translate(550.146791 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_24\">\n    <g id=\"ytick_34\">\n     <g id=\"line2d_70\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#md0590c5d67\" y=\"235.93715\"/>\n      </g>\n     </g>\n     <g id=\"text_70\">\n      <!-- 0 -->\n      <g transform=\"translate(456.620652 239.736369)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_35\">\n     <g id=\"line2d_71\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#md0590c5d67\" y=\"280.567692\"/>\n      </g>\n     </g>\n     <g id=\"text_71\">\n      <!-- 50 -->\n      <g transform=\"translate(450.258152 284.36691)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_36\">\n     <g id=\"line2d_72\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#md0590c5d67\" y=\"325.198234\"/>\n      </g>\n     </g>\n     <g id=\"text_72\">\n      <!-- 100 -->\n      <g transform=\"translate(443.895652 328.997452)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_58\">\n    <path d=\"M 469.983152 329.214982 \nL 469.983152 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_59\">\n    <path d=\"M 563.70729 329.214982 \nL 563.70729 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_60\">\n    <path d=\"M 469.983152 329.214982 \nL 563.70729 329.214982 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_61\">\n    <path d=\"M 469.983152 235.490844 \nL 563.70729 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_13\">\n   <g id=\"patch_62\">\n    <path d=\"M 33.2875 441.683948 \nL 127.011638 441.683948 \nL 127.011638 347.95981 \nL 33.2875 347.95981 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p4839ff79d0)\">\n    <image height=\"94\" id=\"image676b4a8f61\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"33.2875\" xlink:href=\"data:image/png;base64,\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\" y=\"-347.683948\"/>\n   </g>\n   <g id=\"matplotlib.axis_25\">\n    <g id=\"xtick_37\">\n     <g id=\"line2d_73\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.733805\" xlink:href=\"#m5544d91069\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_73\">\n      <!-- 0 -->\n      <g transform=\"translate(30.552555 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_38\">\n     <g id=\"line2d_74\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"78.364347\" xlink:href=\"#m5544d91069\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_74\">\n      <!-- 50 -->\n      <g transform=\"translate(72.001847 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_39\">\n     <g id=\"line2d_75\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"122.994889\" xlink:href=\"#m5544d91069\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_75\">\n      <!-- 100 -->\n      <g transform=\"translate(113.451139 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_26\">\n    <g id=\"ytick_37\">\n     <g id=\"line2d_76\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#md0590c5d67\" y=\"348.406115\"/>\n      </g>\n     </g>\n     <g id=\"text_76\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 352.205334)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_38\">\n     <g id=\"line2d_77\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#md0590c5d67\" y=\"393.036657\"/>\n      </g>\n     </g>\n     <g id=\"text_77\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 396.835876)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_39\">\n     <g id=\"line2d_78\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#md0590c5d67\" y=\"437.667199\"/>\n      </g>\n     </g>\n     <g id=\"text_78\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 441.466418)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_63\">\n    <path d=\"M 33.2875 441.683948 \nL 33.2875 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_64\">\n    <path d=\"M 127.011638 441.683948 \nL 127.011638 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_65\">\n    <path d=\"M 33.2875 441.683948 \nL 127.011638 441.683948 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_66\">\n    <path d=\"M 33.2875 347.95981 \nL 127.011638 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_14\">\n   <g id=\"patch_67\">\n    <path d=\"M 178.852717 441.683948 \nL 272.576855 441.683948 \nL 272.576855 347.95981 \nL 178.852717 347.95981 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p81b9bfb9dc)\">\n    <image height=\"94\" id=\"image7cfd1f90b5\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"178.852717\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAFaUlEQVR4nO2cTUgbTxTA38akhlTBKqJIDkV6UlHqB3hREL2IoVgxQfCg+HEwQkHIQa3iwZOKKMQP8KCgICiCSr16UPBuoQctJYgHRRESjF+xa14Pf/TvNomxujOvse8Hc3B2d97b3w7j7O5sFEREYKRjoE7gX4XFE8HiiWDxRLB4Ilg8ESyeCBZPBIsngsUTweKJYPFEsHgiWDwRLJ4IFk8EiyeCxRPB4olg8USweCJYPBEsnggWTwSLJ8JIFdjv98PV1dWD+yQmJoLZbJaUkWRQMj6fD7e3t9FutyMAPFgGBgZwe3sbA4GA7DSFI1W8z+fD7u7uqMJ/L9PT06iqqsxUhSNFvKqqODo6ii6X64+l35bh4WEMBoMy0pWCFPGfPn16svDbYjAYsLOzU0a6UlAQxS/TNpvNEAgEQur7+/shPz8/7DGzs7OwsLCgqUtNTYXj42MhOUpH9JUtKytDRVFCenBvby/6/f6Ix4X7XxAXF4c1NTWiU5aCUPEVFRVhpTudTry8vHzw2PPzczw5OcGCggLNsXl5eS9irBd2AxUIBMDr9QLeG8kMBgM4HA5wu91R5+cWiwVSUlIgISEBFEW5q//69StUV1fD9fW1qNTlIOqKhpunv3///kltWa3WkLaamprw9PRU56zlIaTH7+3twcnJiabOZDJBXl7ek9orLCzU9HoAgOnpaejr6wOv1/vkPEnR+0p+//4dbTZbSA9NS0t7VrsNDQ1hp5kulwu9Xq8+yUtEd/FTU1Nh5+CDg4PPaldV1Yg3YE6nE8/Pz3U6AzlIET8zM6PLTOTo6CjiDZbD4YipxwpSHgvX1dWFjNFPISkpCWZmZsJuW1xchNLSUrDb7c+OIwW9r2S4Hh9tzv4nBAIB9Hg82NPTE7bnG41GrK2t1S2eKGJO/C3X19fY1tYW8blOc3Oz7jH1JGbfQJlMJhgfHweHwwFxcXGabcFgEPx+P1xcXBBlF52YFQ8AoCgKLCwsQFVVFWRlZWm2LS4uwtDQEFFm0Ylp8besrq7Ct2/foLKy8q4uIyMD3r17R5jVw5C9c9UbRVFgeXkZXC4XAAAUFRVBfX09cVaReTHiAQDi4+PB7XZTp/EoXsRQE4uweCJYPBEsnggWTwSLJ4LFE8HiiWDxRLB4Ilg8EVLEB4NBGWFiCini37x5o1lRxggQn5SUBMnJyZq6YDAIP3780DtUTKO7eLvdDi0tLZo6VVWhpKRE71AxjZChpri4GDIzM0U0/WIQIv7jx49QUFCgqfP7/TAyMiIiXEwi7Q3UxcUFzM3NQUdHh6yQd3R1dcHOzk7Ybc3NzWCz2SRnBOKWaXs8HszOztZlmfZz+Pz5M75+/Tri0j+r1Yqbm5vS8xL6RUhRUVHIKq/GxkaRITUMDg6i2WyO+mHbly9fpOV0i9B5/NbWFmRkZNz9raoq+Hw+kSE1nJ2dRf16nAqh4k0mk8jmYxrpz2q8Xi/s7+/LDvvXIVz8hw8fwGD4P8zGxga0t7f/83eywsVPTk6GDDlra2vgcrnA4/GIDv/XImUePzY2Bq2trZq61dVVuLq6AqvVqlucV69ewcTExKP3Ly8vh7q6OsjNzdUth0cjY+p0c3ODS0tLz/49g2jFYrFo4no8HiwvLw/Z7+3bt7i+vo67u7syTj8s0n425efPn7iysiJVPCLi4eEh5ufna/bLycmRddoRkTarMRqNYLPZYH5+XpfvoR5Leno6WCwWMBgMd0Vm/EhI+fWO+yAijI2NQW9vr+5tWywWODg4CBvz99O8P9OiQLp45j/4ZTcRLJ4IFk8EiyeCxRPB4olg8USweCJYPBEsnggWTwSLJ4LFE8HiiWDxRPwC5fpvc149l54AAAAASUVORK5CYII=\" y=\"-347.683948\"/>\n   </g>\n   <g id=\"matplotlib.axis_27\">\n    <g id=\"xtick_40\">\n     <g id=\"line2d_79\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.299023\" xlink:href=\"#m5544d91069\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_79\">\n      <!-- 0 -->\n      <g transform=\"translate(176.117773 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_41\">\n     <g id=\"line2d_80\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"223.929565\" xlink:href=\"#m5544d91069\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_80\">\n      <!-- 50 -->\n      <g transform=\"translate(217.567065 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_42\">\n     <g id=\"line2d_81\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.560107\" xlink:href=\"#m5544d91069\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_81\">\n      <!-- 100 -->\n      <g transform=\"translate(259.016357 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_28\">\n    <g id=\"ytick_40\">\n     <g id=\"line2d_82\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#md0590c5d67\" y=\"348.406115\"/>\n      </g>\n     </g>\n     <g id=\"text_82\">\n      <!-- 0 -->\n      <g transform=\"translate(165.490217 352.205334)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_41\">\n     <g id=\"line2d_83\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#md0590c5d67\" y=\"393.036657\"/>\n      </g>\n     </g>\n     <g id=\"text_83\">\n      <!-- 50 -->\n      <g transform=\"translate(159.127717 396.835876)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_42\">\n     <g id=\"line2d_84\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#md0590c5d67\" y=\"437.667199\"/>\n      </g>\n     </g>\n     <g id=\"text_84\">\n      <!-- 100 -->\n      <g transform=\"translate(152.765217 441.466418)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_68\">\n    <path d=\"M 178.852717 441.683948 \nL 178.852717 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_69\">\n    <path d=\"M 272.576855 441.683948 \nL 272.576855 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_70\">\n    <path d=\"M 178.852717 441.683948 \nL 272.576855 441.683948 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_71\">\n    <path d=\"M 178.852717 347.95981 \nL 272.576855 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_15\">\n   <g id=\"patch_72\">\n    <path d=\"M 324.417935 441.683948 \nL 418.142073 441.683948 \nL 418.142073 347.95981 \nL 324.417935 347.95981 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p6a0194a459)\">\n    <image height=\"94\" id=\"image4126e752e7\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"324.417935\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAEyElEQVR4nO2bz0sqXxTAz6SCYxKRJZZhiyCKIihKcBEtiookqlXQP9APIijaJS2MWisV0S6CaGtQ1KY22aqWRUQujKIWakSUGqPe76L3lTeV7720uWeq84FZzJ2Ze44frtc713sFxhgDgjt52An8VEg8EiQeCRKPBIlHgsQjQeKRIPFIkHgkSDwSJB4JEo8EiUeCxCNB4pEg8UiQeCRIPBIkHgkSjwSJR0KLncBnMTQ0BI+Pj1k9OzU1BQ0NDZ+c0V9gX5zJyUnmcDiYVqtlAJDVUV1dzS4uLrjmLTD2NdbVSJIElZWVb8pDoRDE4/Gc67dYLHB6egpFRUU51/UvqE58KpWCZDKZPm9ra4Pj42MAAIjFYorG1uv1cHd3B6IoKhoHQGXik8kkbG1tQV9fX1bPFxcXf+j+WCwGT09PsjJRFCEajWYV/0Nw7dj+QCKRYFtbW1n10VVVVay+vp4lEokPxVxfX2cmk0lWlyiKCn1COapo8bu7u/Dw8AADAwMZ7xEEAZxO57vXFhcXoaKiIqvY4+PjsLCwkD7n1eLRh5Nra2swPDz81/57bGwMvF4vCILAKTNlQRW/tLQELpfrXelOpxM6OzvT5yMjI1ykS5IEMzMz4Ha7lQ3EpUPLQEdHx7t9dnt7O7dx9cnJCWtubpbFt9lsisdFa/FutxsODw/flDc1NcHq6ipYrVYuedTW1oLZbOYS63e4z9UwxoAxBre3t7KhXH5+PoTDYdjb2+MmPRPX19fQ1dWlbBDFv1Ov8Pl8TKPRMEEQZF9vs9nMO5U0iUSC1dXVyfJxOByKxuTe1bx+MwV4GSre3NzwTiWNRqPhPlpSxbSw3W7/NsPEf0UV4nd2diAvTxWppAmFQuD3+xWrX12fVkUEAgFYXl5WrH4S/4u5uTkwGo3c4pH4X/T09IBer+cWj8RnoLGxETwej2L1o0+SqYlgMAgmkwksFgv4/X5F/xAh8b+Rn5+fnrBTenhL4l/B632C+ngkSDwSJB4JEo8EiUeCxCOhCvHxeBwY/ioTrnAXbzAYoLCwUFZmtVohlUrxTgUV7uI7OzvB5XLxDqs6VNHV/ERIPBKqEM8Yg9HRUew0+KLoGoYMXF1dsd7eXtlyCqPRiJEKGigtvry8HEpLSzFCqwZVdDU/ERKPBIlHgsQjQeKRoP9cP8D8/DxEIhEwGAwwOzubU12qER+Px6G/vz+rZ71eL9hstk/OSM709DR4PB6IRqOg0+ng/v5etmnto6hGfCKRAJ/Pl9Wz5+fnoNfrQRTFd3eZfAYHBwfp3YCSJMHe3l5O9alGfC6cnZ0BwMvSDIvFAna7HTY3N2X3qG4ZONYrsyRJrLu7+83OkM84BEFgOp1OdlxeXuaUb0tLiyxGTU1NTvWhtXitVgvb29vQ2toKgUAg63oikQg8Pz/LyhhjIEnSm7JcKCkpgbKysvR5rhvWVLGzOxcmJibS/frR0VHG+4LBYNa7v5Xgy4v/H8YYDA4OQiqVgnA4DPv7+7LrahP/LX5cAV5+PDc2NgDgRfLKyorsekFBAUZaGfk2Lf6rQVMGSJB4JEg8EiQeCRKPBIlHgsQjQeKRIPFIkHgkSDwSJB4JEo8EiUeCxCNB4pEg8UiQeCRIPBIkHon/AB1R04zffwu9AAAAAElFTkSuQmCC\" y=\"-347.683948\"/>\n   </g>\n   <g id=\"matplotlib.axis_29\">\n    <g id=\"xtick_43\">\n     <g id=\"line2d_85\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.86424\" xlink:href=\"#m5544d91069\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_85\">\n      <!-- 0 -->\n      <g transform=\"translate(321.68299 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_44\">\n     <g id=\"line2d_86\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.494782\" xlink:href=\"#m5544d91069\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_86\">\n      <!-- 50 -->\n      <g transform=\"translate(363.132282 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_45\">\n     <g id=\"line2d_87\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"414.125324\" xlink:href=\"#m5544d91069\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_87\">\n      <!-- 100 -->\n      <g transform=\"translate(404.581574 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_30\">\n    <g id=\"ytick_43\">\n     <g id=\"line2d_88\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#md0590c5d67\" y=\"348.406115\"/>\n      </g>\n     </g>\n     <g id=\"text_88\">\n      <!-- 0 -->\n      <g transform=\"translate(311.055435 352.205334)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_44\">\n     <g id=\"line2d_89\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#md0590c5d67\" y=\"393.036657\"/>\n      </g>\n     </g>\n     <g id=\"text_89\">\n      <!-- 50 -->\n      <g transform=\"translate(304.692935 396.835876)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_45\">\n     <g id=\"line2d_90\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#md0590c5d67\" y=\"437.667199\"/>\n      </g>\n     </g>\n     <g id=\"text_90\">\n      <!-- 100 -->\n      <g transform=\"translate(298.330435 441.466418)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_73\">\n    <path d=\"M 324.417935 441.683948 \nL 324.417935 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_74\">\n    <path d=\"M 418.142073 441.683948 \nL 418.142073 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_75\">\n    <path d=\"M 324.417935 441.683948 \nL 418.142073 441.683948 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_76\">\n    <path d=\"M 324.417935 347.95981 \nL 418.142073 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_16\">\n   <g id=\"patch_77\">\n    <path d=\"M 469.983152 441.683948 \nL 563.70729 441.683948 \nL 563.70729 347.95981 \nL 469.983152 347.95981 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pc75ef63a2a)\">\n    <image height=\"94\" id=\"image5c5df00366\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"469.983152\" xlink:href=\"data:image/png;base64,\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\" y=\"-347.683948\"/>\n   </g>\n   <g id=\"matplotlib.axis_31\">\n    <g id=\"xtick_46\">\n     <g id=\"line2d_91\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"470.429458\" xlink:href=\"#m5544d91069\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_91\">\n      <!-- 0 -->\n      <g transform=\"translate(467.248208 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_47\">\n     <g id=\"line2d_92\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"515.059999\" xlink:href=\"#m5544d91069\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_92\">\n      <!-- 50 -->\n      <g transform=\"translate(508.697499 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_48\">\n     <g id=\"line2d_93\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"559.690541\" xlink:href=\"#m5544d91069\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_93\">\n      <!-- 100 -->\n      <g transform=\"translate(550.146791 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_32\">\n    <g id=\"ytick_46\">\n     <g id=\"line2d_94\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#md0590c5d67\" y=\"348.406115\"/>\n      </g>\n     </g>\n     <g id=\"text_94\">\n      <!-- 0 -->\n      <g transform=\"translate(456.620652 352.205334)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_47\">\n     <g id=\"line2d_95\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#md0590c5d67\" y=\"393.036657\"/>\n      </g>\n     </g>\n     <g id=\"text_95\">\n      <!-- 50 -->\n      <g transform=\"translate(450.258152 396.835876)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_48\">\n     <g id=\"line2d_96\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#md0590c5d67\" y=\"437.667199\"/>\n      </g>\n     </g>\n     <g id=\"text_96\">\n      <!-- 100 -->\n      <g transform=\"translate(443.895652 441.466418)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_78\">\n    <path d=\"M 469.983152 441.683948 \nL 469.983152 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_79\">\n    <path d=\"M 563.70729 441.683948 \nL 563.70729 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_80\">\n    <path d=\"M 469.983152 441.683948 \nL 563.70729 441.683948 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_81\">\n    <path d=\"M 469.983152 347.95981 \nL 563.70729 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_17\">\n   <g id=\"patch_82\">\n    <path d=\"M 33.2875 554.152913 \nL 127.011638 554.152913 \nL 127.011638 460.428775 \nL 33.2875 460.428775 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pd9ed14236a)\">\n    <image height=\"94\" id=\"image367ebbfd4e\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"33.2875\" xlink:href=\"data:image/png;base64,\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\" y=\"-460.152913\"/>\n   </g>\n   <g id=\"matplotlib.axis_33\">\n    <g id=\"xtick_49\">\n     <g id=\"line2d_97\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.733805\" xlink:href=\"#m5544d91069\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_97\">\n      <!-- 0 -->\n      <g transform=\"translate(30.552555 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_50\">\n     <g id=\"line2d_98\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"78.364347\" xlink:href=\"#m5544d91069\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_98\">\n      <!-- 50 -->\n      <g transform=\"translate(72.001847 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_51\">\n     <g id=\"line2d_99\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"122.994889\" xlink:href=\"#m5544d91069\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_99\">\n      <!-- 100 -->\n      <g transform=\"translate(113.451139 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_34\">\n    <g id=\"ytick_49\">\n     <g id=\"line2d_100\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#md0590c5d67\" y=\"460.875081\"/>\n      </g>\n     </g>\n     <g id=\"text_100\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 464.6743)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_50\">\n     <g id=\"line2d_101\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#md0590c5d67\" y=\"505.505623\"/>\n      </g>\n     </g>\n     <g id=\"text_101\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 509.304841)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_51\">\n     <g id=\"line2d_102\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#md0590c5d67\" y=\"550.136165\"/>\n      </g>\n     </g>\n     <g id=\"text_102\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 553.935383)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_83\">\n    <path d=\"M 33.2875 554.152913 \nL 33.2875 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_84\">\n    <path d=\"M 127.011638 554.152913 \nL 127.011638 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_85\">\n    <path d=\"M 33.2875 554.152913 \nL 127.011638 554.152913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_86\">\n    <path d=\"M 33.2875 460.428775 \nL 127.011638 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_18\">\n   <g id=\"patch_87\">\n    <path d=\"M 178.852717 554.152913 \nL 272.576855 554.152913 \nL 272.576855 460.428775 \nL 178.852717 460.428775 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p8f5f6284a9)\">\n    <image height=\"94\" id=\"image2bb2ac01a0\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"178.852717\" xlink:href=\"data:image/png;base64,\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\" y=\"-460.152913\"/>\n   </g>\n   <g id=\"matplotlib.axis_35\">\n    <g id=\"xtick_52\">\n     <g id=\"line2d_103\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.299023\" xlink:href=\"#m5544d91069\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_103\">\n      <!-- 0 -->\n      <g transform=\"translate(176.117773 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_53\">\n     <g id=\"line2d_104\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"223.929565\" xlink:href=\"#m5544d91069\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_104\">\n      <!-- 50 -->\n      <g transform=\"translate(217.567065 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_54\">\n     <g id=\"line2d_105\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.560107\" xlink:href=\"#m5544d91069\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_105\">\n      <!-- 100 -->\n      <g transform=\"translate(259.016357 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_36\">\n    <g id=\"ytick_52\">\n     <g id=\"line2d_106\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#md0590c5d67\" y=\"460.875081\"/>\n      </g>\n     </g>\n     <g id=\"text_106\">\n      <!-- 0 -->\n      <g transform=\"translate(165.490217 464.6743)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_53\">\n     <g id=\"line2d_107\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#md0590c5d67\" y=\"505.505623\"/>\n      </g>\n     </g>\n     <g id=\"text_107\">\n      <!-- 50 -->\n      <g transform=\"translate(159.127717 509.304841)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_54\">\n     <g id=\"line2d_108\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#md0590c5d67\" y=\"550.136165\"/>\n      </g>\n     </g>\n     <g id=\"text_108\">\n      <!-- 100 -->\n      <g transform=\"translate(152.765217 553.935383)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_88\">\n    <path d=\"M 178.852717 554.152913 \nL 178.852717 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_89\">\n    <path d=\"M 272.576855 554.152913 \nL 272.576855 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_90\">\n    <path d=\"M 178.852717 554.152913 \nL 272.576855 554.152913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_91\">\n    <path d=\"M 178.852717 460.428775 \nL 272.576855 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_19\">\n   <g id=\"patch_92\">\n    <path d=\"M 324.417935 554.152913 \nL 418.142073 554.152913 \nL 418.142073 460.428775 \nL 324.417935 460.428775 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p5c87d90fe3)\">\n    <image height=\"94\" id=\"image225771a9ff\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"324.417935\" xlink:href=\"data:image/png;base64,\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\" y=\"-460.152913\"/>\n   </g>\n   <g id=\"matplotlib.axis_37\">\n    <g id=\"xtick_55\">\n     <g id=\"line2d_109\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.86424\" xlink:href=\"#m5544d91069\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_109\">\n      <!-- 0 -->\n      <g transform=\"translate(321.68299 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_56\">\n     <g id=\"line2d_110\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.494782\" xlink:href=\"#m5544d91069\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_110\">\n      <!-- 50 -->\n      <g transform=\"translate(363.132282 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_57\">\n     <g id=\"line2d_111\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"414.125324\" xlink:href=\"#m5544d91069\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_111\">\n      <!-- 100 -->\n      <g transform=\"translate(404.581574 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_38\">\n    <g id=\"ytick_55\">\n     <g id=\"line2d_112\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#md0590c5d67\" y=\"460.875081\"/>\n      </g>\n     </g>\n     <g id=\"text_112\">\n      <!-- 0 -->\n      <g transform=\"translate(311.055435 464.6743)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_56\">\n     <g id=\"line2d_113\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#md0590c5d67\" y=\"505.505623\"/>\n      </g>\n     </g>\n     <g id=\"text_113\">\n      <!-- 50 -->\n      <g transform=\"translate(304.692935 509.304841)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_57\">\n     <g id=\"line2d_114\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#md0590c5d67\" y=\"550.136165\"/>\n      </g>\n     </g>\n     <g id=\"text_114\">\n      <!-- 100 -->\n      <g transform=\"translate(298.330435 553.935383)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_93\">\n    <path d=\"M 324.417935 554.152913 \nL 324.417935 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_94\">\n    <path d=\"M 418.142073 554.152913 \nL 418.142073 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_95\">\n    <path d=\"M 324.417935 554.152913 \nL 418.142073 554.152913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_96\">\n    <path d=\"M 324.417935 460.428775 \nL 418.142073 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_20\">\n   <g id=\"patch_97\">\n    <path d=\"M 469.983152 554.152913 \nL 563.70729 554.152913 \nL 563.70729 460.428775 \nL 469.983152 460.428775 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p5b9292c946)\">\n    <image height=\"94\" id=\"imageba53048e47\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"469.983152\" xlink:href=\"data:image/png;base64,\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\" y=\"-460.152913\"/>\n   </g>\n   <g id=\"matplotlib.axis_39\">\n    <g id=\"xtick_58\">\n     <g id=\"line2d_115\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"470.429458\" xlink:href=\"#m5544d91069\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_115\">\n      <!-- 0 -->\n      <g transform=\"translate(467.248208 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_59\">\n     <g id=\"line2d_116\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"515.059999\" xlink:href=\"#m5544d91069\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_116\">\n      <!-- 50 -->\n      <g transform=\"translate(508.697499 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_60\">\n     <g id=\"line2d_117\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"559.690541\" xlink:href=\"#m5544d91069\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_117\">\n      <!-- 100 -->\n      <g transform=\"translate(550.146791 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_40\">\n    <g id=\"ytick_58\">\n     <g id=\"line2d_118\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#md0590c5d67\" y=\"460.875081\"/>\n      </g>\n     </g>\n     <g id=\"text_118\">\n      <!-- 0 -->\n      <g transform=\"translate(456.620652 464.6743)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_59\">\n     <g id=\"line2d_119\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#md0590c5d67\" y=\"505.505623\"/>\n      </g>\n     </g>\n     <g id=\"text_119\">\n      <!-- 50 -->\n      <g transform=\"translate(450.258152 509.304841)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_60\">\n     <g id=\"line2d_120\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#md0590c5d67\" y=\"550.136165\"/>\n      </g>\n     </g>\n     <g id=\"text_120\">\n      <!-- 100 -->\n      <g transform=\"translate(443.895652 553.935383)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_98\">\n    <path d=\"M 469.983152 554.152913 \nL 469.983152 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_99\">\n    <path d=\"M 563.70729 554.152913 \nL 563.70729 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_100\">\n    <path d=\"M 469.983152 554.152913 \nL 563.70729 554.152913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_101\">\n    <path d=\"M 469.983152 460.428775 \nL 563.70729 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p7672219432\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"33.2875\" y=\"10.552913\"/>\n  </clipPath>\n  <clipPath id=\"p57b2f2dac4\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"178.852717\" y=\"10.552913\"/>\n  </clipPath>\n  <clipPath id=\"pa920e3822b\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"324.417935\" y=\"10.552913\"/>\n  </clipPath>\n  <clipPath id=\"pdaf1a56068\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"469.983152\" y=\"10.552913\"/>\n  </clipPath>\n  <clipPath id=\"p29b92e3b0c\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"33.2875\" y=\"123.021879\"/>\n  </clipPath>\n  <clipPath id=\"p84e82fc35c\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"178.852717\" y=\"123.021879\"/>\n  </clipPath>\n  <clipPath id=\"p36ec68eb19\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"324.417935\" y=\"123.021879\"/>\n  </clipPath>\n  <clipPath id=\"p192844acd5\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"469.983152\" y=\"123.021879\"/>\n  </clipPath>\n  <clipPath id=\"pb68fe24773\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"33.2875\" y=\"235.490844\"/>\n  </clipPath>\n  <clipPath id=\"p14a02e5372\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"178.852717\" y=\"235.490844\"/>\n  </clipPath>\n  <clipPath id=\"p4f6c731ded\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"324.417935\" y=\"235.490844\"/>\n  </clipPath>\n  <clipPath id=\"pa8cf3aa859\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"469.983152\" y=\"235.490844\"/>\n  </clipPath>\n  <clipPath id=\"p4839ff79d0\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"33.2875\" y=\"347.95981\"/>\n  </clipPath>\n  <clipPath id=\"p81b9bfb9dc\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"178.852717\" y=\"347.95981\"/>\n  </clipPath>\n  <clipPath id=\"p6a0194a459\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"324.417935\" y=\"347.95981\"/>\n  </clipPath>\n  <clipPath id=\"pc75ef63a2a\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"469.983152\" y=\"347.95981\"/>\n  </clipPath>\n  <clipPath id=\"pd9ed14236a\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"33.2875\" y=\"460.428775\"/>\n  </clipPath>\n  <clipPath id=\"p8f5f6284a9\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"178.852717\" y=\"460.428775\"/>\n  </clipPath>\n  <clipPath id=\"p5c87d90fe3\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"324.417935\" y=\"460.428775\"/>\n  </clipPath>\n  <clipPath id=\"p5b9292c946\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"469.983152\" y=\"460.428775\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plot_images(os.path.join(data_path, 'images_background/Arcadian/character03/'))\n",
    "print(\"Arcadian language, 20 samples of the third character.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'plot_images(os.path.join(data_path, \\'images_evaluation/Electronics,PortableAudio&Video/Batteries\\'))\\nprint(\"Electronics. Computer & Accessories, 20 samples of the CaseFan Category.\")'"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "'''plot_images(os.path.join(data_path, 'images_evaluation/Electronics,PortableAudio&Video/Batteries'))\n",
    "print(\"Electronics. Computer & Accessories, 20 samples of the CaseFan Category.\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Korean language, 20 samples of the seventh character.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 720x720 with 20 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"578.031038pt\" version=\"1.1\" viewBox=\"0 0 576.434291 578.031038\" width=\"576.434291pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-01-24T03:35:19.531928</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 578.031038 \nL 576.434291 578.031038 \nL 576.434291 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 104.277051 \nL 127.011638 104.277051 \nL 127.011638 10.552913 \nL 33.2875 10.552913 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pb97b3d75c0)\">\n    <image height=\"94\" id=\"imageaabe92648e\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"33.2875\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAADE0lEQVR4nO3cMUtqYQDG8Ue5xBEaGisspF3a+gLRHDQ09hUCaWgPghZJCPwCDuLQEI1BJEJDEFRTh6CoSEKXEkE60dvUBaluh+v73sfz3ucHDTmc8/BHXkqzlDHGQP65NHvA/0rhSRSeROFJFJ5E4UkUnkThSRSeROFJFJ5E4UkUnkThSRSeROFJFJ5E4UkUnkThSRSeROFJFJ5E4UkUnkThSRSeROFJvAofRRFKpRJqtRp7yo9+sQfYYozB6uoqyuUyJiYm0Ov1sLKywp71PeOJ5eVlA+D31+TkpKlUKuxZ3/LmqDk4OOj7/uHhAVdXV6Q1P/MmfNJ4Hf7l5QWvr6/sGV/yJnw2m8XU1FTfY5ubm6hWq6RFf+bNTzVnZ2fodrsYHR1lT4nFm2d80ig8icKTeB9+Y2MDFxcX7BmfeBU+k8mgXq/3PRaGIZ6fn0mLvudV+HQ6jWw2y54Ri1fhk0ThSRSeROFJFJ7E6Ws1a2trCMPQ5S0AADs7O5iennZ+H5uchq/X6zg5OXF5CwDA5eUlgiDA6emp83vZ4sVRE4Yhzs/PMTMzw54SmxfhP7TbbfaE2JweNePj45/enLCt1Wqh1+sBQGJ+awUch9/b23N5eQBAoVDA8fExAKDRaOD29tb5PW1I/DtQxWKRPeGveHXGJ4nCkyg8ycBn/OzsLDqdjo0tA8tkMtjf32fPiGXg8Dc3N0P1Dk8+n2dPiMW7o6bb7bInxOJV+FQqlZiXDQY+aubn54fmWRYEAba3txMRf+Dwu7u7NnZYc319zZ4Qi1dHTZIoPInCkyg8icKTKDyJwpMoPInCkyg8icKTKDyJwpMoPInCkyg8icKTKDyJwpMoPInCk1D/TDuKIry9vVm73sjIiLVruUYL//T0hMXFRRwdHVm75t3dHVqtlrXruUQLv76+bjU6AOcf+7FJZzwJLfzCwgJyuRzr9nS0o2ZpaQlBEOD+/t7K9QqFwtD8DWccKWOMYY+w4fDwEFEU4fHxEVtbW30fSpubm8PY2Bhv3Be8Cf8hiiI0m82h/98G3oVPinepdyGjKVS2JgAAAABJRU5ErkJggg==\" y=\"-10.277051\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m52cc47af11\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.733805\" xlink:href=\"#m52cc47af11\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(30.552555 118.875489)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"78.364347\" xlink:href=\"#m52cc47af11\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <g transform=\"translate(72.001847 118.875489)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"122.994889\" xlink:href=\"#m52cc47af11\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <g transform=\"translate(113.451139 118.875489)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_4\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m5b2242b6c4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m5b2242b6c4\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m5b2242b6c4\" y=\"55.629761\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 59.428979)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m5b2242b6c4\" y=\"100.260302\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 104.059521)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 104.277051 \nL 33.2875 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 127.011638 104.277051 \nL 127.011638 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 104.277051 \nL 127.011638 104.277051 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 10.552913 \nL 127.011638 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 178.852717 104.277051 \nL 272.576855 104.277051 \nL 272.576855 10.552913 \nL 178.852717 10.552913 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p027cdb35df)\">\n    <image height=\"94\" id=\"imagedd8f820673\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"178.852717\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAADvUlEQVR4nO2cv0vrUBiG35garFalg1gUXARd/EFBcHBycCg4+R8EWqXuLi5SBFEsiC6OHQVXQQfBUXC1g3+Amzio0BRtyLnLNait3luteU+T74EMp6dN3jw9fIGckxhKKQUhcDrYAaKKiCch4kmIeBIinoSIJyHiSYh4EiKehIgnIeJJxNgBgqZareLw8BAAkE6nMT8/zwmiIoTrusq2bQVAAVCTk5Pq4uKCkiVSpcbzPJRKJb9dLpdRLpcpWSIl3jAMdgSfSInXCRFPIlLilUaTbZESLzVeEPEsIiU+m82yI/hESvzx8TE7gk+kxH/Etm3Ytk05dmTEu65b91kikUBvby8hTYTEj46Oolqt+m3LspBIJGh5IiP+IwsLC9ja2qIdP7Li2Yh4EiKehIgnIeJJtMVk92/czlVK+ful3LWkzPQ2geM4amRkRJmm+aMNfye4XzfDMJRpmqpYLKparRb4eWkvfnZ2tk5aq7eTk5PAz0tqPInIi5+ZmcHw8HDgx22Li+tbBgcHsbKy0tRvSqUSbm9v/fbAwADy+TwAIJPJIJ1OtzTj/2AopdEMcAPOzs5wf3/vt5PJJBYXF5vax9zcHC4vL/32xMQEbSHTK9qP+Ewmw47wK0S+xrPQQrzneejr60NPT4+/bW9vw3VduK6r1XqYVqFFjfc8D52dnfA8r2H/9fU1hoaGmt5vPB5Hd3e31PjvMjU19a3fra6uIp/Pw3GcFif6OW0x4luNDiNeixpvGAaWl5fZMQJFi1JjGAb29/cxNjbWsL9QKODx8THgVL+LFqXmX5yfn79bIfAVd3d3yOVyX35Hh1LTFuKb4eXlBTc3N357fX0dp6enfru/vx9XV1cYHx9nxPPRotS0EsuyMD097beTyeS7/lgsRpcOaHJxjSIinoSIJyHiSYh4EiKehIgnIeJJiHgSIp6EiCch4kmIeBIinoSIJyHiSYRavM6Ta6EWv7m5iaOjI3aMhoRWvOM4eHh4qFurk0qlSIk+EPgzKAHw9PSkNjY26h65MU1TeZ7HjqeUCuGjOJVKBXt7eygUCnV9S0tLhESfwP7nW8nz87NaW1v79CGzSqXCjugTqhFfq9Wwu7vbsG9nZweWZQWc6HNCt66mEQcHB8jlcojF9DndUI34RhSLRWSzWXR1dbGjvEOfIdAiOjrej6VUKoV4PE5K8zmhWjup3ryf4BXDMLR6w+oroRLfToS+xuuKiCch4kmIeBIinoSIJyHiSYh4EiKehIgnIeJJiHgSIp6EiCch4kn8AVt1aP+CBqiUAAAAAElFTkSuQmCC\" y=\"-10.277051\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.299023\" xlink:href=\"#m52cc47af11\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(176.117773 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"223.929565\" xlink:href=\"#m52cc47af11\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 50 -->\n      <g transform=\"translate(217.567065 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.560107\" xlink:href=\"#m52cc47af11\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 100 -->\n      <g transform=\"translate(259.016357 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#m5b2242b6c4\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0 -->\n      <g transform=\"translate(165.490217 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#m5b2242b6c4\" y=\"55.629761\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 50 -->\n      <g transform=\"translate(159.127717 59.428979)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#m5b2242b6c4\" y=\"100.260302\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 100 -->\n      <g transform=\"translate(152.765217 104.059521)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 178.852717 104.277051 \nL 178.852717 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 272.576855 104.277051 \nL 272.576855 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 178.852717 104.277051 \nL 272.576855 104.277051 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 178.852717 10.552913 \nL 272.576855 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g id=\"patch_12\">\n    <path d=\"M 324.417935 104.277051 \nL 418.142073 104.277051 \nL 418.142073 10.552913 \nL 324.417935 10.552913 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pa8e7ef2c91)\">\n    <image height=\"94\" id=\"image58e7ffcb79\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"324.417935\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAEE0lEQVR4nO3cv0tqfxzH8dexgjIVssKGhkCiCEGaDGwoaGoICieDlqb+gaY40Rg0VBBEQ1OL1CBCOUTUIEHQ1LGxhA4hIkKDCCWdzx0uHb7n6+V7rXM8b8/5vh8gdE7Hc949iaMcf0hCCAFmOw/1AP9XHJ4IhyfC4YlweCIcngiHJ8LhiXB4IhyeiGPCa5qGi4sL6jEs00k9QLP29vYgyzK2trb0dbOzs5icnCScygThEN3d3QKA4ba/v0891o855lTjNo4Nv7KyguXlZeoxfsyx4YPBIILBIPUYP+bY8E7H4YlweCIcngiHJ8LhiXB4IhyeiKUXyTRNw+fnp5W7/M9j1et1AEBnZyckSbLluJax6qJPvV4XZ2dnDRey7LgpimLVn2Eby041hUIBiUTCqt25nmXhBb8F81ssC9/X14fFxUWrdud6krDwX1VVVaTTaat2p5NlGW9vb4Z1MzMzWFpaAgAkk0n09/dbftyWIn6Macrw8LDhwTQWi4nHx0fqsUxx5PP4UCiEiYkJ6jFMcWR4N+DwRDg8EQ5PhMMT4fBEODwRDk+EwxPh8EQ4PBEOT8T0a65CCGSzWcO6sbExhMNhs7t2NdPhj46OsLa2ZngFamFhAXNzc4btvF4vVldXzR7ONUyHX19fb3jZL5PJIJPJGNZ5vV7k83l9OZFIIB6Pmz28Y9n2GaharYbd3V19+erqCiMjI/pyT08PUqmUXeOQMx0+l8shGo1++8VuRVGgKIq+7PF4EI1G/7htqVQyNWM7Mh0+EomgVCrh+fkZU1NTP96Ppml4eHgwO45jmA4vSRIGBwcxMDCAj4+Pht+Pj4+jWCwC+B33/f3d7CFdwbJzvCRJ6Orqalj/9PSk/5zP5zE/P//H+6uqatUojmDrB4wjkQheXl4a1gshEI/HDY8Tqqri9fXVzvFs1Raf7JYkCbe3t4Z1qVQK6XQal5eXqFQqRJO1EOV7S5oRi8UM76kJhULi5OSEeizTHHetZmhoyNEfLP7iuPBuweGJcHgiHJ4IhyfC4YlweCIcngiHJ8LhiXB4IhyeCIcn0hbX4/+tXC6jUCgAAKrVKvE0rdFW4avVKs7Pz3Fzc4PDw0PqcVqqLcILIbC9vY1KpYKdnR3qcWxBHn5jYwPFYhHHx8d/3dbv92Nzc9OGqVrP9vC5XA6yLOvLd3d3qNVqf71fNpuFz+fD9PR0K8ezje3hy+Uyrq+vm97+9PQU0WgU4XAYHo97noSRn2r+6StsMpnEwcEBAKC3txcdHR2UY7VEW4QPBALw+/36e24kSXLed4x9k+3hfT4fRkdHDevu7+8RCATsHoWUpV8UxJrnnkcrh+HwRDg8EQ5PhMMT4fBEODwRDk+EwxPh8EQ4PBEOT4TDE+HwRH4BnDg+rcrDmcUAAAAASUVORK5CYII=\" y=\"-10.277051\"/>\n   </g>\n   <g id=\"matplotlib.axis_5\">\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.86424\" xlink:href=\"#m52cc47af11\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0 -->\n      <g transform=\"translate(321.68299 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.494782\" xlink:href=\"#m52cc47af11\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 50 -->\n      <g transform=\"translate(363.132282 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"414.125324\" xlink:href=\"#m52cc47af11\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 100 -->\n      <g transform=\"translate(404.581574 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_6\">\n    <g id=\"ytick_7\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#m5b2242b6c4\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0 -->\n      <g transform=\"translate(311.055435 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#m5b2242b6c4\" y=\"55.629761\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 50 -->\n      <g transform=\"translate(304.692935 59.428979)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#m5b2242b6c4\" y=\"100.260302\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 100 -->\n      <g transform=\"translate(298.330435 104.059521)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 324.417935 104.277051 \nL 324.417935 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 418.142073 104.277051 \nL 418.142073 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 324.417935 104.277051 \nL 418.142073 104.277051 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 324.417935 10.552913 \nL 418.142073 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_4\">\n   <g id=\"patch_17\">\n    <path d=\"M 469.983152 104.277051 \nL 563.70729 104.277051 \nL 563.70729 10.552913 \nL 469.983152 10.552913 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pbdff4b990f)\">\n    <image height=\"94\" id=\"image68448b6047\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"469.983152\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAD/0lEQVR4nO3cvy9rfxzH8Vdve1RE/ag0dDB0IoLBJiESS4OEQSMM0oXEILFI/AMWIiIRYmlsIgbSgZXJxIhoiKV+DCKlKm0059zp2+hX701ve05f59T7kRiOnhxvz9TH0Z7DpmmaBlFyv9gD/FQSnkTCk0h4EglPIuFJJDyJhCeR8CQSnkTCk0h4EglPIuFJJDyJhCeR8CQSnkTCk0h4Egd7gH/x/v6OpaUlAEB3dzcGBwfJExXOMuHT6TSCwSD29/cBAC0tLbDb7fD7/eTJCmOZpUbTtEx0ALi+vkYkEiFOVBzLhC83Ep5EwpNIeBIJTyLhSSQ8iYQnkfAklglfbvdPWCZ8Q0ND1rbD4YCiKKRpimeZ8Ol0Omt7dnYWMzMzpGmKZ4nwZ2dnUFU1s+12u+Hz+YgTFc9mhZvPmpubEY1GM9vDw8MIh8PEiYpniWd8OZLwJBKeRJe3/uLxODo7O/U4VE6Pj4+GHZtG00EsFtMAlOzDbrdrVVVV2sbGhqaqqh7fQsnpclbz+vqKurq6Yg9TkMPDQ0tebSBrPIkua7zD4cDAwIAeh8rp+PgYyWTy2+fb2trg8XgM+7pGsuQfUD6fD0NDQwgEAujr6yNOVjjLXND0VUdHB9bX19ljFEXWeBIJTyLhSSQ8iYQnkfAkEp5EwpNIeBIJTyLhSSQ8iYQnkfAkhrwsHIlEcHd3l/f+/f39lr4OshCGhA+FQlheXs57/5WVFdTW1mJqasqIcUzJFG+EzM/PQ1EUXF1d5Xw8FotlbV9cXCAcDmNkZKQE0xnDFOEB4PPzE6urq3nte3t7i5OTE0uHl1+uJIa82R2Px5FIJPLat7e3Fzc3N/90fL/fj729PdTU1BQynikYstS4XC64XK689r28vISqqqivr8+6Bv6rVCqVte10Oi0dHTDBGv/faeTHx8cf9/n/5R3lQNZ4EglPIuFJJDyJhCcxffhAIICnp6fMdmtrK7a2togT6cP04aPRaNbNxU6nE16vlziRPkwdPpFIfLuju1yYNvzz8zPGxsZwfn7OHsUQpgz/8PCAubk5HB0dsUcxDP0lg6/e3t6wu7uL09NT7OzsfHu8srISo6OjhMkMwLzl8KtkMqlNT0//9TZLj8fDHlM3pnjGa5qGiYkJHBwc/HEfu92OUChUwqmMZYqbz1RVhaIoOV8W3t7eRnt7O2w2G7q6umCz2QgTGoD54zY+Pq55vV6tqanp27KysLCg3d/fa6lUijmiYahLzcvLS87/UzA5OYnFxUU4HKZYCQ1hutNJRVHgdrvLOjpgstPJiooKBINBrK2tsUcxHDV8T08PqqurM9uNjY3Y3NwkTlQ6pjir+YlMt8b/FBKeRMKTSHiS39oOybq3u3hsAAAAAElFTkSuQmCC\" y=\"-10.277051\"/>\n   </g>\n   <g id=\"matplotlib.axis_7\">\n    <g id=\"xtick_10\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"470.429458\" xlink:href=\"#m52cc47af11\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 0 -->\n      <g transform=\"translate(467.248208 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"515.059999\" xlink:href=\"#m52cc47af11\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 50 -->\n      <g transform=\"translate(508.697499 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"559.690541\" xlink:href=\"#m52cc47af11\" y=\"104.277051\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 100 -->\n      <g transform=\"translate(550.146791 118.875489)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_8\">\n    <g id=\"ytick_10\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#m5b2242b6c4\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 0 -->\n      <g transform=\"translate(456.620652 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#m5b2242b6c4\" y=\"55.629761\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 50 -->\n      <g transform=\"translate(450.258152 59.428979)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#m5b2242b6c4\" y=\"100.260302\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 100 -->\n      <g transform=\"translate(443.895652 104.059521)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_18\">\n    <path d=\"M 469.983152 104.277051 \nL 469.983152 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path d=\"M 563.70729 104.277051 \nL 563.70729 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path d=\"M 469.983152 104.277051 \nL 563.70729 104.277051 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path d=\"M 469.983152 10.552913 \nL 563.70729 10.552913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_5\">\n   <g id=\"patch_22\">\n    <path d=\"M 33.2875 216.746017 \nL 127.011638 216.746017 \nL 127.011638 123.021879 \nL 33.2875 123.021879 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p800930ead5)\">\n    <image height=\"94\" id=\"imagec8611c6ba5\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"33.2875\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAEzUlEQVR4nO2byy8zXxjHv22pu1AWLonaEZFauSRiJ5KGEAkLCxsL/gIW/gAWVt7EJWLFXqi4bERESLpTl1IbQSIuaZCKRlOed/HLb6JafcvbmWe883ySSXrOzJznmU9PpqfnzJiIiCBojpk7AaMi4pkQ8UyIeCZEPBMingkRz4SIZ0LEM5HCnYBeODk5QTAYhNlsRk1NjfoBSaDd3V2y2+0EgKxWK62vr6se00QkczVtbW1YWVlRymVlZTg/P1c1ptzjmRDxTBhe/MjICLa2tjSPa2jxRITr62s8PT2xBDcsv379IpPJRACULSMjg8LhsOqxDd/j6cOgrry8HBaLRfXYhhYfi/39fU3iiHgmRDwTIp4JEc+EiGdCxDMh4pkQ8UyI+A98/CerFiL+AyaTSZM4Ip4JEc+EYcVvbW1hdHSULb4hxRMRAoEArq+vI+p9Pp8mU8KAQcUfHR2ho6Mjoi49PR02m02zH1ddPNB0eXmJt7e3mPuKioqQlpaW1HhEFBXP7XajsLAwqXHioQvxDocDDw8PMffNzMygqqoKAJCamoq6ujoNM1MPXYiPR39/v/I5JycHk5OTSrm2thYVFRUcaf01uhf/nkAggN7eXqXsdDpRX18f95zOzk44HA61U/s6qi+nJ0BeXl7ESn8yt8bGRvL5fBHx9vf3o47zeDyaXrMuevza2hrC4XDMfQMDA/B6vd9ue2dnB11dXcjNzVXqnp+fv91estD9Q6tXV1d4eXkBAPj9ftTW1qoSx+PxaHpL0kWPj0dJSYnyuby8POKpr8HBQczMzESdQzGGi3pD9z0+Hp+l7na70draimAwiGAwmFBb0uO/wGf/MhsaGuD3+zExMYGpqSmlnoj+6vcimfzoHv9VXl9f0d7ejkAggO3t7Yh90uNVxGKxYGVlBQcHB+xje0NOkukBEc+EiGdCxDMh4pkQ8UyIeCZEPBMingkRz4SIZ0J3czVutxs9PT1xj2lubo45D58I9/f3cDqdEXWzs7OorKz8VnvfRtOFxgTY3Nz84zqqxWKh7OxsZTs8PKRwOJzQG9l3d3dR7S0vL2twZZHorscnwuvra8RKVHV1NUwmEzIzM3F5eQkAsFqtyMrKijiPiPD4+BhRl5GRgdTUVPWT/ojmX/UfSKTHJ7K1t7fT8fExeb1e8nq9FAqFKBQKRR03Pj7Ocp266/E2mw0tLS1xj7m9vcXe3l7cY1wuF1wul1Kem5tDQUFBMlJMCj9yBcrj8WB6ehoAsLq6iouLi2+31d3djbGxMdjt9mSllxA/Uvx7FhYWcHZ2BiLC0NDQt95h6uzsxMTEBIqLi1XIMDY/Xvz/EBGWlpZARDg5OcHw8PCXztd6zfWfEf+ep6cnnJ6eKuW+vj54PJ6452gtXnejGjXw+/1UWlr66Qhofn6eQqGQpjnpblSjBjabLeoVG6fTicXFRQBASkoKzGZtZ08MIT4WZrMZVquVLz5bZINjCPEbGxsRj2bn5uaiqamJMaN/dFTzkcbGRuzu7irl6upqHBwcMGZkkB6vR0Q8EyKeCRHPhIhnQsQzIeKZEPFMiHgmRDwTIp4JEc+EiGdCxDMh4pkQ8UyIeCYMsdg9NDSEm5sbpZyfn8+YzX8YYulPj8ithgkRz8RvF3YWeKnM8DYAAAAASUVORK5CYII=\" y=\"-122.746017\"/>\n   </g>\n   <g id=\"matplotlib.axis_9\">\n    <g id=\"xtick_13\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.733805\" xlink:href=\"#m52cc47af11\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 0 -->\n      <g transform=\"translate(30.552555 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"78.364347\" xlink:href=\"#m52cc47af11\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 50 -->\n      <g transform=\"translate(72.001847 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"122.994889\" xlink:href=\"#m52cc47af11\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_27\">\n      <!-- 100 -->\n      <g transform=\"translate(113.451139 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_10\">\n    <g id=\"ytick_13\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m5b2242b6c4\" y=\"123.468184\"/>\n      </g>\n     </g>\n     <g id=\"text_28\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 127.267403)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m5b2242b6c4\" y=\"168.098726\"/>\n      </g>\n     </g>\n     <g id=\"text_29\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 171.897945)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m5b2242b6c4\" y=\"212.729268\"/>\n      </g>\n     </g>\n     <g id=\"text_30\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 216.528487)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_23\">\n    <path d=\"M 33.2875 216.746017 \nL 33.2875 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path d=\"M 127.011638 216.746017 \nL 127.011638 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path d=\"M 33.2875 216.746017 \nL 127.011638 216.746017 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path d=\"M 33.2875 123.021879 \nL 127.011638 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_6\">\n   <g id=\"patch_27\">\n    <path d=\"M 178.852717 216.746017 \nL 272.576855 216.746017 \nL 272.576855 123.021879 \nL 178.852717 123.021879 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pd98fdeba03)\">\n    <image height=\"94\" id=\"image156fe7eda5\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"178.852717\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAD+UlEQVR4nO3cP0g6fxzH8fedloHQnyEcXALJoYZsaqilJYiiBoOChiBagnAJK3BqjLZoaIiioaEIGoqWIFCoqaamhrgIwclKKqqDu89v+PEN/PrNBL3P63Pn+wFBWXCvnthpmmlCCEFMOh09oF5xeBAOD8LhQTg8CIcH4fAgHB6Ew4NweBAOD8LhQTg8CIcH4fAgHB6Ew4P40QMqJYQgwzCIiCgYDFIoFAIvqo5rrvHpdJoikQhFIhGKx+N0f3+PnlQV14QfHh7+fv/y8pI2NzeBa6rnmvBew+FBODwIhwfh8CCuCJ/P58lrf/DmivCxWIy+vr7QM2rKFeH/FgqFqLu7Gz2jKq4M39fXR3Nzc+gZVXFleC/g8CAcHkT58F67G/mH8uH7+/spm82iZ9Sc8uFt2y76uKGhgdrb20FraufXZ6Curq6KftzD4TB1dHQ4uamsrq4u2t7ehh2/VsqGF0LQ4OAgmab5fdno6ChNTk46PoyIaGBgQMpxIEQZtm2LxsZGQUSQt4mJCREOh4su6+npKTfZNZR+svvo6Ag9wTHK37h6VdnwmqbR7e2trC0lNjY2KBaLwY7vpF9PNZ2dnfT29iZjS4lAIED7+/uQYzvt1/CaplEwGJSxpa7wOR6Ew4NweBAOD6J0+PX1dbq7u0PPcITS4W9ubujl5eX747a2Njo8PMQNqiGlw//N7/dTNBpFz6gJV4X3Eg4PwuFBODwIhwep+IkQwzBodXXVyS1F4vG4tGMhVBw+n8/T3t6ek1uKpNNpen9/l3Y82ZR96u/h4QE9wVHKnuM1TUNPcFTF13ifz0etra0OTimWSqXo+vqaDg4OpB1TporD9/b20vPzs5NbSkxNTUk9nkzKnmq8jsODcHgQDg/C4UE4PAiHB+HwIMqGz2az9PT0hJ7hGCXDG4ZBiUSCzs/P0VMco1z4x8dHSiaTdHx8XHS5ruuUSCRAq2pPE0KNF5J+fn7SwsIC5XI5Ojs7K/m83+8n0zQ986ilEo/Hj42NUaFQoEwm8+PXnJ6eSlwkAfpFWENDQ0LX9bIvQstkMsKyLPTUmoKFtyxLjI+P/xhd13Wxu7srCoWC56ILAQr/8fEhZmdn/xnc5/OJ+fl5YVmWsG0bMU8K6eFfX1/F8vJySXBN00Q0GhXT09OyJ0FIuXG1bZsuLi6I6P9/T7u2tlbyNSMjI3RyciJjjhKk3J00TZMCgcCPn5+ZmaGdnR3SdeV+rXAM/DtdXFykra2tuopOBL4fn0qlaGlpiZqampAzIGBXs5WVFUomk9Tc3IyaACXlHC+EoFwuV3RZS0tLXb9wWZnHaupNfd2iKYTDg3B4EA4PwuFBODwIhwfh8CD/AV1CUF6r/0bmAAAAAElFTkSuQmCC\" y=\"-122.746017\"/>\n   </g>\n   <g id=\"matplotlib.axis_11\">\n    <g id=\"xtick_16\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.299023\" xlink:href=\"#m52cc47af11\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_31\">\n      <!-- 0 -->\n      <g transform=\"translate(176.117773 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"223.929565\" xlink:href=\"#m52cc47af11\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_32\">\n      <!-- 50 -->\n      <g transform=\"translate(217.567065 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.560107\" xlink:href=\"#m52cc47af11\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_33\">\n      <!-- 100 -->\n      <g transform=\"translate(259.016357 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_12\">\n    <g id=\"ytick_16\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#m5b2242b6c4\" y=\"123.468184\"/>\n      </g>\n     </g>\n     <g id=\"text_34\">\n      <!-- 0 -->\n      <g transform=\"translate(165.490217 127.267403)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#m5b2242b6c4\" y=\"168.098726\"/>\n      </g>\n     </g>\n     <g id=\"text_35\">\n      <!-- 50 -->\n      <g transform=\"translate(159.127717 171.897945)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#m5b2242b6c4\" y=\"212.729268\"/>\n      </g>\n     </g>\n     <g id=\"text_36\">\n      <!-- 100 -->\n      <g transform=\"translate(152.765217 216.528487)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_28\">\n    <path d=\"M 178.852717 216.746017 \nL 178.852717 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path d=\"M 272.576855 216.746017 \nL 272.576855 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path d=\"M 178.852717 216.746017 \nL 272.576855 216.746017 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path d=\"M 178.852717 123.021879 \nL 272.576855 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_7\">\n   <g id=\"patch_32\">\n    <path d=\"M 324.417935 216.746017 \nL 418.142073 216.746017 \nL 418.142073 123.021879 \nL 324.417935 123.021879 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pa204df92fc)\">\n    <image height=\"94\" id=\"imagebca56a8bdd\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"324.417935\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAACtElEQVR4nO3cMUtqYQDG8ceLaB2iITEEQ/ALNLhIrn0CNxHEQRAE15Y+Q4tb3yFw0HBwaSii5YDSEMJRCMRBLDiDIAS+d4u493K39zwv+vy20+Dz8h9eC7SYMcZAIveLfYB9FWcfwCWPj4/4+vrCwcEBSqWS3TEjxhhjut2uOTo6MgDMycmJ6ff7VvdixuiOB4Dz83O8vr5+P19cXOD5+dnanu54EoUnUXgShSdReBKFJ1F4EoUnUXgShSdReBKFJ1F4EoUnUXgShSdReBKFJ1F4EoUnUXgShSdReBKFJ1F4EoUHcHt7i8ViEemmwgO4v7/Hx8fH93M6ncbNzY3VTYX/h8PDQ+ufFlZ4EoUnUXgSp78RMplMsFqtrO98fn5a3/iTs+FHoxFarRZeXl7YR7HC2avm7u5uZ6MDDoffdc6GbzabuLy8ZB/DGqe/fLZcLrFer63vNBoNPDw8fD/ncjm8v79b3XT2zRUATk9PI9nxPC+SnZ+cvWp2ncKTKDyJwpMoPInCkyg8icKT0P+A2mw22G63tP1EIkHZpYZfLpcolUqYTqe0M3Q6Hcou9aqp1WrU6Ey640mo4avVKlKpFPMINNQ7vl6v4/j4GGEY0s5QLBYxHA4j36X/VlMul9lHoNAdT6LwJApPovAkCk+i8CQKT6LwJApPovAkCk+i8CQKT6LwJApPovAkCk+i8CQKT6LwJApPovAkCk9C/1wN09PTE3q9Ht7e3iLf3tvwvu+j3W5jPB5T9p0Of319be3jdWEYIgiCv36eTCbR7/etbP7kdPjZbAbf9yPdDIIAZ2dn1nf05gogHo8jkUhgPp8jm81GsrnX4T3PQyaTwWAwwGazQTabRSwWi2Tb6asmn8+jUChYe/1KpYKrqytrr/8/Tv/blF2211cNk8KTKDyJwpMoPMlvPl78mkwHJjYAAAAASUVORK5CYII=\" y=\"-122.746017\"/>\n   </g>\n   <g id=\"matplotlib.axis_13\">\n    <g id=\"xtick_19\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.86424\" xlink:href=\"#m52cc47af11\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_37\">\n      <!-- 0 -->\n      <g transform=\"translate(321.68299 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_20\">\n     <g id=\"line2d_38\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.494782\" xlink:href=\"#m52cc47af11\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_38\">\n      <!-- 50 -->\n      <g transform=\"translate(363.132282 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_21\">\n     <g id=\"line2d_39\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"414.125324\" xlink:href=\"#m52cc47af11\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_39\">\n      <!-- 100 -->\n      <g transform=\"translate(404.581574 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_14\">\n    <g id=\"ytick_19\">\n     <g id=\"line2d_40\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#m5b2242b6c4\" y=\"123.468184\"/>\n      </g>\n     </g>\n     <g id=\"text_40\">\n      <!-- 0 -->\n      <g transform=\"translate(311.055435 127.267403)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_41\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#m5b2242b6c4\" y=\"168.098726\"/>\n      </g>\n     </g>\n     <g id=\"text_41\">\n      <!-- 50 -->\n      <g transform=\"translate(304.692935 171.897945)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_42\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#m5b2242b6c4\" y=\"212.729268\"/>\n      </g>\n     </g>\n     <g id=\"text_42\">\n      <!-- 100 -->\n      <g transform=\"translate(298.330435 216.528487)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_33\">\n    <path d=\"M 324.417935 216.746017 \nL 324.417935 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path d=\"M 418.142073 216.746017 \nL 418.142073 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path d=\"M 324.417935 216.746017 \nL 418.142073 216.746017 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path d=\"M 324.417935 123.021879 \nL 418.142073 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_8\">\n   <g id=\"patch_37\">\n    <path d=\"M 469.983152 216.746017 \nL 563.70729 216.746017 \nL 563.70729 123.021879 \nL 469.983152 123.021879 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p640e23e479)\">\n    <image height=\"94\" id=\"image836239fa88\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"469.983152\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAADo0lEQVR4nO2cPUgrQRSFz25WDfgDiig2imIRJJWNgiBopRiwsVFsrUSwEgT7lJJGm4CQLkUQC0tBwUK7NGIjiIqiIiiEoBvRedXLY5O88JS3c9ad+0GKmcCem4/l7mZ2EksppSBox2YXYCoinoSIJyHiSYh4EiKehIgnIeJJiHgSIp6EiCfhsAsIAru7u7i8vAQArK6uwrY1nI/KcLLZrOrr61MAFAC1srKiJdf4VnN8fIyrq6vyOJ1Oa8k1XjwLEU9CxJMQ8SSMFn97e4u7uztKttHic7kccrkcJdto8bXY2NjQkiPiK1hbW9OSI+JJWEqZua/m4OAAc3NzeHl58cy/v7/DcfxfwjL2jH99fa2Sfn19jUgkoiXfWPG1iEajsCxLS5aIJyHiSYh4EiKehIgnIeJJGCk+CN8ZjRR/dnaG2dlZz5zOe3jAUPFKKXx+fnrmTk9P0dnZqa0GI8UHAePEu66L/f19dhnmiS8UClhfX/fMzc/Po7u7W2sdxomvxcLCgog3BRFPQsSTEPEkArE//vz8HB8fH1qyKh/3saCLPzo6wszMDIrFIiV/aGgIXV1d+oO17MKvQywWK/8ogPFKpVKUz03v8SoAK4UM6OK3trbQ3NxMyU4kEkgkEpRseo+fnJzEycmJ7xfXqakp3N/fe+b6+/sxMDDga+7foIsHgHg87ntGY2Oj7xlfgd5qWExPT2Nzc5OWb6x427a1bdermU9LNhwRT0LEkxDxJEQ8CRFPQtsXKKUU8vk8xsfHfTm+ZVl4enqqukW0bVvrRqV/RZv4h4cHDA8P+5oRjUar5jKZDBYXF33N/Q6BWDL4X/yklU5tPb6pqQmjo6O64gKPtjO+vb0d2WwWyWTSl+Pv7OzAdV1fju0HWltNb28vtre3fTl2PB7H29tb1bzf15XvEpoev7y8zC7hS8h9PAkRT8II8aVSKXC3mkaIn5iYwM3NTXnsOA46OjqIFRkivpJYLIZMJkOtwUjxQUDEkwi9+HQ6jYuLC3YZVYRe/OHhIR4fH8vjtrY2bf8fXI9Qi0+lUtjb2/PMNTQ0YGRkhFTRH0L1n2TFYhE9PT3lseu6KJVK5XEkEsHz8zNaW1sZ5XkIzVrNbwqFQt33W1paNFVSn1C3mkoGBwfZJZQxRvzY2Bjy+Xxgnr+GqtU4joOlpaWa7yWTyZrPZFmE6uL6kzCm1QQNEU9CxJMQ8SREPAkRT0LEkxDxJEQ8CRFPQsSTEPEkfgENY09oJRwajwAAAABJRU5ErkJggg==\" y=\"-122.746017\"/>\n   </g>\n   <g id=\"matplotlib.axis_15\">\n    <g id=\"xtick_22\">\n     <g id=\"line2d_43\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"470.429458\" xlink:href=\"#m52cc47af11\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_43\">\n      <!-- 0 -->\n      <g transform=\"translate(467.248208 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_23\">\n     <g id=\"line2d_44\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"515.059999\" xlink:href=\"#m52cc47af11\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_44\">\n      <!-- 50 -->\n      <g transform=\"translate(508.697499 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_24\">\n     <g id=\"line2d_45\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"559.690541\" xlink:href=\"#m52cc47af11\" y=\"216.746017\"/>\n      </g>\n     </g>\n     <g id=\"text_45\">\n      <!-- 100 -->\n      <g transform=\"translate(550.146791 231.344454)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_16\">\n    <g id=\"ytick_22\">\n     <g id=\"line2d_46\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#m5b2242b6c4\" y=\"123.468184\"/>\n      </g>\n     </g>\n     <g id=\"text_46\">\n      <!-- 0 -->\n      <g transform=\"translate(456.620652 127.267403)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_47\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#m5b2242b6c4\" y=\"168.098726\"/>\n      </g>\n     </g>\n     <g id=\"text_47\">\n      <!-- 50 -->\n      <g transform=\"translate(450.258152 171.897945)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_24\">\n     <g id=\"line2d_48\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#m5b2242b6c4\" y=\"212.729268\"/>\n      </g>\n     </g>\n     <g id=\"text_48\">\n      <!-- 100 -->\n      <g transform=\"translate(443.895652 216.528487)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_38\">\n    <path d=\"M 469.983152 216.746017 \nL 469.983152 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_39\">\n    <path d=\"M 563.70729 216.746017 \nL 563.70729 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_40\">\n    <path d=\"M 469.983152 216.746017 \nL 563.70729 216.746017 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_41\">\n    <path d=\"M 469.983152 123.021879 \nL 563.70729 123.021879 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_9\">\n   <g id=\"patch_42\">\n    <path d=\"M 33.2875 329.214982 \nL 127.011638 329.214982 \nL 127.011638 235.490844 \nL 33.2875 235.490844 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pd7089acac0)\">\n    <image height=\"94\" id=\"imageb31797cb7e\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"33.2875\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAFlUlEQVR4nO2aTWgTXRSG35lG0zRGUkibRWsIRSFYYkEqMYqlm7oRRFQEaaFCoC7auAgEirhyYUGwoWCE+Ltp98WN9QcxSBKEQFAQxJCNRgzENhQJNbXJdVE6X+PENn9fziRzH5jFnLlz75knk2Hm3Cswxhg4DUekTkCtcPFEcPFEcPFEcPFEcPFEcPFEcPFEcPFEcPFEcPFEcPFEcPFEcPFEqEL86OgoDh8+DLvdDsVUwVmLMzExwTQaDQPAALDOzk524sQJVigUSPNq+Ts+m81iY2ND2s9kMgiHwzhz5gxyuRxZXi0vvqurC1qtVhZ/9uwZvF4vQUabtLx4n88Hl8tVUj4lLS8eAPx+PyYnJ9HW1kadioQqxAPAnTt3sHfvXuo0JFQjXmlw8URw8URw8URw8URw8USoRvyrV6+Qz+ep05BQjXiXy4X19XXqNCQ0lIP7/X58+vSprLazs7PYs2dP3cbu7+/HxYsX69ZfpZCKf/r0KV68eFFW23g8DlEU0dfXh7t379Y8ttVqxdDQUM39VAup+Ep4/vw5AECr1eLNmzdFxxYXF3Hw4EGCrKqnacRvkcvl8PHjx6LY4OCgrAB26dIl3Lt3r5GpVYSixAuCAJ1OJ4uvra3tOGW3uroqiwUCATx48EDa//uNJp/PI5vNAtj8F2k0jVVBKr67uxsWi0Xat1gsePv2razd8ePH8f37dwDA+vo6UqnUrn0zxnZ8fVxaWsK+ffsAAI8fP8bo6GhDq5cC2+lWUiDxeBxXrlwpisViMaytrdXU7/v373HkyJGa+qiEphNfiunpaSSTSWk/nU6X/ba0BRdfB758+YJHjx7J4rdu3Sqa+N7i8uXL8Pl8MJvNjUhvE7oFDo1Hp9NJyzy2trGxMfb169eG56KakgHw37fAdo4dO4be3t6G56Iq8U6nkzoFCVWJVxKqF5/NZkmqlqoTb7PZivavX79e8atnPVCVeI1GU/LLmAJViVcSXDwRVRfJbty4gQ8fPsjip0+fxtTUVE1JqYGqxUciEbx+/VoWD4VCReXYcuno6EAkEqk2naaj7mXhlZUVrKysVHyeIAjo6uqSxZeWlnD06NGidq2AYiZCGGP48eOHLO5wOCTZJpMJiUQCbW1tilvvXilVi+/u7saBAweKYr9+/UI6na45qe1sn8xIpVLQ6/UYGRnBw4cP0dnZCYPBUHZfjDF8+/atrvlVS13LwtFoFG63u+LzGGN49+5dxed5PB5picbg4OCuyz82NjZkbaxWK+bn53Hy5MmKx68FRdTjGWMYGxtDoVCQHXv58iWWl5d37SMQCMBgMECv1+Ps2bMl25QSPzc3h2vXrlWXeA0o4hkvCAIWFhZKHgsEAkgmk8jn85iZmflnH1evXgUAGI1GeDwe2O12nDt37v9Itz40fAagSvL5PHvy5Am7efOmbDKj1NbX18fGx8fZ+Pg4+/z5M2OMMZfLJWs3NzdHcj1NI36Lnz9/smAwyILBIDt//nxZP8LAwAA7deoUE0WxKH7hwgWWTCZJrqPpxG8nnU6zRCLBDh06VNYP8PfmdrvJclfEM75aTCYTTCYTYrEYCoUCstksenp6wDZvKOr0dqQlimR6vR4GgwFmsxm/f/9GKBSC0WhEe3s7dWr/pCXEbyEIAkRRhNPpRCaTwe3bt2Gz2aRNSeWGpn7U7Ibb7S76oOvo6Kh5xVm9aKk7vpng4ong4ong4ong4ong4ong4ong4ong4olQtfhoNIpoNEoytqrFRyIRhMNhkrFVLZ4SLp4IVYkXRfnlUpWKVSU+k8lg//79ADbXynu9XrIFtopYV9NIMpkMHA4HhoeHcf/+fbI8VCdeKajqUaMkuHgiuHgiuHgiuHgiuHgiuHgiuHgiuHgiuHgiuHgiuHgiuHgiuHgi/gDrRw9uKKu4GQAAAABJRU5ErkJggg==\" y=\"-235.214982\"/>\n   </g>\n   <g id=\"matplotlib.axis_17\">\n    <g id=\"xtick_25\">\n     <g id=\"line2d_49\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.733805\" xlink:href=\"#m52cc47af11\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_49\">\n      <!-- 0 -->\n      <g transform=\"translate(30.552555 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_26\">\n     <g id=\"line2d_50\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"78.364347\" xlink:href=\"#m52cc47af11\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_50\">\n      <!-- 50 -->\n      <g transform=\"translate(72.001847 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_27\">\n     <g id=\"line2d_51\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"122.994889\" xlink:href=\"#m52cc47af11\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_51\">\n      <!-- 100 -->\n      <g transform=\"translate(113.451139 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_18\">\n    <g id=\"ytick_25\">\n     <g id=\"line2d_52\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m5b2242b6c4\" y=\"235.93715\"/>\n      </g>\n     </g>\n     <g id=\"text_52\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 239.736369)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_26\">\n     <g id=\"line2d_53\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m5b2242b6c4\" y=\"280.567692\"/>\n      </g>\n     </g>\n     <g id=\"text_53\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 284.36691)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_27\">\n     <g id=\"line2d_54\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m5b2242b6c4\" y=\"325.198234\"/>\n      </g>\n     </g>\n     <g id=\"text_54\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 328.997452)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_43\">\n    <path d=\"M 33.2875 329.214982 \nL 33.2875 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_44\">\n    <path d=\"M 127.011638 329.214982 \nL 127.011638 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_45\">\n    <path d=\"M 33.2875 329.214982 \nL 127.011638 329.214982 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_46\">\n    <path d=\"M 33.2875 235.490844 \nL 127.011638 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_10\">\n   <g id=\"patch_47\">\n    <path d=\"M 178.852717 329.214982 \nL 272.576855 329.214982 \nL 272.576855 235.490844 \nL 178.852717 235.490844 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pec2a7b0133)\">\n    <image height=\"94\" id=\"imagefd7dd01e1d\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"178.852717\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAD0UlEQVR4nO2cPUjrUBiG37TiD2hBBR06d7BU6iC46qC1IoIilFgdBB2dHdzExUldpNRRcHITBEUcHRxc3DqUgAq6qA1YajTmTjeotWjvvT3fuf2+Z0ua8r59OJyEU04Mz/M8CMoJUBfgiognQsQTIeKJEPFEiHgiRDwRIp4IEU+EiCdCxBMh4okQ8USIeCJYi9/d3UU0GkU0GkUul1OabXBdjz8+Psbk5CSKxSIAIBQKIZ/Po7OzU0k+2xHvOI4vHQBs28bb25uyfJbiHcfB3d0daQd24l9eXrC3t4eFhQXSHuzEFwoFzM/Pl52fmJhAc3Ozsh7sxH/F7Owsstks2tralGWKeACpVArd3d1KM0U8EazEv76+oq+vj7oGAGbiAeDm5ubDcSAQQCCgXgM78Z/Z2NhAMplUnstePAAYhqE8k5V4nZalWInPZDLUFXxYiV9eXqau4MNKvE6wFp9Op5FOp0myG0hS/5J/dZPs6OhQ9sfHZ7QQXywWq5J5eHgI0zSrznFdt+rv1AotxIfDYTw+PlLXUArrOZ4SEU+EFlPNnzAwMIBEIvHj67e2tlAoFGrYqDq0EJ/JZOA4TlXf6enpQX9//4+vj8fjME2z6pxaoYX4VCpV84ypqSkEg8Ga5/wUmeOJEPFEsBZfKpVQKpVIslmL39nZQTabJclmJX5kZIS6gg8r8fv7+9QVfFiJ1wkRT4SIJ0LEEyHiiRDxRLAXb1kWHh4elOcq3fV3f3+Po6MjVXFluK6Lubm5svMHBwcYHx9X2kXpsrBlWZiZmVEZqS3spxoqRDwRIp4IpXN8PB6HbdsqI31isRiur68BQOkO7kooFR8MBpVuaXyPZVnwPA/5fB6RSISkw3vYTDWGYcB13TLpXV1daG1tVd6HjXjg6y03KysrGBwcVN6FlXidEPFEsBIvm8+IoNhWWQlW4mXEEyEjnggZ8URcXFxQV/BhJX5oaOjDcSQSIVs+YCX+M6OjoyRv7gCYi6eEjXjTNPH8/Exdw4eN+LOzMy3W4X/DQvzw8DCurq7KzlM+19e9+KenJ9i2XfYMPzY2hs3NTZpSqGPxtm0jl8vBNE2cn59/+KyxsRHhcJh0xNfNa8w9z8Pp6ak/sk9OTrC+vv7ltbFYDJeXlyrrlaHFPtd/RSKR+PYNHU1NTZienlbUqDJ1Jf471tbWEAqFsLS0RF2Fj/jt7W0sLi6ioUGPn6xHixqyurqKZDKJ3t5ebaQDdXZzvb29LXtsbG9vR0tLC1GrytSN+P+Nun2O1x0RT4SIJ0LEEyHiiRDxRIh4IkQ8Eb8A5w8NV2ToMHwAAAAASUVORK5CYII=\" y=\"-235.214982\"/>\n   </g>\n   <g id=\"matplotlib.axis_19\">\n    <g id=\"xtick_28\">\n     <g id=\"line2d_55\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.299023\" xlink:href=\"#m52cc47af11\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_55\">\n      <!-- 0 -->\n      <g transform=\"translate(176.117773 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_29\">\n     <g id=\"line2d_56\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"223.929565\" xlink:href=\"#m52cc47af11\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_56\">\n      <!-- 50 -->\n      <g transform=\"translate(217.567065 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_30\">\n     <g id=\"line2d_57\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.560107\" xlink:href=\"#m52cc47af11\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_57\">\n      <!-- 100 -->\n      <g transform=\"translate(259.016357 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_20\">\n    <g id=\"ytick_28\">\n     <g id=\"line2d_58\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#m5b2242b6c4\" y=\"235.93715\"/>\n      </g>\n     </g>\n     <g id=\"text_58\">\n      <!-- 0 -->\n      <g transform=\"translate(165.490217 239.736369)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_29\">\n     <g id=\"line2d_59\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#m5b2242b6c4\" y=\"280.567692\"/>\n      </g>\n     </g>\n     <g id=\"text_59\">\n      <!-- 50 -->\n      <g transform=\"translate(159.127717 284.36691)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_30\">\n     <g id=\"line2d_60\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#m5b2242b6c4\" y=\"325.198234\"/>\n      </g>\n     </g>\n     <g id=\"text_60\">\n      <!-- 100 -->\n      <g transform=\"translate(152.765217 328.997452)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_48\">\n    <path d=\"M 178.852717 329.214982 \nL 178.852717 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_49\">\n    <path d=\"M 272.576855 329.214982 \nL 272.576855 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_50\">\n    <path d=\"M 178.852717 329.214982 \nL 272.576855 329.214982 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_51\">\n    <path d=\"M 178.852717 235.490844 \nL 272.576855 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_11\">\n   <g id=\"patch_52\">\n    <path d=\"M 324.417935 329.214982 \nL 418.142073 329.214982 \nL 418.142073 235.490844 \nL 324.417935 235.490844 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p28589e5952)\">\n    <image height=\"94\" id=\"image3e8f07902f\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"324.417935\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAFSElEQVR4nO2czS8rXxjHv6eKhrbJZYEgFS8hxAIV7xZYIf4ASRNJ+Qe6I5FUJFYSGzsRInY2km4kQoQiLCwFSSssvFQaMl7qpXp+ixvNHS237dU+M/2dT3IS85iZfPtxejoz5xTjnHMIEo6GOsD/FSGeCCGeCCGeCCGeCCGeCCGeCCGeCCGeCCGeCCGeCCGeCC11gJ/G6/VidHQUANDT04Oenh7iRF/Akwifz8fb29s5AA6Al5SU8LW1NepYYUmqoeb9/R2bm5vBbZfLhYuLC8JEX5NU4r+CK3DKgXElpoqRx8dH6PV6WY0xBqfTiebmZqJU4UmqHs8YQ0FBgazGOVdkj08q8RkZGbIxXskklXg1IcQTIcQTIcQTIcQTIcQTIcQTIcQTIcQTIcQTIcQTIcQTIcQTIcQTQT7Z7fP5EAgEZDWtVov09HSiRImBtMd7PB5UV1dDr9fLmsViwdnZGR4fHynjxRVS8RaLBS6XK6S+tLSEoqIiTE5O4v7+niBZ/CEfar7DbrcDAEpKSoK1X79+KXetTDRQri1ZWFjg2dnZwXUwkbScnBxut9v5yspK2HO63e6QY5xOZ4Jf2d8h7fEWiwVGoxGSJAVrR0dHmJiY+PKY6+tr2O12VFRUwGw2B+uMMczNzcU1749C/Zf/zN3dHR8eHo7qXfDROjs7VdPjFSeec84lSeJut5v39vZGJZ4xxgsKClQhXtELmp6fn/H+/h7cvrm5QWlpKTjnIdf+3+F0OtHS0hKPiDGj6DtXnU6HzMzMYDOZTHh7e4PD4UBWVhaysrKQmppKHTMmFC3+M4wxMMbQ3d0Nr9cLr9eLrq4u6lgxoejr+Eior6+Xbft8PmxsbNCEiQJFj/GxcHp6iuLiYllNjPGCIEI8EUI8EUI8EWRXNa+vr/D7/bKaRqOBTqcjSpRYSHq8JEmwWq2ym6PMzEyUl5fj/Pxc1l5eXigixh2SHj8zM4PFxcWQ+vn5OUwmU8i+VVVVaGhogEaTPCOj4m+ghoaGAADz8/NIS0sDAOTn56O9vZ0y1j9DIr6xsRG1tbU4ODiI+JiBgYHgzxUVFejv75f9njGGkZGRn4oYd8juXHd3d3F8fAzg9zfzrFbrP387b3BwECMjI6q4c1XEIwPOOba2tkLqY2NjWF9fj/g8Go0GZrMZ+/v7sroQHyUejwcPDw9oaWnB1dVVzOdRonhFzkB95v7+nkuSxCVJ4tvb21yj0cgavpmVmpmZ4X6/n/olhKCK6zO9Xg+DwQCDwYCmpia8vb3JWmVl5ZfH6nQ6pKSkJDBtZCj+cvIzH5Mhf1JcXIxAIIBAIICTkxOiZNGhOvHhcDgcAML/EwmlooqhJhI455idnZXVzGYzysrKiBJ9T9KIBwCbzSbb7u3tRUNDA1Ga70kq8WpCiCdCiCdCiCdCiCdCiCdCFTdQDocDT09Pwe38/Hy0trYSJvoBqB8WRcLnpdd9fX0h+0xNTXHGWHCfqqoqvrq6SpA2MpJmqBkfH5dNpNTU1Ch6Qasqxe/s7KCjowMdHR3Y29ujjhMb1G+5SAj3LY+PlpeXx0tLS2XP5evq6vjV1RV17G9RxYfr4eEhcnNzZR+wH1xeXobUdDodcnJyEhEtZlQx1BgMBtze3sJoNFJH+TFU0eMBIC0tDW63G01NTQB+T4h4PB7c3d3J9tNqtSgsLCRIGB2Knuz+G9PT01heXgYAbG5uoq2tDXl5eWFXqSkNVYv/E5vNhsnJSdUs80sa8WpDHd0jCRHiiRDiiRDiiRDiiRDiiRDiiRDiiRDiiRDiiRDiiRDiifgPaAuAnWVw5SMAAAAASUVORK5CYII=\" y=\"-235.214982\"/>\n   </g>\n   <g id=\"matplotlib.axis_21\">\n    <g id=\"xtick_31\">\n     <g id=\"line2d_61\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.86424\" xlink:href=\"#m52cc47af11\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_61\">\n      <!-- 0 -->\n      <g transform=\"translate(321.68299 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_32\">\n     <g id=\"line2d_62\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.494782\" xlink:href=\"#m52cc47af11\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_62\">\n      <!-- 50 -->\n      <g transform=\"translate(363.132282 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_33\">\n     <g id=\"line2d_63\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"414.125324\" xlink:href=\"#m52cc47af11\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_63\">\n      <!-- 100 -->\n      <g transform=\"translate(404.581574 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_22\">\n    <g id=\"ytick_31\">\n     <g id=\"line2d_64\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#m5b2242b6c4\" y=\"235.93715\"/>\n      </g>\n     </g>\n     <g id=\"text_64\">\n      <!-- 0 -->\n      <g transform=\"translate(311.055435 239.736369)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_32\">\n     <g id=\"line2d_65\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#m5b2242b6c4\" y=\"280.567692\"/>\n      </g>\n     </g>\n     <g id=\"text_65\">\n      <!-- 50 -->\n      <g transform=\"translate(304.692935 284.36691)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_33\">\n     <g id=\"line2d_66\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#m5b2242b6c4\" y=\"325.198234\"/>\n      </g>\n     </g>\n     <g id=\"text_66\">\n      <!-- 100 -->\n      <g transform=\"translate(298.330435 328.997452)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_53\">\n    <path d=\"M 324.417935 329.214982 \nL 324.417935 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_54\">\n    <path d=\"M 418.142073 329.214982 \nL 418.142073 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_55\">\n    <path d=\"M 324.417935 329.214982 \nL 418.142073 329.214982 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_56\">\n    <path d=\"M 324.417935 235.490844 \nL 418.142073 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_12\">\n   <g id=\"patch_57\">\n    <path d=\"M 469.983152 329.214982 \nL 563.70729 329.214982 \nL 563.70729 235.490844 \nL 469.983152 235.490844 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pe66fc70c5d)\">\n    <image height=\"94\" id=\"image3b9af60a2b\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"469.983152\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAADkElEQVR4nO3dsUsycRzH8Y+miR3kEEQQFRIhLQ0NDQ0tCYFTW0NB0e5g0P8QDYEuQktURENDW1NzUQrl0NgQBCFEUKF2dXbPlHClPt7znPe5/H1f4ODpdV/fyE88TvKZpmlCuM7PHkBVEp5EwpNIeBIJTxJgD+CmmZkZvL29YXR0FIeHh9RZlAk/NTWFfD4P0zRxdXWFYDCIvb092jzKLDXFYhFfX1kMw8DT0xN1HmXCe42EJ5HwJBKeRMKTSHgSCU8i4UkkPImEJ5HwJMqGv7+/x8XFBe34yoYvFAo4ODigHV+Z8Ol0Gt3d3ewxapQJPz8/j66uLvYYNcqE9xoJTyLhSSQ8iYQnkfAkEp5EwpN49oKmTCaD7e1tW/uEw2Hkcrk2TeQsz4YvFou4ubmxvV9fX1/d7dfX1/85kbM8G/5fNbpCLBqNolqtWrYZhoGPjw8Eg0E3RrNQZo3/Hh0AstksstksYRoPv+NHRkYwPT1ta5+zs7M2TeM8X6f8+Mw0TSwvL+Pz87Pu48fHxyiXy5ZtsVgM6XQac3NzboxoZSpia2vLDAQCJoDaLZlM0uZRZo1PpVKUD9FGlAnvNRKeRMKTSHgSCU8i4UkkPImEJ5HwJJ45SVYqlWydFFtaWsL6+nobJ2ovz5wke319RW9vb8vPD4fD0DTtx/bh4WHk8/m6+2iahkqlUrufTCaRyWTsD+sAz7zj7apUKpaIXx4fHxtenGoYxo/75XIZoVDI/esqaafnvnl5ebGcOXTztrOzY+q67urr9cxSUyqVEI/HW37+w8MD7u7uHDt+oVDAxMSEY3/vbzyz1GiahvPz85aff3Jygv39fcu2arWKo6Mjp0drC8+EtyuRSCCRSFi2vb+/IxaLNdxnY2PjxzoPAAsLC+jv73d8xmY8s9S4oaenx/KBHI/Hsbi4iNnZWQwNDbk6y699xzthfHwcKysrlGPLN1cSCU8i4UkkPImEJ5HwJBKeRMKTSHgSCU8i4UkkPImEJ5HwJBKeRMKTSHgSCU8i4UkkPImEJ1HiKoNcLofn5+eGv/pm6Jjwu7u70HW97mObm5u4vb11eaLmOib82toa/Z9q2aHsGj85OYnV1VXa8TvmHd+qSCSC09NTRCIRjI2N0eZQJvzl5SUGBwfh9/sxMDDAHqdzLlrVdR3NXkooFILP53NxouY6Jvxvo+yHK5uEJ5HwJBKeRMKTSHgSCU/yB0SqbNV+lMrDAAAAAElFTkSuQmCC\" y=\"-235.214982\"/>\n   </g>\n   <g id=\"matplotlib.axis_23\">\n    <g id=\"xtick_34\">\n     <g id=\"line2d_67\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"470.429458\" xlink:href=\"#m52cc47af11\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_67\">\n      <!-- 0 -->\n      <g transform=\"translate(467.248208 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_35\">\n     <g id=\"line2d_68\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"515.059999\" xlink:href=\"#m52cc47af11\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_68\">\n      <!-- 50 -->\n      <g transform=\"translate(508.697499 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_36\">\n     <g id=\"line2d_69\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"559.690541\" xlink:href=\"#m52cc47af11\" y=\"329.214982\"/>\n      </g>\n     </g>\n     <g id=\"text_69\">\n      <!-- 100 -->\n      <g transform=\"translate(550.146791 343.81342)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_24\">\n    <g id=\"ytick_34\">\n     <g id=\"line2d_70\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#m5b2242b6c4\" y=\"235.93715\"/>\n      </g>\n     </g>\n     <g id=\"text_70\">\n      <!-- 0 -->\n      <g transform=\"translate(456.620652 239.736369)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_35\">\n     <g id=\"line2d_71\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#m5b2242b6c4\" y=\"280.567692\"/>\n      </g>\n     </g>\n     <g id=\"text_71\">\n      <!-- 50 -->\n      <g transform=\"translate(450.258152 284.36691)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_36\">\n     <g id=\"line2d_72\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#m5b2242b6c4\" y=\"325.198234\"/>\n      </g>\n     </g>\n     <g id=\"text_72\">\n      <!-- 100 -->\n      <g transform=\"translate(443.895652 328.997452)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_58\">\n    <path d=\"M 469.983152 329.214982 \nL 469.983152 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_59\">\n    <path d=\"M 563.70729 329.214982 \nL 563.70729 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_60\">\n    <path d=\"M 469.983152 329.214982 \nL 563.70729 329.214982 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_61\">\n    <path d=\"M 469.983152 235.490844 \nL 563.70729 235.490844 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_13\">\n   <g id=\"patch_62\">\n    <path d=\"M 33.2875 441.683948 \nL 127.011638 441.683948 \nL 127.011638 347.95981 \nL 33.2875 347.95981 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pa314e1f0f5)\">\n    <image height=\"94\" id=\"image0ad4cc79ec\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"33.2875\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAEYUlEQVR4nO3bPUhqfxzH8c8RLeyByCAbjIaMoiXoYcick4gKIpqCgmpoCYpWqamlyaGIGiragxoiB4cSXaInmgQLMip6QCKSyKfff/hz5Z5Sr7e/nu/R//cFQef8vPbtzeF47kklIYQAU5yGeoD/Kw5PhMMT4fBEODwRDk+EwxPh8EQ4PBEOT4TDE+HwRDg8EQ5PhMMT4fBEODwRDk9ESz1ANsViMXg8HgBAdXU1mpqaiCdKQxSIeDwu1tbWBAABQFgsFnF2dkY9VkoFdaqZmppKfO/1erG7u0s4TXoFFT6fcHgiHJ4IhyfC4YlweCIcngiHJ8LhiXB4IhyeCIcnwuGJcHgiHJ4IhyfC4YmQ/c11c3MTLpcr5frQ0BAGBgYUnEhZZOGtVitWVlZwfHycdP3o6AhLS0uJbZfLheLiYqXGyzmy8GazGXt7e+jq6sL19fW39UAggEAgkNhubGyEJEkpn+/09DQnc+YK6ds7ampqcHl5idraWgSDwbSPvbm5SbtuMpkQi8Vk++LxOKLRqGyfVquOd7RIQtB/pD4ej8NoNCIajeL19TVnP0er1eLx8REGgyFnPyNTqgj/y+3tLXp6er7t9/l8347cn6qqqsLLy0tWnuu/UFX4VEZHR/H8/Jxy3e124/39PaPnUkt4dZzw/mBrayvt+uLiIh4eHrC8vIx0x5EkSZiYmMj2eD+SF0d8JoQQ0Ol0shdYm80Gm82W2NZoNJienk57daSUvDjif6qzsxMzMzPUYyTFtwyIcHgiHJ4Ihyei+ItrMBiEz+eT7evo6FDNf+WVovhve3h4iMHBQdm+9fV1jI+Pq+IyTymqOMwmJydxf3+fCN/S0oL+/n7iqXJLFeEBYH5+PvG92WzGzs4OAMBut6O+vp5qrJxRTfjf+f1++P1+AMDFxQXKy8tl61arFYuLixSjZY3i4bu7u3F1dQUAGBsbg9vtTvv48/Pzb/tOTk6g1+tht9tzMaIyqD5uKIQQoVBIvL29icrKSqHRaIRGoxGSJCU+MpnuS6fTibKyMtnX18csLCxQ/nppkZ5qSkpKAEB2m9br9aKvrw8AEAqFEIlEkv7bSCSScu2Xj48PhMNhFBUVZWni7FH13cnZ2Vns7+8ntr9e/2die3sbIyMj2RwrK1Qd/nfRaBS9vb0AgKenp6Tn/mTUGl6VVzXJaLVaOJ1OAP9e6ayursrWNzY28Pn5KdvX3t6OhoYGxWb8K7QvMdljMBhkL6ytra3C4/FQj5VSwd4ka25uhsVioR4jpYINr3YcngiHJ8LhiXB4IgUR/uDgAOFwmHqMv1IQ4efm5mRv4TOZTIn7PWqV9+EdDgfu7u5k++rq6jA8PEw0UWbyPrzT6ZS9tdtoNMLhcNANlKG8D/+VXq9HW1sb9Rh/VHDh80Xe3J1MpbS0FBUVFYntr3+fVau8uR9faPhUQ4TDE+HwRDg8EQ5PhMMT4fBEODwRDk+EwxPh8EQ4PBEOT4TDE+HwRP4BuIchx6ON/rQAAAAASUVORK5CYII=\" y=\"-347.683948\"/>\n   </g>\n   <g id=\"matplotlib.axis_25\">\n    <g id=\"xtick_37\">\n     <g id=\"line2d_73\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.733805\" xlink:href=\"#m52cc47af11\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_73\">\n      <!-- 0 -->\n      <g transform=\"translate(30.552555 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_38\">\n     <g id=\"line2d_74\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"78.364347\" xlink:href=\"#m52cc47af11\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_74\">\n      <!-- 50 -->\n      <g transform=\"translate(72.001847 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_39\">\n     <g id=\"line2d_75\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"122.994889\" xlink:href=\"#m52cc47af11\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_75\">\n      <!-- 100 -->\n      <g transform=\"translate(113.451139 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_26\">\n    <g id=\"ytick_37\">\n     <g id=\"line2d_76\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m5b2242b6c4\" y=\"348.406115\"/>\n      </g>\n     </g>\n     <g id=\"text_76\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 352.205334)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_38\">\n     <g id=\"line2d_77\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m5b2242b6c4\" y=\"393.036657\"/>\n      </g>\n     </g>\n     <g id=\"text_77\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 396.835876)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_39\">\n     <g id=\"line2d_78\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m5b2242b6c4\" y=\"437.667199\"/>\n      </g>\n     </g>\n     <g id=\"text_78\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 441.466418)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_63\">\n    <path d=\"M 33.2875 441.683948 \nL 33.2875 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_64\">\n    <path d=\"M 127.011638 441.683948 \nL 127.011638 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_65\">\n    <path d=\"M 33.2875 441.683948 \nL 127.011638 441.683948 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_66\">\n    <path d=\"M 33.2875 347.95981 \nL 127.011638 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_14\">\n   <g id=\"patch_67\">\n    <path d=\"M 178.852717 441.683948 \nL 272.576855 441.683948 \nL 272.576855 347.95981 \nL 178.852717 347.95981 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p1522d09347)\">\n    <image height=\"94\" id=\"imagea2f82280d5\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"178.852717\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAEqklEQVR4nO2dTUgqXRjH/zNmFAgSIUGLiFoFEWlBmy6Em6AWQSBEBGWLoF2boFXboEXRImiRmyhoEUWRCy0ky1oGfWGBoG0s1Cztw3CyeRfve+W1tFvd63m8M+cHs5gzc3wefh4fzzgfCrIsy1AJsizj/v4+o62kpATFxcXMcxGZRyQklUpBr9dnLHNzcyS5qEp8IcHFE6F68Zubmzg/P2ceV1XiRVHE/Px8RtvW1hZ8Ph/7XJhHJEQURXR1dVGnAUBl4nPR398Pr9fLNKbqxAuCgKKiooy2aDSKRCIBloc0qhNfXl6O4+Nj6PX6jPampiZEIhFmeahOPADU1dVhcXHxXfvp6SmzUa9K8bkwm814eXlhEouLf4MgCEziqFZ8fX09LBYLWXzViq+urkZra+u7dl7jieClRuFw8URw8W/gNZ4IXuMVDhdPBBdPBBdPBBdPBBdPBBdPBBdPBBf/Bn7kSgQ/clU4XDwRRb/e5feRJAlGozHrto2NDdTU1OQ1fl9fHyYnJ1FZWZnXOF+BiXhZlnF2dpZ1m8lkgkajyWv8eDwOu92O4uJiXF9fM6vjH8FE/EfEYjEmce7u7gAAFRUV8Pv9Od9sPqvJE+FwGDqdDp2dnbi9vX23ndWnQWBxD5QkSWhrawPw73WKFNejfxZJkt5dW5kPmIj/P2tra+ju7s5os1gsf/QGsHA4DKfT+a2+ihXv9XqxvLyc0TY2NobS0tI/FsPj8eDHjx/pdavVitXV1U99nyhWPAtCoRAcDkd6vb29HR6PB/F4HFar9cO+XHwekGUZ+/v76fWVlRXMzMxk7JNMJqHVavOeC/l0kiWCIGRctnd4eJh1HxaobjpZKHDxRHDxRHDxRKhWfCAQwO7uLll8Vc1qfnJ1dYWRkRGsr6+T5aDKER+JREilAyoVXwhw8f+xs7ODUCiU95MyP1Fljc9GWVkZDAYDs3h8xBNBMuKDwSAuLy/T64IgoKWlpSDOhbKCqfhoNAqHwwG73Y6lpaV0uyAIkCSJWX0tBJiKDwQC6O3tZRmyYCmIL1dZljE4OJj3UiOKImw2W15jfJaCEA8ACwsLTOL4fD48PT0xifURBSOeFXt7e9QpACCYToqiqKrZSy6Yjnij0QhJkjA7O4vx8XGWoRGLxZg+c+xXqOZkt9lsRjAYxMXFRdbtR0dHaGhoYJaPamq8y+VCKpVCR0cHHh4ecHBwQJqPasQDgEajgcPhwMnJCdPRnQ3+Ww0RXDwRXDwRXDwRXDwRXDwRXDwRXDwRXDwRXDwRXDwRXDwRXDwRXDwRXDwRqvo9/vX1FS6XC36/nzoVdYm32WwYGhqiTgOAws+5Op1ObG9vp9enp6dz/uvNwMAADAYDJiYmmFxKqFjxbrcbw8PDX/4rOVa31Cuq1CQSifTDI25ubhAIBGgT+gBFjfjHx0fodLrfeg3+LIMvIsvyt+7o0Gg0GTWdP8vgGySTyS/3mZqawvPzc3phUd8BhYlvbGz8Vj9BENILKxRTagRBgNvtflfjm5ubUVVVlbNfbW1tvlPLimLEA4BWq8Xo6GhGW09PD0wmE1FGuVHUrOZvQlE1/m+CiyeCiyfiH4HLhvpn5qRKAAAAAElFTkSuQmCC\" y=\"-347.683948\"/>\n   </g>\n   <g id=\"matplotlib.axis_27\">\n    <g id=\"xtick_40\">\n     <g id=\"line2d_79\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.299023\" xlink:href=\"#m52cc47af11\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_79\">\n      <!-- 0 -->\n      <g transform=\"translate(176.117773 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_41\">\n     <g id=\"line2d_80\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"223.929565\" xlink:href=\"#m52cc47af11\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_80\">\n      <!-- 50 -->\n      <g transform=\"translate(217.567065 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_42\">\n     <g id=\"line2d_81\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.560107\" xlink:href=\"#m52cc47af11\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_81\">\n      <!-- 100 -->\n      <g transform=\"translate(259.016357 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_28\">\n    <g id=\"ytick_40\">\n     <g id=\"line2d_82\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#m5b2242b6c4\" y=\"348.406115\"/>\n      </g>\n     </g>\n     <g id=\"text_82\">\n      <!-- 0 -->\n      <g transform=\"translate(165.490217 352.205334)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_41\">\n     <g id=\"line2d_83\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#m5b2242b6c4\" y=\"393.036657\"/>\n      </g>\n     </g>\n     <g id=\"text_83\">\n      <!-- 50 -->\n      <g transform=\"translate(159.127717 396.835876)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_42\">\n     <g id=\"line2d_84\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#m5b2242b6c4\" y=\"437.667199\"/>\n      </g>\n     </g>\n     <g id=\"text_84\">\n      <!-- 100 -->\n      <g transform=\"translate(152.765217 441.466418)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_68\">\n    <path d=\"M 178.852717 441.683948 \nL 178.852717 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_69\">\n    <path d=\"M 272.576855 441.683948 \nL 272.576855 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_70\">\n    <path d=\"M 178.852717 441.683948 \nL 272.576855 441.683948 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_71\">\n    <path d=\"M 178.852717 347.95981 \nL 272.576855 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_15\">\n   <g id=\"patch_72\">\n    <path d=\"M 324.417935 441.683948 \nL 418.142073 441.683948 \nL 418.142073 347.95981 \nL 324.417935 347.95981 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p04c66b3db3)\">\n    <image height=\"94\" id=\"imaged6bd31fe0e\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"324.417935\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAEr0lEQVR4nO2cTSh8XxjHv3dmZBLKSF5iZCEvCzXlJQtlI2/JwsrOhjUWlhILRZFJI0oRCxI2VrOZBZliMUUJK2QWIy/NJMa83f/i9zeZGYbfj3ueGff51Kk5554555lPp2fGufeQZFmWwQhHQx2AWmHxRLB4Ilg8ESyeCBZPBIsngsUToWrxT09PcLlceHl5ET636sQHAgE4HA44HA6MjIwgLy8P8/PzeH5+FhuIrCKCwaC8vLwsA4gp09PTstfrFRaLqsT7/f53pb+W0dFRORgMColFkmX1bJIFAgGkpKTE7eP3+6HT6RSPRVU5XqPRYH19PW4fUetQVSse+LPqz8/PAQCrq6sYHx+PuC5qxSs/Q4Kh0+lQWVkJAMjLyyOLQ1WpJpFg8VFcXFwImYfFR1FVVSVkHlWLr6ioQHl5Ocncqhbf1NSEpqYmkrlVLZ4SFk8EiyeCxRPB4olg8USweCJYPBFJszs5ODgIv98frjc2NqKrq4swou+RNOLn5ubg9XrD9Z2dHSwuLobra2tryMzM/PY8Ly8v6OnpwdLS0rfHikfSiI/m4uIiYiextrY2fAPDZDJhZWXln8YNhUKw2Ww/EWJcklZ8NGdnZxGv09PTYbFY4r5HkiSlw/oYIbfUfwCDwSCnpaXJer0+7pMCr0WSJFmr1X5Y2tra5MfHR9nj8cjNzc0R7zUajYp/nqS753pycoLW1lYAwP39PR4fH398DqPRiMvLyx8f9y1JJ/4tU1NT2NzcDNf39/d/ZFwR4pMm1XxGKBSSu7u7v5SGPisiUs2v+XKVJAlLS0soLS2N2+/5+RmTk5OCovqYpE41/4LX68XGxgYAwGq1YnV1NaYPpxqFcTqd8u7urlxfX8+pRiQFBQUoKCiAwWAQPjdvkhHB4olg8USweCJYPBGqEG82mzEwMACfz0cdSphf93PS5XKht7c3om1vbw8PDw84Pz+HVqsFAFRXV2N4eBgLCws4PDwUHmdS/uVqs9nQ39//7jWfz4fT09NPx8jIyEBJSQmcTifu7u4irql+d7Kvrw/b29sx7T6fDx6PR5E59Xo9rq+vkZ2drcj4ryR0qvF4PLi9vf32OKmpqfD5fF86WCZJkuLSgV/45arValFUVBRR9vb2UFdXh8LCQurwwiT0iv8KZWVlyMrKCtfz8/OxtbUV089utyMQCKChoQFPT084Ojp6d7yamhrFYn1LQud4s9kMu90et8/Q0BBMJtNfjXt8fPzhkRtRxy0TWrxSuFwuzM7O4uDgAFarNeIan+xWkNzcXIyNjaG9vZ0sBlWKTwRYfBSiMi+Lj0LU02UsnggWHwWnGiI41fxyWDwRLD4KzvFEcI4nglc8EbziFcbr9cLtdse0i1rxqtwWBv4c1+zo6Ihp521hBXG73djd3Y1p7+zsFJZqVLPiLRZLOLW4XC7MzMzE9Lm5uUFOTo6QeFQjvri4GFdXV3H7iBSvylSTCLD4/9nZ2RF6MkQ14jUazbtlYmICbrcbLS0t4ecqRaCaHB8Khd5tlySJ5H8aqEZ8oqGaVJNosHgiWDwRLJ4IFk8EiyeCxRPB4olg8USweCJYPBEsnggWTwSLJ4LFE/EfpJEJ9duC7pwAAAAASUVORK5CYII=\" y=\"-347.683948\"/>\n   </g>\n   <g id=\"matplotlib.axis_29\">\n    <g id=\"xtick_43\">\n     <g id=\"line2d_85\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.86424\" xlink:href=\"#m52cc47af11\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_85\">\n      <!-- 0 -->\n      <g transform=\"translate(321.68299 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_44\">\n     <g id=\"line2d_86\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.494782\" xlink:href=\"#m52cc47af11\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_86\">\n      <!-- 50 -->\n      <g transform=\"translate(363.132282 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_45\">\n     <g id=\"line2d_87\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"414.125324\" xlink:href=\"#m52cc47af11\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_87\">\n      <!-- 100 -->\n      <g transform=\"translate(404.581574 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_30\">\n    <g id=\"ytick_43\">\n     <g id=\"line2d_88\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#m5b2242b6c4\" y=\"348.406115\"/>\n      </g>\n     </g>\n     <g id=\"text_88\">\n      <!-- 0 -->\n      <g transform=\"translate(311.055435 352.205334)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_44\">\n     <g id=\"line2d_89\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#m5b2242b6c4\" y=\"393.036657\"/>\n      </g>\n     </g>\n     <g id=\"text_89\">\n      <!-- 50 -->\n      <g transform=\"translate(304.692935 396.835876)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_45\">\n     <g id=\"line2d_90\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#m5b2242b6c4\" y=\"437.667199\"/>\n      </g>\n     </g>\n     <g id=\"text_90\">\n      <!-- 100 -->\n      <g transform=\"translate(298.330435 441.466418)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_73\">\n    <path d=\"M 324.417935 441.683948 \nL 324.417935 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_74\">\n    <path d=\"M 418.142073 441.683948 \nL 418.142073 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_75\">\n    <path d=\"M 324.417935 441.683948 \nL 418.142073 441.683948 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_76\">\n    <path d=\"M 324.417935 347.95981 \nL 418.142073 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_16\">\n   <g id=\"patch_77\">\n    <path d=\"M 469.983152 441.683948 \nL 563.70729 441.683948 \nL 563.70729 347.95981 \nL 469.983152 347.95981 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p18f5b8ca87)\">\n    <image height=\"94\" id=\"image8ac8ab2468\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"469.983152\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAEfUlEQVR4nO2cTyg8bxzH37u2tUuxB9uWgwNFUcpexMFlSylc5CCRk3JZWwoXKTe5LA4OnFyUcpC2aO8SqVUcFklJks1i2c3Oms/3IPrtHz++XzyfmZ3nVZPmmZ193r3m8Zm/OyYiIkiEY+YOYFSkeCakeCakeCakeCakeCakeCakeCakeCakeCakeCakeCakeCakeCakeCakeCakeCakeCakeCakeCYs3AG4CQQC2NnZAQCMj4+juLhYTMdkYDY3N6m6upoAEADq6uqiVColpG9Dl5pwOIzj4+P3+bW1NZCgp10MLT4XtbW1YjoS8n+lURRFoaGhofdSA4DsdruQvg094i0WCywWnuMLQ4vnRIpnQopnQopnQopnQopnQopnQopnQopnQopnQopnQopnQopnQopnQopnQopnQopnwlCPdxARjo6O0tpub2+zPnN4eIiioiJUVlb+WhYTkf5eInF9fY39/f2/Xu/l5QWdnZ1f+mxTUxO2t7f/uo+vorsRf3Nzg4mJCSwuLnJH+Ra6q/GXl5e6lw7oUHy+oDvxNTU18Pv9/7RuQUEBQqFQ2tTT05P2GZvNhlAohOXl5R9I+zG63Lk+Pz/j4eHhn9YtKyuDyWR6n/d6vZifn3+ft9vtiMfj3874GbrbuQJAYWEhnE4nd4xvobtSky9I8UxI8UwYWvzu7i729vbS2kZHR4X0rcujmp9ibm4Ow8PDaW2Kogh5gtjQI54TKT4DUQXAsOKJKKfk/55c/SaGrfGBQADt7e1pbaWlpYhEIkJqvKbOXJPJJM7OzkBEOUfeW3uuvwD+d9nb8rfvubi4yPr+k5MTYT/N0Yz4VCqFlZUVDAwMcEcRgmZqfCKRYJXe3d0Nu90urD/N1PhYLIaSkhK2/g8ODlBfXy+sP82MeJvNhoWFBe4YwtDMiAder7NnPgXw09zd3cHj8aS1+f1+DA4OCi01mtm5Aq/X2d1u96/2EYlEstqqqqqESgc0VGqMhhTPhBTPhKHEq6qK9fX1tLaGhgaUl5cLz6Kpo5rfRlEUWK3WtLbZ2Vl4vV7hWQw14kVdefwKhhKfeTWSFSGvI9IALS0tZDKZ0t7G1N/fT9FolCWPIUa8qqo4Pz/PuvHhcDjgcDh4QrFsboE8Pj5SY2Nj2kgHQK2trfT09MSWK2/F39/fUzgcpo6Ojizpb1NfXx+Fw2GKxWLC8+Wl+Gg0SmNjYx8Kz5w2NjaEZ9TURbLvQERYWloCEeH09BQzMzOfrlNXV4fm5mZUVFQISJiB8E39S0xOTmYdtXw0uVwump6epmAwyJY3L8T7fD6yWq2fCjebzbS6ukpbW1vckfOj1ASDQSSTyZzLpqam0NbWBuD1zNXtdmvjDJZ7y38HVVWpt7eXLBZLzhE+MjJC8XicO2ZOdC3e5/OR2WzOEt7U1ESJRIIUReGO+CG6LjWpVAqqqr7Pm0wmuFwuOJ1O2Gw2xmSfo2vxmdjtdlxdXXHH+BKGuFajRXQ94j0eT9rTAZk3ObSMoe5AaQlZapiQ4pmQ4pn4A1bb07V6dNPXAAAAAElFTkSuQmCC\" y=\"-347.683948\"/>\n   </g>\n   <g id=\"matplotlib.axis_31\">\n    <g id=\"xtick_46\">\n     <g id=\"line2d_91\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"470.429458\" xlink:href=\"#m52cc47af11\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_91\">\n      <!-- 0 -->\n      <g transform=\"translate(467.248208 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_47\">\n     <g id=\"line2d_92\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"515.059999\" xlink:href=\"#m52cc47af11\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_92\">\n      <!-- 50 -->\n      <g transform=\"translate(508.697499 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_48\">\n     <g id=\"line2d_93\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"559.690541\" xlink:href=\"#m52cc47af11\" y=\"441.683948\"/>\n      </g>\n     </g>\n     <g id=\"text_93\">\n      <!-- 100 -->\n      <g transform=\"translate(550.146791 456.282385)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_32\">\n    <g id=\"ytick_46\">\n     <g id=\"line2d_94\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#m5b2242b6c4\" y=\"348.406115\"/>\n      </g>\n     </g>\n     <g id=\"text_94\">\n      <!-- 0 -->\n      <g transform=\"translate(456.620652 352.205334)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_47\">\n     <g id=\"line2d_95\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#m5b2242b6c4\" y=\"393.036657\"/>\n      </g>\n     </g>\n     <g id=\"text_95\">\n      <!-- 50 -->\n      <g transform=\"translate(450.258152 396.835876)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_48\">\n     <g id=\"line2d_96\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#m5b2242b6c4\" y=\"437.667199\"/>\n      </g>\n     </g>\n     <g id=\"text_96\">\n      <!-- 100 -->\n      <g transform=\"translate(443.895652 441.466418)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_78\">\n    <path d=\"M 469.983152 441.683948 \nL 469.983152 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_79\">\n    <path d=\"M 563.70729 441.683948 \nL 563.70729 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_80\">\n    <path d=\"M 469.983152 441.683948 \nL 563.70729 441.683948 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_81\">\n    <path d=\"M 469.983152 347.95981 \nL 563.70729 347.95981 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_17\">\n   <g id=\"patch_82\">\n    <path d=\"M 33.2875 554.152913 \nL 127.011638 554.152913 \nL 127.011638 460.428775 \nL 33.2875 460.428775 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pbf709862c2)\">\n    <image height=\"94\" id=\"image7642127663\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"33.2875\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAEMElEQVR4nO3cuUsjcRjG8WcGDwwG4oGIwSMWNolYaGMsLES08EIbwU47CyFip3+BmsJSQSzSeBQiNpYKgoWIeASxEoIBhRBFMSHm+G2x7LDRXLuZ5J0Z3w8MOIfh5WtMTGaiJIQQYEUnUw/wU3F4IhyeCIcnwuGJcHgiHJ4IhyfC4YmUUA+gpng8juvrawBAdXU1mpubiSfKQBhEIpEQ29vbAoAAIHp7e8Xd3R31WGkZ6qFmampK+frk5AQ7OzuE02RmqPB6wuGJcHgiHJ4IhyfC4YkYJvzZ2RmEjs5iGib88PAwEomEst7a2oqOjg7CiTIzTPivnE4nxsbGqMdIy7DhtY7DE+HwRAwRvrOzE8FgUFnv7u7G5uYm4UTZGSJ8JBJJWpdlGWVlZUTT5MYQ4fWIwxMhOfV3cXGB09PTpG1zc3OQJIliHBIk4Y+Pj7GwsJC0zefzwe12U4xDQjMPNWtra5iZmaEeo2g0Ez4ej8Pj8cDhcMDhcGB9fZ16pILSTHgAiEaj8Hq98Hq9cLlcqKmpUZZwOKyrdx+zIQnvcrkQjUYxNDQEk8mU8phwOIxgMKgsZrMZDQ0N+Pj4SFpisViRp1cHyZOrLMuQZRmHh4cAgPb2dry+vuLx8THt98TjcTw9PaGysjJp+9bWFj4/P5O2RSIRBAIB1NbWqj+8SiShkd/fUCiE/v5+Zd3n82X8QWQzMDCAjY0NNDU1qTGe6jQT/qu9vT0cHBwAAPb39xEKhf75NsbHx7G6ugqbzab2ePmjvIwtV263WywtLSmX5/3L4vF4qMdPSRcXrc7PzyORSKCtre3bPrfbjaurK4Kp8kT9k8/X7e2tsNlsKe/t09PT4vn5mXrElHRxj8/Ebrd/+5O0q6sLu7u7sFgsqKqqIposM92HT6W8vFybT6h/0dQr1//x9vamyxdRug8/OTmJ+/t7Zb20tFTz93bAAOG/slqt8Hg81GNkZbjwesHhiXB4IhyeCIcnwuGJcHgiHJ4IhyfC4YlweCIcngiHJ8LhiRT0DNT7+3vOJyksFgtfpq2Wvr4+nJ+f53Ss1+uFJEkwmUza/pdWKtHMOVe73Q7g9+V8KysraY8zm81wOp3FGqtgNBP+j5ubGwwODqbdb7Vasbi4qKz7fL5ijKU6zYXPxu/3Y3Z2lnqMvBU0/PLyMl5eXnI6dmJiwlDXv2ejiYtWhRC4vLyE3+/HyMhIXrfV0tKCh4cHlSYrHE2E/yMWiyEQCKTd19jYmPU29BJeU4/xJSUlqK+vT7lPCPHtE9wAMDo6iqOjo0KPpjpNhc9EkqSUH5PX64sufsuACIcnwuGJcHgiHJ4IhyfC4YlweCIcngiHJ8LhiXB4IhyeCIcnwuGJcHgiHJ4Ihyeim1N/6fT09KCiokJZr6urI5wmd5q6yuAn4YcaIhyeCIcnwuGJ/AIvOCtukkVsUwAAAABJRU5ErkJggg==\" y=\"-460.152913\"/>\n   </g>\n   <g id=\"matplotlib.axis_33\">\n    <g id=\"xtick_49\">\n     <g id=\"line2d_97\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.733805\" xlink:href=\"#m52cc47af11\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_97\">\n      <!-- 0 -->\n      <g transform=\"translate(30.552555 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_50\">\n     <g id=\"line2d_98\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"78.364347\" xlink:href=\"#m52cc47af11\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_98\">\n      <!-- 50 -->\n      <g transform=\"translate(72.001847 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_51\">\n     <g id=\"line2d_99\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"122.994889\" xlink:href=\"#m52cc47af11\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_99\">\n      <!-- 100 -->\n      <g transform=\"translate(113.451139 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_34\">\n    <g id=\"ytick_49\">\n     <g id=\"line2d_100\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m5b2242b6c4\" y=\"460.875081\"/>\n      </g>\n     </g>\n     <g id=\"text_100\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 464.6743)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_50\">\n     <g id=\"line2d_101\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m5b2242b6c4\" y=\"505.505623\"/>\n      </g>\n     </g>\n     <g id=\"text_101\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 509.304841)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_51\">\n     <g id=\"line2d_102\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m5b2242b6c4\" y=\"550.136165\"/>\n      </g>\n     </g>\n     <g id=\"text_102\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 553.935383)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_83\">\n    <path d=\"M 33.2875 554.152913 \nL 33.2875 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_84\">\n    <path d=\"M 127.011638 554.152913 \nL 127.011638 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_85\">\n    <path d=\"M 33.2875 554.152913 \nL 127.011638 554.152913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_86\">\n    <path d=\"M 33.2875 460.428775 \nL 127.011638 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_18\">\n   <g id=\"patch_87\">\n    <path d=\"M 178.852717 554.152913 \nL 272.576855 554.152913 \nL 272.576855 460.428775 \nL 178.852717 460.428775 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p492cd8b690)\">\n    <image height=\"94\" id=\"imageb51d381387\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"178.852717\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAC+ElEQVR4nO3cv0tyURzH8c99fJCLTc6CBhL4B+jq0OAibeHY1NDW0OQabg7+CBwbc3ATcXEotKWpraEgoikHU4cWUU9TwsWHByE5n4v384KGe4Z7vryRu9xzc4wxBmLdH/YAQaXwJApPovAkCk+i8CQKT6LwJApPovAkCk+i8CQKT6LwJApPovAkCk+i8CR/2QPYdH9/j+VyiUgkgnQ6zR3GBESr1TLhcNgAMLFYzPR6Peo8jjHBeNmdSCTw/v6+us7n8+h0OrR59IwnUXgShSdReBKFJ1F4EoUnUXgShSdReBKFJ1F4EoUnUXgShSdReBKFJ1F4EoUnoZ4yODk5wWAwWFs/Pj5GuVwmTGQPNfxwOMTb29vaerVaRaPRWF0/PT0hFov9ai+/vdP35bma+XyO+Xy+ut7f39/6HrPZDF9fX9jb29v6vTcR2Gd8r9dDsVik7U/9xWcyGYRCIc/a5+cnHh4eSBPZQw1fKpXW1l5eXlCv11fXrVYLw+Fw63snEglks9mt33dTvj9J1mw28fHx8ev7XF5eYjKZrK7ZJ8l8H35bdIRPACg8jcKTKDyJwpMoPInCkyg8icKTKDyJwpMoPInCkyg8icKTKDyJwpNYf+d6e3uLi4sLq3teX19b3W8T1sNPJhM8Pj5a3fPw8BDT6XRt3RgDx3GszvIjEI+a8XiM5XLpWet2uzg/P/ccnLIpEOH/xRiDq6srz1FBm6w/aqLRKDKZjLX9Xl9fMRqNrO23Mdr/hrKkXq+bQqFgIpGIAeD5Ozg4MN1ulzLXzof/EY/HPdGTyaRpt9u0eQL7jE+lUjg6OqLtH9jwbApPovAkCk+i8CQKT6LwJApPovAkCk8SiPDGh595+fLL7m1YLBarlxy5XM7z4Zkf7Owv/ubmBq7rwnVd9Pt99jhrdjb8/7iui2QySZ0hcOHD4TDOzs5Qq9WocwQqvOM4KBaLqFQq7FF298vu5+dn3N3dedYcx8Hp6SntSIdnll0N73eBetT4icKTfAOcCOpP/K8O4gAAAABJRU5ErkJggg==\" y=\"-460.152913\"/>\n   </g>\n   <g id=\"matplotlib.axis_35\">\n    <g id=\"xtick_52\">\n     <g id=\"line2d_103\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.299023\" xlink:href=\"#m52cc47af11\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_103\">\n      <!-- 0 -->\n      <g transform=\"translate(176.117773 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_53\">\n     <g id=\"line2d_104\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"223.929565\" xlink:href=\"#m52cc47af11\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_104\">\n      <!-- 50 -->\n      <g transform=\"translate(217.567065 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_54\">\n     <g id=\"line2d_105\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.560107\" xlink:href=\"#m52cc47af11\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_105\">\n      <!-- 100 -->\n      <g transform=\"translate(259.016357 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_36\">\n    <g id=\"ytick_52\">\n     <g id=\"line2d_106\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#m5b2242b6c4\" y=\"460.875081\"/>\n      </g>\n     </g>\n     <g id=\"text_106\">\n      <!-- 0 -->\n      <g transform=\"translate(165.490217 464.6743)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_53\">\n     <g id=\"line2d_107\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#m5b2242b6c4\" y=\"505.505623\"/>\n      </g>\n     </g>\n     <g id=\"text_107\">\n      <!-- 50 -->\n      <g transform=\"translate(159.127717 509.304841)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_54\">\n     <g id=\"line2d_108\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.852717\" xlink:href=\"#m5b2242b6c4\" y=\"550.136165\"/>\n      </g>\n     </g>\n     <g id=\"text_108\">\n      <!-- 100 -->\n      <g transform=\"translate(152.765217 553.935383)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_88\">\n    <path d=\"M 178.852717 554.152913 \nL 178.852717 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_89\">\n    <path d=\"M 272.576855 554.152913 \nL 272.576855 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_90\">\n    <path d=\"M 178.852717 554.152913 \nL 272.576855 554.152913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_91\">\n    <path d=\"M 178.852717 460.428775 \nL 272.576855 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_19\">\n   <g id=\"patch_92\">\n    <path d=\"M 324.417935 554.152913 \nL 418.142073 554.152913 \nL 418.142073 460.428775 \nL 324.417935 460.428775 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p989bdbfff1)\">\n    <image height=\"94\" id=\"image8b21b584d4\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"324.417935\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAE+klEQVR4nO2dTyg8fRzH37P72BU2fzYHkdrLIn5tRGrJXiglDm5yspEDipNcXBz2ICklkYNNOTiIItniuA6k1n9rc5GolX9hmc1+n4OefdqsXTzPzGfMfl+1h/nO+H7eXo3Znf1+GgJjjIEjOxrqAIkKF08EF08EF08EF08EF08EF09EwogPBoMQRRFKuW1JCPE3NzcwmUxISUmB3++njgMgQcQ3Njbi4uICb29vMJvNODo6oo6kfvHb29u4ubkJb9/f36O+vh6bm5uEqVQu3u12o7OzE8fHxxHjT09PODg4IEr1jqrFr62tYWdn58N4Tk4O2tvbCRL9i6rFR8NgMGBiYoI6BgQ1fy3s9/tht9uxvLwcHjMajbi+viZM9Y6qz/js7Gykp6dTx4iKqsUrGS6eCC6eCC6eCC6eCC6eCC6eCC6eCC6eCC6eCC6eCC6eCC6eiL/kLujxeDA9Pf3tn9NqtRgdHYUgCBKkkh9Zxft8Ptjt9qirQvEQBAFerzfmMcnJyVhYWPhpPFmRRXxZWRlEUUQgEMDZ2dmP5mCMYXV1NeYxgiCgpKQEwHtngcPh+FEtOZB0BYoxhj9//pAsLOt0OqSlpeHx8RGiKIbHlbICJekZb7VaY0rX6/XQarVfmuv5+flbtUVRjGjrUBqSin97e4s6bjAYkJmZidnZWdTU1HxpLrPZjNfX15jHMMZwfn7+7ZwUSCreYrHA4/FAFEUkJSWhoqICANDS0oLu7u5vzRXvjRUAAoEAamtr8fDwgP39/R9llgvJuwx6enpwe3uL9PR0jI+PS1kqzMnJCYaGhrC7u4u9vb2IfUq5xoOpmMHBQQYg4mU0GqljMcYYU/Wda3NzM6xWK3WMqKhavMVigclkoo4RFVWLVzJcPBFcPBFcPBFcPBFcPBFcPBFcPBFcPBFcPBFcPBFcPBFcPBFcPBGyiHc4HBgdHZWj1K9B8r6avr4+jI2NQafTwePxSFpLr9djcnJS0hr/F5KK7+zsxMzMDEKhEF5eXuB0OqUsB41GE/FIlLa2Nknr/SekWlPs6upier3+w5qnnK+MjAyWmpqqyDVXyboMAoEAbDYbtra2pJj+xyily0DyFr7i4uLwn79Wq4XBYJCqHO7u7uIekxDi/8FisUAURRQVFUnWzcsYQ0lJCV5eXmI2xiaUeDm5urpCU1PTp5c4Ll5CfD4fRkZGAADr6+s4PT0N71OKeFV3kjHGWGtra8SnmuTkZDY8PEwdS92dZADQ29uLwsLC8LYc9xNfQfXiy8vLkZWVRR3jA6oXr1S4eCK4eCK4eCK4eCK4eCK4eCK4eCISUrzX60VHRwdphoQQ73K5kJeXF94WRRGXl5eEiRJEfGpqKjSayF81GAwiEAgQJUoQ8dFwuVzo7+8nq58w4qurqz+c9ZQoJ4lEbGxsYGpqCjab7ctPCpED2R+NJRWMMQwMDCAUCkWMr6ys4PDwkCjV56hGvN1uh9PpVMx/NouHatZcjUbjtx4MVFVVhbm5OeTn50uY6nN+nfiDgwPU1dV9GL+6uvry2V5QUAC32026MvXrLjXBYDDuzY8gCEhKSgpvu1wuVFZWhrc1Gg10Op1kGb/CrxMfD61Wi4aGBiwuLobHlPisSlWJLy0tRW5uLpaWlqijxOXXic/IyEBzc3PUffPz84r6rB6LX/fmqhZUf+eqVLh4Irh4Irh4Iv4G5LxlKr/8M6wAAAAASUVORK5CYII=\" y=\"-460.152913\"/>\n   </g>\n   <g id=\"matplotlib.axis_37\">\n    <g id=\"xtick_55\">\n     <g id=\"line2d_109\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.86424\" xlink:href=\"#m52cc47af11\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_109\">\n      <!-- 0 -->\n      <g transform=\"translate(321.68299 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_56\">\n     <g id=\"line2d_110\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.494782\" xlink:href=\"#m52cc47af11\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_110\">\n      <!-- 50 -->\n      <g transform=\"translate(363.132282 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_57\">\n     <g id=\"line2d_111\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"414.125324\" xlink:href=\"#m52cc47af11\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_111\">\n      <!-- 100 -->\n      <g transform=\"translate(404.581574 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_38\">\n    <g id=\"ytick_55\">\n     <g id=\"line2d_112\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#m5b2242b6c4\" y=\"460.875081\"/>\n      </g>\n     </g>\n     <g id=\"text_112\">\n      <!-- 0 -->\n      <g transform=\"translate(311.055435 464.6743)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_56\">\n     <g id=\"line2d_113\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#m5b2242b6c4\" y=\"505.505623\"/>\n      </g>\n     </g>\n     <g id=\"text_113\">\n      <!-- 50 -->\n      <g transform=\"translate(304.692935 509.304841)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_57\">\n     <g id=\"line2d_114\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"324.417935\" xlink:href=\"#m5b2242b6c4\" y=\"550.136165\"/>\n      </g>\n     </g>\n     <g id=\"text_114\">\n      <!-- 100 -->\n      <g transform=\"translate(298.330435 553.935383)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_93\">\n    <path d=\"M 324.417935 554.152913 \nL 324.417935 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_94\">\n    <path d=\"M 418.142073 554.152913 \nL 418.142073 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_95\">\n    <path d=\"M 324.417935 554.152913 \nL 418.142073 554.152913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_96\">\n    <path d=\"M 324.417935 460.428775 \nL 418.142073 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_20\">\n   <g id=\"patch_97\">\n    <path d=\"M 469.983152 554.152913 \nL 563.70729 554.152913 \nL 563.70729 460.428775 \nL 469.983152 460.428775 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pbff3c5c878)\">\n    <image height=\"94\" id=\"image687d16353a\" transform=\"scale(1 -1)translate(0 -94)\" width=\"94\" x=\"469.983152\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAF4AAABeCAYAAACq0qNuAAAC+UlEQVR4nO3csU/iYBzG8QeoxUQSB6MbYdVFFxk1MXFw7b/hZgIjTCyO7o4uDsbFxMGFFWJ0c3JyUDAdHMSAKX1vuFxzXPUuJ8jz0j6f5B2A0PzyVV8LVjLGGAOZuix7gLRSeBKFJ1F4EoUnUXgShSdReBKFJ1F4EoUnUXgShSdReBKFJ1F4EoUnUXgShSdReBKFJ1F4Eoc9wLR4nodut4vFxUVcXl6yxwFMCnieZ3K5nAFgstms2d3dZY9kUrHVdLtdDIdDAEAYhuh0OuSJtMfTKDyJwpMoPInCkyg8icKTKDyJwpMoPInCkyg8icKTKDyJwpMoPInCkyg8icKTKDyJwpMoPInCkyg8ifXh9/b2sLy8HK2dnR0YY6I1q6wP//LyAt/3o9VsNuG6LlzXxdHREd7e3mKr3++zx/4n68N/JAgCBEGAg4MDLCwsxNbW1hYeHh6iNRgM2CPHJPJq4evra5RKpQ8fcxwH6+vrU54oLmMs3yhrtRru7+8BAIPBAOfn52Mdb2lpCb7vT2K0sVgf/nevr684PDwEALRaLVxdXf33MRR+THd3d2i327H7fd9HtVr99HkK/036/f7IF6TRaIz8ZNgSPnG/XOfn57G9vR3dXllZIU7zuZk8nUwChSdReBKFJ1F4EqvPak5OTnB7ezvWMW5ubiY0zWRZHf7i4gKnp6fsMb6FthoShSexequZm5tDPp//8vODIIj+lR4AcrkcHh8fJzHa+EifoTAV9XrdAIiW4zgmDEP2WMaYlHyIxC/GovcDUxU+k8mwR4ikKrxNUhVeW40oPIvCkyg8icKTKDxJqsLrBRSJTefxVr87+RXPz8/Y398H8PNqM1slLnyv18PZ2dnIfcfHx9jY2LBqq0lMeGMMisUi3t/fY4+trq5ic3OTMNXnEhMeADqdzsgfPmyWqPB/KhQKKBQKcF2XPUpMos9qKpUKnp6eUC6X2aPEJOo73vM8hGEY3V5bWyNO83eJuz5+ViR6q7GZwpMoPInCk/wAOfaxVhhYOrkAAAAASUVORK5CYII=\" y=\"-460.152913\"/>\n   </g>\n   <g id=\"matplotlib.axis_39\">\n    <g id=\"xtick_58\">\n     <g id=\"line2d_115\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"470.429458\" xlink:href=\"#m52cc47af11\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_115\">\n      <!-- 0 -->\n      <g transform=\"translate(467.248208 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_59\">\n     <g id=\"line2d_116\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"515.059999\" xlink:href=\"#m52cc47af11\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_116\">\n      <!-- 50 -->\n      <g transform=\"translate(508.697499 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_60\">\n     <g id=\"line2d_117\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"559.690541\" xlink:href=\"#m52cc47af11\" y=\"554.152913\"/>\n      </g>\n     </g>\n     <g id=\"text_117\">\n      <!-- 100 -->\n      <g transform=\"translate(550.146791 568.751351)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_40\">\n    <g id=\"ytick_58\">\n     <g id=\"line2d_118\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#m5b2242b6c4\" y=\"460.875081\"/>\n      </g>\n     </g>\n     <g id=\"text_118\">\n      <!-- 0 -->\n      <g transform=\"translate(456.620652 464.6743)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_59\">\n     <g id=\"line2d_119\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#m5b2242b6c4\" y=\"505.505623\"/>\n      </g>\n     </g>\n     <g id=\"text_119\">\n      <!-- 50 -->\n      <g transform=\"translate(450.258152 509.304841)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_60\">\n     <g id=\"line2d_120\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"469.983152\" xlink:href=\"#m5b2242b6c4\" y=\"550.136165\"/>\n      </g>\n     </g>\n     <g id=\"text_120\">\n      <!-- 100 -->\n      <g transform=\"translate(443.895652 553.935383)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_98\">\n    <path d=\"M 469.983152 554.152913 \nL 469.983152 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_99\">\n    <path d=\"M 563.70729 554.152913 \nL 563.70729 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_100\">\n    <path d=\"M 469.983152 554.152913 \nL 563.70729 554.152913 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_101\">\n    <path d=\"M 469.983152 460.428775 \nL 563.70729 460.428775 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pb97b3d75c0\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"33.2875\" y=\"10.552913\"/>\n  </clipPath>\n  <clipPath id=\"p027cdb35df\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"178.852717\" y=\"10.552913\"/>\n  </clipPath>\n  <clipPath id=\"pa8e7ef2c91\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"324.417935\" y=\"10.552913\"/>\n  </clipPath>\n  <clipPath id=\"pbdff4b990f\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"469.983152\" y=\"10.552913\"/>\n  </clipPath>\n  <clipPath id=\"p800930ead5\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"33.2875\" y=\"123.021879\"/>\n  </clipPath>\n  <clipPath id=\"pd98fdeba03\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"178.852717\" y=\"123.021879\"/>\n  </clipPath>\n  <clipPath id=\"pa204df92fc\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"324.417935\" y=\"123.021879\"/>\n  </clipPath>\n  <clipPath id=\"p640e23e479\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"469.983152\" y=\"123.021879\"/>\n  </clipPath>\n  <clipPath id=\"pd7089acac0\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"33.2875\" y=\"235.490844\"/>\n  </clipPath>\n  <clipPath id=\"pec2a7b0133\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"178.852717\" y=\"235.490844\"/>\n  </clipPath>\n  <clipPath id=\"p28589e5952\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"324.417935\" y=\"235.490844\"/>\n  </clipPath>\n  <clipPath id=\"pe66fc70c5d\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"469.983152\" y=\"235.490844\"/>\n  </clipPath>\n  <clipPath id=\"pa314e1f0f5\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"33.2875\" y=\"347.95981\"/>\n  </clipPath>\n  <clipPath id=\"p1522d09347\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"178.852717\" y=\"347.95981\"/>\n  </clipPath>\n  <clipPath id=\"p04c66b3db3\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"324.417935\" y=\"347.95981\"/>\n  </clipPath>\n  <clipPath id=\"p18f5b8ca87\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"469.983152\" y=\"347.95981\"/>\n  </clipPath>\n  <clipPath id=\"pbf709862c2\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"33.2875\" y=\"460.428775\"/>\n  </clipPath>\n  <clipPath id=\"p492cd8b690\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"178.852717\" y=\"460.428775\"/>\n  </clipPath>\n  <clipPath id=\"p989bdbfff1\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"324.417935\" y=\"460.428775\"/>\n  </clipPath>\n  <clipPath id=\"pbff3c5c878\">\n   <rect height=\"93.724138\" width=\"93.724138\" x=\"469.983152\" y=\"460.428775\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plot_images(os.path.join(data_path, 'images_background/Korean/character07/'))\n",
    "print(\"Korean language, 20 samples of the seventh character.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading alphabet: Braille\n",
      "loading alphabet: Balinese\n",
      "loading alphabet: Anglo-Saxon_Futhorc\n",
      "loading alphabet: Futurama\n",
      "loading alphabet: Ojibwe_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Armenian\n",
      "loading alphabet: Cyrillic\n",
      "loading alphabet: Malay_(Jawi_-_Arabic)\n",
      "loading alphabet: Hebrew\n",
      "loading alphabet: Early_Aramaic\n",
      "loading alphabet: Blackfoot_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Korean\n",
      "loading alphabet: Arcadian\n",
      "loading alphabet: Japanese_(katakana)\n",
      "loading alphabet: Latin\n",
      "loading alphabet: Syriac_(Estrangelo)\n",
      "loading alphabet: Burmese_(Myanmar)\n",
      "loading alphabet: Greek\n",
      "loading alphabet: Gujarati\n",
      "loading alphabet: N_Ko\n",
      "loading alphabet: Bengali\n",
      "loading alphabet: Inuktitut_(Canadian_Aboriginal_Syllabics)\n",
      "loading alphabet: Tifinagh\n",
      "loading alphabet: Japanese_(hiragana)\n",
      "loading alphabet: Grantha\n",
      "loading alphabet: Sanskrit\n",
      "loading alphabet: Asomtavruli_(Georgian)\n",
      "loading alphabet: Tagalog\n",
      "loading alphabet: Alphabet_of_the_Magi\n",
      "loading alphabet: Mkhedruli_(Georgian)\n",
      "loading alphabet: Atlantean\n",
      "loading alphabet: Avesta\n",
      "loading alphabet: Mongolian\n",
      "loading alphabet: ULOG\n",
      "loading alphabet: Keble\n",
      "loading alphabet: Gurmukhi\n",
      "loading alphabet: Atemayar_Qelisayer\n",
      "loading alphabet: Angelic\n",
      "loading alphabet: Sylheti\n",
      "loading alphabet: Tibetan\n",
      "loading alphabet: Malayalam\n",
      "loading alphabet: Oriya\n",
      "loading alphabet: Kannada\n",
      "loading alphabet: Tengwar\n",
      "loading alphabet: Ge_ez\n",
      "loading alphabet: Manipuri\n",
      "loading alphabet: Glagolitic\n",
      "loading alphabet: Syriac_(Serto)\n",
      "loading alphabet: Aurek-Besh\n",
      "loading alphabet: Old_Church_Slavonic_(Cyrillic)\n"
     ]
    }
   ],
   "source": [
    "from load_data import *"
   ]
  },
  {
   "source": [
    "### Data Pre-processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training alphabets: \n\n['Braille', 'Balinese', 'Anglo-Saxon_Futhorc', 'Futurama', 'Ojibwe_(Canadian_Aboriginal_Syllabics)', 'Armenian', 'Cyrillic', 'Malay_(Jawi_-_Arabic)', 'Hebrew', 'Early_Aramaic', 'Blackfoot_(Canadian_Aboriginal_Syllabics)', 'Korean', 'Arcadian', 'Japanese_(katakana)', 'Latin', 'Syriac_(Estrangelo)', 'Burmese_(Myanmar)', 'Greek', 'Gujarati', 'N_Ko', 'Bengali', 'Inuktitut_(Canadian_Aboriginal_Syllabics)', 'Tifinagh', 'Japanese_(hiragana)', 'Grantha', 'Sanskrit', 'Asomtavruli_(Georgian)', 'Tagalog', 'Alphabet_of_the_Magi', 'Mkhedruli_(Georgian)']\n\nValidation alphabets:\n\n['Atlantean', 'Avesta', 'Mongolian', 'ULOG', 'Keble', 'Gurmukhi', 'Atemayar_Qelisayer', 'Angelic', 'Sylheti', 'Tibetan', 'Malayalam', 'Oriya', 'Kannada', 'Tengwar', 'Ge_ez', 'Manipuri', 'Glagolitic', 'Syriac_(Serto)', 'Aurek-Besh', 'Old_Church_Slavonic_(Cyrillic)']\n"
     ]
    }
   ],
   "source": [
    "save_path = '/home/sina/Desktop/data/'\n",
    "\n",
    "with open(os.path.join(save_path, \"train.pickle\"), \"rb\") as f:\n",
    "    (X, classes) = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(save_path, \"val.pickle\"), \"rb\") as f:\n",
    "    (Xval, val_classes) = pickle.load(f)\n",
    "\n",
    "print(\"Training alphabets: \\n\")\n",
    "print(list(classes.keys()), end=\"\\n\\n\")\n",
    "print(\"Validation alphabets:\", end=\"\\n\\n\")\n",
    "print(list(val_classes.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading data from /home/sina/Desktop/data/train.pickle\n",
      "loading data from /home/sina/Desktop/data/val.pickle\n"
     ]
    }
   ],
   "source": [
    "class Siamese_Loader:\n",
    "    \"\"\"For loading batches and testing tasks to a siamese net\"\"\"\n",
    "    def __init__(self, path, data_subsets = [\"train\", \"val\"]):\n",
    "        self.data = {}\n",
    "        self.categories = {}\n",
    "        self.info = {}\n",
    "        \n",
    "        for name in data_subsets:\n",
    "            file_path = os.path.join(path, name + \".pickle\")\n",
    "            print(\"loading data from {}\".format(file_path))\n",
    "            with open(file_path,\"rb\") as f:\n",
    "                (X,c) = pickle.load(f)\n",
    "                self.data[name] = X\n",
    "                self.categories[name] = c\n",
    "\n",
    "    def get_batch(self,batch_size,s=\"train\"):\n",
    "        \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "        X=self.data[s]\n",
    "        n_classes, n_examples, w, h = X.shape\n",
    "\n",
    "        #randomly sample several classes to use in the batch\n",
    "        categories = rng.choice(n_classes,size=(batch_size,),replace=False)\n",
    "        #initialize 2 empty arrays for the input image batch\n",
    "        pairs=[np.zeros((batch_size, h, w,1)) for i in range(2)]\n",
    "        #initialize vector for the targets, and make one half of it '1's, so 2nd half of batch has same class\n",
    "        targets=np.zeros((batch_size,))\n",
    "        targets[batch_size//2:] = 1\n",
    "        for i in range(batch_size):\n",
    "            category = categories[i]\n",
    "            idx_1 = rng.randint(0, n_examples)\n",
    "            pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, 1)\n",
    "            idx_2 = rng.randint(0, n_examples)\n",
    "            #pick images of same class for 1st half, different for 2nd\n",
    "            if i >= batch_size // 2:\n",
    "                category_2 = category  \n",
    "            else: \n",
    "                #add a random number to the category modulo n classes to ensure 2nd image has\n",
    "                # ..different category\n",
    "                category_2 = (category + rng.randint(1,n_classes)) % n_classes\n",
    "            pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(w, h,1)\n",
    "        return pairs, targets\n",
    "    \n",
    "    def generate(self, batch_size, s=\"train\"):\n",
    "        \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
    "        while True:\n",
    "            pairs, targets = self.get_batch(batch_size,s)\n",
    "            yield (pairs, targets)    \n",
    "\n",
    "    def make_oneshot_task(self,N,s=\"val\",language=None):\n",
    "        \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
    "        X=self.data[s]\n",
    "        n_classes, n_examples, w, h= X.shape\n",
    "        indices = rng.randint(0,n_examples,size=(N,))\n",
    "        if language is not None:\n",
    "            low, high = self.categories[s][language]\n",
    "            if N > high - low:\n",
    "                raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "            categories = rng.choice(range(low,high),size=(N,),replace=False)\n",
    "            \n",
    "        else:#if no language specified just pick a bunch of random letters\n",
    "            categories = rng.choice(range(n_classes),size=(N,),replace=False)            \n",
    "        true_category = categories[0]\n",
    "        ex1, ex2 = rng.choice(n_examples,replace=False,size=(2,))\n",
    "        test_image = np.asarray([X[true_category,ex1,:,:]]*N).reshape(N, w, h,1)\n",
    "        support_set = X[categories,indices,:,:]\n",
    "        support_set[0,:,:] = X[true_category,ex2]\n",
    "        support_set = support_set.reshape(N, w, h,1)\n",
    "        targets = np.zeros((N,))\n",
    "        targets[0] = 1\n",
    "        targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "        pairs = [test_image,support_set]\n",
    "\n",
    "        return pairs, targets\n",
    "    \n",
    "    def test_oneshot(self,model,N,k,s=\"val\",verbose=0):\n",
    "        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "        n_correct = 0\n",
    "        if verbose:\n",
    "            print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
    "        for i in range(k):\n",
    "            inputs, targets = self.make_oneshot_task(N,s)\n",
    "            probs = model.predict(inputs)\n",
    "            if np.argmax(probs) == np.argmax(targets):\n",
    "                n_correct+=1\n",
    "        percent_correct = (100.0*n_correct / k)\n",
    "        if verbose:\n",
    "            print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
    "        return percent_correct\n",
    "    \n",
    "    def train(self, model, epochs, verbosity):\n",
    "        model.fit_generator(self.generate(batch_size))\n",
    "    \n",
    "\n",
    "loader = Siamese_Loader(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'class Siamese_Loader:\\n    \"\"\"For loading batches and testing tasks to a siamese net\"\"\"\\n    def __init__(self, path, data_subsets = [\"train\", \"val\"]):\\n        self.data = {}\\n        self.categories = {}\\n        self.info = {}\\n        \\n        for name in data_subsets:\\n            file_path = os.path.join(path, name + \".pickle\")\\n            print(\"loading data from {}\".format(file_path))\\n            with open(file_path,\"rb\") as f:\\n                (X,c) = pickle.load(f)\\n                self.data[name] = X\\n                print(\"Shape of data loaded from pickle {}\".format(X.shape))\\n                self.categories[name] = c\\n\\n    def get_batch(self,batch_size,s=\"train\"):\\n        \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\\n        X=self.data[s]\\n        n_classes, n_examples, w, h, u = X.shape\\n\\n        #randomly sample several classes to use in the batch\\n        categories = rng.choice(n_classes,size=(batch_size,),replace=False)\\n        #initialize 2 empty arrays for the input image batch\\n        pairs=[np.zeros((batch_size, h, w, u)) for i in range(2)]\\n        #initialize vector for the targets, and make one half of it \\'1\\'s, so 2nd half of batch has same class\\n        targets=np.zeros((batch_size,))\\n        targets[batch_size//2:] = 1\\n        for i in range(batch_size):\\n            category = categories[i]\\n            idx_1 = rng.randint(0, n_examples)\\n            pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, u)\\n            idx_2 = rng.randint(0, n_examples)\\n            #pick images of same class for 1st half, different for 2nd\\n            if i >= batch_size // 2:\\n                category_2 = category  \\n            else: \\n                #add a random number to the category modulo n classes to ensure 2nd image has\\n                # ..different category\\n                category_2 = (category + rng.randint(1,n_classes)) % n_classes\\n            pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(w, h, u)\\n        return pairs, targets\\n    \\n    def generate(self, batch_size, s=\"train\"):\\n        \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\\n        while True:\\n            pairs, targets = self.get_batch(batch_size,s)\\n            yield (pairs, targets)    \\n\\n    def make_oneshot_task(self,N,s=\"val\",language=None):\\n        \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\\n        X=self.data[s]\\n        n_classes, n_examples, w, h, u = X.shape\\n        indices = rng.randint(0,n_examples,size=(N,))\\n        if language is not None:\\n            low, high = self.categories[s][language]\\n            if N > high - low:\\n                raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\\n            categories = rng.choice(range(low,high),size=(N,),replace=False)\\n            \\n        else:#if no language specified just pick a bunch of random letters\\n            categories = rng.choice(range(n_classes),size=(N,),replace=False)            \\n        true_category = categories[0]\\n        ex1, ex2 = rng.choice(n_examples,replace=False,size=(2,))\\n        test_image = np.asarray([X[true_category,ex1,:,:,:]]*N).reshape(N, w, h, u)\\n        support_set = X[categories,indices,:,:,:]\\n        support_set[0,:,:,:] = X[true_category,ex2]\\n        support_set = support_set.reshape(N, w, h, u)\\n        targets = np.zeros((N,))\\n        targets[0] = 1\\n        targets, test_image, support_set = shuffle(targets, test_image, support_set)\\n        pairs = [test_image,support_set]\\n\\n        return pairs, targets\\n    \\n    def test_oneshot(self,model,N,k,s=\"val\",verbose=0):\\n        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\\n        n_correct = 0\\n        if verbose:\\n            print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\\n        for i in range(k):\\n            inputs, targets = self.make_oneshot_task(N,s)\\n            probs = model.predict(inputs)\\n            if np.argmax(probs) == np.argmax(targets):\\n            #if np.argmax(probs) == 0:\\n                n_correct+=1\\n        percent_correct = (100.0*n_correct / k)\\n        if verbose:\\n            print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\\n        return percent_correct\\n    \\n    def train(self, model, epochs, verbosity):\\n        model.fit_generator(self.generate(batch_size))\\n    \\n\\nloader = Siamese_Loader(save_path)'"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "'''class Siamese_Loader:\n",
    "    \"\"\"For loading batches and testing tasks to a siamese net\"\"\"\n",
    "    def __init__(self, path, data_subsets = [\"train\", \"val\"]):\n",
    "        self.data = {}\n",
    "        self.categories = {}\n",
    "        self.info = {}\n",
    "        \n",
    "        for name in data_subsets:\n",
    "            file_path = os.path.join(path, name + \".pickle\")\n",
    "            print(\"loading data from {}\".format(file_path))\n",
    "            with open(file_path,\"rb\") as f:\n",
    "                (X,c) = pickle.load(f)\n",
    "                self.data[name] = X\n",
    "                print(\"Shape of data loaded from pickle {}\".format(X.shape))\n",
    "                self.categories[name] = c\n",
    "\n",
    "    def get_batch(self,batch_size,s=\"train\"):\n",
    "        \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "        X=self.data[s]\n",
    "        n_classes, n_examples, w, h, u = X.shape\n",
    "\n",
    "        #randomly sample several classes to use in the batch\n",
    "        categories = rng.choice(n_classes,size=(batch_size,),replace=False)\n",
    "        #initialize 2 empty arrays for the input image batch\n",
    "        pairs=[np.zeros((batch_size, h, w, u)) for i in range(2)]\n",
    "        #initialize vector for the targets, and make one half of it '1's, so 2nd half of batch has same class\n",
    "        targets=np.zeros((batch_size,))\n",
    "        targets[batch_size//2:] = 1\n",
    "        for i in range(batch_size):\n",
    "            category = categories[i]\n",
    "            idx_1 = rng.randint(0, n_examples)\n",
    "            pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, u)\n",
    "            idx_2 = rng.randint(0, n_examples)\n",
    "            #pick images of same class for 1st half, different for 2nd\n",
    "            if i >= batch_size // 2:\n",
    "                category_2 = category  \n",
    "            else: \n",
    "                #add a random number to the category modulo n classes to ensure 2nd image has\n",
    "                # ..different category\n",
    "                category_2 = (category + rng.randint(1,n_classes)) % n_classes\n",
    "            pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(w, h, u)\n",
    "        return pairs, targets\n",
    "    \n",
    "    def generate(self, batch_size, s=\"train\"):\n",
    "        \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
    "        while True:\n",
    "            pairs, targets = self.get_batch(batch_size,s)\n",
    "            yield (pairs, targets)    \n",
    "\n",
    "    def make_oneshot_task(self,N,s=\"val\",language=None):\n",
    "        \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
    "        X=self.data[s]\n",
    "        n_classes, n_examples, w, h, u = X.shape\n",
    "        indices = rng.randint(0,n_examples,size=(N,))\n",
    "        if language is not None:\n",
    "            low, high = self.categories[s][language]\n",
    "            if N > high - low:\n",
    "                raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "            categories = rng.choice(range(low,high),size=(N,),replace=False)\n",
    "            \n",
    "        else:#if no language specified just pick a bunch of random letters\n",
    "            categories = rng.choice(range(n_classes),size=(N,),replace=False)            \n",
    "        true_category = categories[0]\n",
    "        ex1, ex2 = rng.choice(n_examples,replace=False,size=(2,))\n",
    "        test_image = np.asarray([X[true_category,ex1,:,:,:]]*N).reshape(N, w, h, u)\n",
    "        support_set = X[categories,indices,:,:,:]\n",
    "        support_set[0,:,:,:] = X[true_category,ex2]\n",
    "        support_set = support_set.reshape(N, w, h, u)\n",
    "        targets = np.zeros((N,))\n",
    "        targets[0] = 1\n",
    "        targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "        pairs = [test_image,support_set]\n",
    "\n",
    "        return pairs, targets\n",
    "    \n",
    "    def test_oneshot(self,model,N,k,s=\"val\",verbose=0):\n",
    "        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "        n_correct = 0\n",
    "        if verbose:\n",
    "            print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
    "        for i in range(k):\n",
    "            inputs, targets = self.make_oneshot_task(N,s)\n",
    "            probs = model.predict(inputs)\n",
    "            if np.argmax(probs) == np.argmax(targets):\n",
    "            #if np.argmax(probs) == 0:\n",
    "                n_correct+=1\n",
    "        percent_correct = (100.0*n_correct / k)\n",
    "        if verbose:\n",
    "            print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
    "        return percent_correct\n",
    "    \n",
    "    def train(self, model, epochs, verbosity):\n",
    "        model.fit_generator(self.generate(batch_size))\n",
    "    \n",
    "\n",
    "loader = Siamese_Loader(save_path)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'def concat_images(X):\\n    fig, axs = plt.subplots(nrows=5, ncols=4, figsize=(4, 5))\\n    print(\"shape of X:{}\".format(X.shape))\\n    print(\"shape of axs:{}\".format(axs.shape))\\n    for ax, img in zip(axs.ravel(), X):\\n        ax.imshow(img)\\n        ax.axis(\\'off\\')\\n    return img\\n\\ndef plot_oneshot_task(pairs):\\n    \"\"\"Takes a one-shot task given to a siamese net and  \"\"\"\\n    fig, ax1 = plt.subplots(1)\\n    ax1.matshow(pairs[0][0].reshape(150,150,3), cmap=\\'gray\\')\\n    ax2 = concat_images(pairs[1])\\n    #print(\"shape of ax1:{}\".format(type(ax1)))\\n    #ax1.get_yaxis().set_visible(False)\\n    #ax1.get_xaxis().set_visible(False)\\n\\n    #ax2.matshow(img,cmap=\\'gray\\')\\n    #plt.xticks([])\\n    #plt.yticks([])\\n    #plt.show()\\n#example of a one-shot learning task\\npairs, targets = loader.make_oneshot_task(20,\"train\",\"Electronics,Camera&Photo\")\\nplot_oneshot_task(pairs)'"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "\n",
    "'''def concat_images(X):\n",
    "    fig, axs = plt.subplots(nrows=5, ncols=4, figsize=(4, 5))\n",
    "    print(\"shape of X:{}\".format(X.shape))\n",
    "    print(\"shape of axs:{}\".format(axs.shape))\n",
    "    for ax, img in zip(axs.ravel(), X):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    return img\n",
    "\n",
    "def plot_oneshot_task(pairs):\n",
    "    \"\"\"Takes a one-shot task given to a siamese net and  \"\"\"\n",
    "    fig, ax1 = plt.subplots(1)\n",
    "    ax1.matshow(pairs[0][0].reshape(150,150,3), cmap='gray')\n",
    "    ax2 = concat_images(pairs[1])\n",
    "    #print(\"shape of ax1:{}\".format(type(ax1)))\n",
    "    #ax1.get_yaxis().set_visible(False)\n",
    "    #ax1.get_xaxis().set_visible(False)\n",
    "\n",
    "    #ax2.matshow(img,cmap='gray')\n",
    "    #plt.xticks([])\n",
    "    #plt.yticks([])\n",
    "    #plt.show()\n",
    "#example of a one-shot learning task\n",
    "pairs, targets = loader.make_oneshot_task(20,\"train\",\"Electronics,Camera&Photo\")\n",
    "plot_oneshot_task(pairs)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"238.84pt\" version=\"1.1\" viewBox=\"0 0 116.736364 238.84\" width=\"116.736364pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-01-24T03:35:47.208744</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 238.84 \nL 116.736364 238.84 \nL 116.736364 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 10.7 109.536364 \nL 109.536364 109.536364 \nL 109.536364 10.7 \nL 10.7 10.7 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p5327d82ebb)\">\n    <image height=\"99\" id=\"imagee1709a373a\" transform=\"scale(1 -1)translate(0 -99)\" width=\"99\" x=\"10.7\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAGMAAABjCAYAAACPO76VAAACX0lEQVR4nO3c0W6cMBRFURPl/3/ZeYicWMQBM2N8tzlnP7WVqmZYuhhcmC3nnJND9BH9A7jfjAHKGKCMAcoYoIwByhigjAHKGKA+o/7hbdt+fu1NgO9CJqOGKL/f/5li0zGODro6yHSMs1OS8pSEnKZ61ghFkLCrKS/afwu9tM05H6KoTQfiPuMIRQkEgeG+Q2GoryMojJS0QXAYrVTWDSSG6nQgMVopTAcWozUdTwfBYqSkB4LGSEkLBI+h1LIYT5yOJTBULnWXwFDJGKCWxnjaurEMhsK6sQyGQsYAZQxQxgBlDFDGAGUMUMYAZQxQxgBlDFDTXyNrbe4p7Dv1hJgM5Rdk6hAYJXUUFEZJFSTs1eOzFF9Nno6xP7A9U6AyKeGnqbNXyZTaSN9ROGICQB/nciiM0ujTEvAjNkNi1I2EgX9UPkZJYVqWwUjpvqsqyiHA3mfs69nTehVr//eicJbB6GnUty1E4YTfZ8yq3M+8cmBn3XQ+ajJ6e2cX4M4pkcTYVx/gM5g798xkTlO9XTmVjT59GeOfelFGghjjpJkbmcbobAaIMS7UmpKRSL6aeqG7psSTAcoYoG4/TVE24VbIkwHKGKBCvphe5dGbq92+ZuScmwf/CER1XUHeZyg+TZjS5P8Df/X0pAIS8kCCUdohng55Z0EH/PjDQmDUKcPgMOrUYNAYJZUHopfAKD0dZSmMfe/i0D760hh1T4B5DEbdqjCPxGh1FSjisMhsoV89uBE7y8iNwru68hinJ2NiRw+n+f2MoEhLpuxkEDMGKGOAMgYoY4AyBihjgDIGqC+xSxaILN2RVgAAAABJRU5ErkJggg==\" y=\"-10.536364\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 10.7 109.536364 \nL 10.7 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 109.536364 109.536364 \nL 109.536364 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 10.7 109.536364 \nL 109.536364 109.536364 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 10.7 10.7 \nL 109.536364 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 10.7 228.14 \nL 109.536364 228.14 \nL 109.536364 129.303636 \nL 10.7 129.303636 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pf734fd0c43)\">\n    <image height=\"99\" id=\"imageddd84add1b\" transform=\"scale(1 -1)translate(0 -99)\" width=\"99\" x=\"10.7\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAGMAAABjCAYAAACPO76VAAAEHklEQVR4nO2c627DIAyFw9T3f+Xsx5SNMmwMtuFA/UmTqq4YwuFqQ9J1XfcVQPC1ugDBHyEGECEGECEGECEGECEGECEGECEGECEGECEGECEGECEGECEGECEGECEGECEGECEGECEGECEGEK/7jhA4CtEzgAgxgHh5Z5BS+v0cQyKPe8+47/v3L6X0Jo4WrS0uvWU5pbBiWFdeLoqVvRWV5gUrhnXl5XbRK3FFGUXD1KcKMpuuOQNRkJNEFYvhtRJaVZkpJbjVXXVp+1ROWVi0wu9OuexPN1PDtdbj1aK0dnvS13piLW1vmSS/pxr6dTU2fc8QYl35ozZbw1n5fy6PWiPTkg+5uX3pxrd7B64RiGsV0rw521K7td9ZiUNtbiVla4ph1TukXRhlXtKUYzSteDWVqz2SWatVj9o9CfEw5VlRM3rdDoh34IE/R8QzTmksR4hxCiEGECEGECEGEF078JFdZSCHFEPiTDtlfY8C67UVGVD4qXrTSb2tu+J+VMcSCzd3Lf1o3tZsJUYNrSPTUmDtGTG1GFxl9MQfegJDaEPTUyYqhsGRpxkSozcjScQQsZKlcPGRnudixeAq3XplJQ1Xtsq1M82wKwqtXlb7jFR+CdMOPtdas0dQadRWWT7tBnek95qJQU3k1EPkv0WI9FnnXZvMW3lA+KbKlQjFrpO8yQSegzBpemz6LJEs5buP6uzkdkArl+YQm9o3FdgBMWcEP4QYQIQYQIQYQIQYQHSdtQ14tLeDo2cIkFay9nawSAykXS6HR+8d8ZuNCrJ92DXHssFYXOzptTH13SEUFpVo1Xst4yH5LSbxzaUe3/1ot7XC0t1ee24vV7pEkFcrclYzvnJlZRkDmT0PtgT5ooLpXCtZLchThuvym7RH7VLpTC5YPoaosOnKoJBlo8h7mkVvq809rTNe3Zfyy4zyzDgbnqFVje0ZId8yD6peh1ZTmtuuXocQNPe2OSznplYZXZe23AEF66FLe8yzZMXJFfZKgGfXRdzRrz5zJY6BjzD7gUZ7x2oRHqpiILZaT1Ce9zivLUrFjnCcGDsTYgDRFGO12+OTEL9v6vl8Gp5L+N7wgco3JS0QuoieZeRO4ZdMeRc68lCnjVtr886ZMoHnES9UZgti5ijUZG7hYR1N38Lav9XL9KVtPiz0tsQ86GUdx2h9Z5UXZ/tfz7C8m8HFPayC/doLNDWvspcYLdtvwSVqCNFeNqTsUPTY97pPrnE4tsIE5PeSyzK1B35YvWy1aiiUbam9MqLJlclUDDS8Q7qakaAmEDXEbu2b6jkgRqWfQXna5sm3zP9tAvfo8l5DWtkbVi9LpVDD/XUVYlATt+VBMSvBOTcDgiBl66+VqfxO9O4Qywf8hLj6aH0NOQpRHhqV4TlMspoK5rD1auo0vgHEtmB4CT/DHAAAAABJRU5ErkJggg==\" y=\"-129.14\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\"/>\n   <g id=\"matplotlib.axis_2\"/>\n   <g id=\"patch_8\">\n    <path d=\"M 10.7 228.14 \nL 10.7 129.303636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 109.536364 228.14 \nL 109.536364 129.303636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 10.7 228.14 \nL 109.536364 228.14 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 10.7 129.303636 \nL 109.536364 129.303636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p5327d82ebb\">\n   <rect height=\"98.836364\" width=\"98.836364\" x=\"10.7\" y=\"10.7\"/>\n  </clipPath>\n  <clipPath id=\"pf734fd0c43\">\n   <rect height=\"98.836364\" width=\"98.836364\" x=\"10.7\" y=\"129.303636\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "def concat_images(X):\n",
    "    \"\"\"Concatenates a bunch of images into a big matrix for plotting purposes.\"\"\"\n",
    "    nc, h , w, _ = X.shape\n",
    "    X = X.reshape(nc, h, w)\n",
    "    n = np.ceil(np.sqrt(nc)).astype(\"int8\")\n",
    "    img = np.zeros((n*w,n*h))\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for example in range(nc):\n",
    "        img[x*w:(x+1)*w,y*h:(y+1)*h] = X[example]\n",
    "        y += 1\n",
    "        if y >= n:\n",
    "            y = 0\n",
    "            x += 1\n",
    "    return img\n",
    "\n",
    "\n",
    "def plot_oneshot_task(pairs):\n",
    "    fig,(ax1,ax2) = plt.subplots(2)\n",
    "    ax1.matshow(pairs[0][0].reshape(105,105), cmap='gray')\n",
    "    img = concat_images(pairs[1])\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax2.matshow(img,cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "pairs, targets = loader.make_oneshot_task(20,\"train\",\"Japanese_(katakana)\")\n",
    "plot_oneshot_task(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting training process!\n",
      "-------------------------------------\n",
      "Time for 10 iterations: 0.6232459545135498\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 22.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 22.8, previous best: -1\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 20 iterations: 10.336519002914429\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 17.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.7802428603172302, 0.6875],\n",
      "Time for 30 iterations: 19.338347911834717\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 18.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 40 iterations: 28.20417022705078\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 23.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 23.6, previous best: 22.8\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.782454788684845, 0.75],\n",
      "Time for 50 iterations: 38.7485568523407\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 22.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 60 iterations: 47.47203469276428\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 25.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 25.2, previous best: 23.6\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.6324524879455566, 0.8125],\n",
      "Time for 70 iterations: 56.86534595489502\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 32.4, previous best: 25.2\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 80 iterations: 66.85677862167358\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 26.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.7194472551345825, 0.8125],\n",
      "Time for 90 iterations: 75.71288967132568\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 24.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 100 iterations: 84.5169358253479\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 20.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.8578136563301086, 0.59375],\n",
      "Time for 110 iterations: 93.37538814544678\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 21.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 120 iterations: 102.14721274375916\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 16.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.6371089220046997, 0.8125],\n",
      "Time for 130 iterations: 110.8443284034729\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 23.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 140 iterations: 119.60868048667908\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 23.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5263923406600952, 0.84375],\n",
      "Time for 150 iterations: 128.43185877799988\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 21.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 160 iterations: 137.31813836097717\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 25.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.6027863025665283, 0.84375],\n",
      "Time for 170 iterations: 146.16408801078796\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 26.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 180 iterations: 154.96567463874817\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.6890575885772705, 0.8125],\n",
      "Time for 190 iterations: 163.69196605682373\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 200 iterations: 172.66685676574707\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 25.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5590035915374756, 0.8125],\n",
      "Time for 210 iterations: 181.91317892074585\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 24.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 220 iterations: 191.0262839794159\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 27.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.600003719329834, 0.75],\n",
      "Time for 230 iterations: 199.95228791236877\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 26.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 240 iterations: 209.05188632011414\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.6487475633621216, 0.78125],\n",
      "Time for 250 iterations: 217.79638481140137\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 260 iterations: 226.66507244110107\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5268858671188354, 0.78125],\n",
      "Time for 270 iterations: 235.62933588027954\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 280 iterations: 244.4015347957611\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 27.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.6444740891456604, 0.78125],\n",
      "Time for 290 iterations: 253.2213168144226\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 22.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 300 iterations: 261.88382172584534\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 29.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.6120402216911316, 0.84375],\n",
      "Time for 310 iterations: 270.7149934768677\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 23.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 320 iterations: 279.5055956840515\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 26.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.6412465572357178, 0.6875],\n",
      "Time for 330 iterations: 288.30286622047424\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 26.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 340 iterations: 297.0393314361572\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 22.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4881320595741272, 0.875],\n",
      "Time for 350 iterations: 305.6868164539337\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 360 iterations: 314.4224920272827\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 27.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.6901116371154785, 0.6875],\n",
      "Time for 370 iterations: 323.11943793296814\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 31.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 380 iterations: 332.08297991752625\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 36.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 36.4, previous best: 32.4\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.5944758057594299, 0.8125],\n",
      "Time for 390 iterations: 341.37420201301575\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 25.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 400 iterations: 350.0903730392456\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.46098047494888306, 0.9375],\n",
      "Time for 410 iterations: 358.8964536190033\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 27.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 420 iterations: 368.13743925094604\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 33.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5948585867881775, 0.84375],\n",
      "Time for 430 iterations: 376.9140384197235\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 440 iterations: 385.87496995925903\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 29.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4755113422870636, 0.8125],\n",
      "Time for 450 iterations: 394.46459579467773\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 460 iterations: 403.3271758556366\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.41590312123298645, 0.9375],\n",
      "Time for 470 iterations: 412.2314100265503\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 480 iterations: 420.9688787460327\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 26.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.46708613634109497, 0.8125],\n",
      "Time for 490 iterations: 429.7425639629364\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 500 iterations: 438.56659507751465\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 27.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5736825466156006, 0.78125],\n",
      "Time for 510 iterations: 447.63077902793884\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 29.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 520 iterations: 456.60211300849915\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.42139482498168945, 0.9375],\n",
      "Time for 530 iterations: 465.4998469352722\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 26.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 540 iterations: 474.20477771759033\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 26.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5105571150779724, 0.8125],\n",
      "Time for 550 iterations: 483.17747950553894\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 560 iterations: 492.38260436058044\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 27.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.578697681427002, 0.75],\n",
      "Time for 570 iterations: 501.14633226394653\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 36.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 36.8, previous best: 36.4\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 580 iterations: 510.6028709411621\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5938102006912231, 0.78125],\n",
      "Time for 590 iterations: 519.4951555728912\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 29.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 600 iterations: 528.3295917510986\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 26.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5559675097465515, 0.8125],\n",
      "Time for 610 iterations: 537.1569178104401\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 620 iterations: 546.0638074874878\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 27.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4916168451309204, 0.84375],\n",
      "Time for 630 iterations: 555.3558638095856\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 640 iterations: 564.3619372844696\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 27.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.469838947057724, 0.90625],\n",
      "Time for 650 iterations: 573.3285014629364\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 33.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 660 iterations: 582.5176177024841\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 33.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4910314381122589, 0.84375],\n",
      "Time for 670 iterations: 591.4076447486877\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 680 iterations: 600.7412395477295\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.45944374799728394, 0.8125],\n",
      "Time for 690 iterations: 609.5384719371796\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 700 iterations: 618.1206119060516\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 34.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5342510938644409, 0.8125],\n",
      "Time for 710 iterations: 626.8421008586884\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 31.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 720 iterations: 635.5651783943176\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5196667909622192, 0.75],\n",
      "Time for 730 iterations: 644.4028153419495\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 29.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 740 iterations: 653.0654373168945\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 33.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.40149974822998047, 0.9375],\n",
      "Time for 750 iterations: 662.1129381656647\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 34.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 760 iterations: 671.0943486690521\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 35.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4427688717842102, 0.8125],\n",
      "Time for 770 iterations: 679.7622015476227\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 33.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 780 iterations: 688.6855571269989\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.48673734068870544, 0.875],\n",
      "Time for 790 iterations: 697.4702563285828\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 800 iterations: 706.2922909259796\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 29.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4615255892276764, 0.875],\n",
      "Time for 810 iterations: 715.0907590389252\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 820 iterations: 724.0122637748718\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 29.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5519496202468872, 0.78125],\n",
      "Time for 830 iterations: 732.8615372180939\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 38.0, previous best: 36.8\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 840 iterations: 742.3194262981415\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 26.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5560888051986694, 0.84375],\n",
      "Time for 850 iterations: 751.1748085021973\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 860 iterations: 760.1704795360565\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 36.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4155110716819763, 0.875],\n",
      "Time for 870 iterations: 768.9705631732941\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 880 iterations: 777.6914377212524\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 37.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5449162721633911, 0.71875],\n",
      "Time for 890 iterations: 786.538387298584\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 25.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 900 iterations: 795.2594017982483\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 35.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.43761658668518066, 0.9375],\n",
      "Time for 910 iterations: 804.0370531082153\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 920 iterations: 812.7286913394928\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.43218567967414856, 0.90625],\n",
      "Time for 930 iterations: 821.5927789211273\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 35.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 940 iterations: 830.3315184116364\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 31.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5629640817642212, 0.8125],\n",
      "Time for 950 iterations: 839.2157497406006\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 33.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 960 iterations: 847.9120571613312\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 38.4, previous best: 38.0\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.3321216106414795, 0.9375],\n",
      "Time for 970 iterations: 857.4598441123962\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 980 iterations: 866.3531742095947\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 36.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4406071901321411, 0.90625],\n",
      "Time for 990 iterations: 875.270759344101\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 39.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 39.2, previous best: 38.4\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 1000 iterations: 884.696382522583\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 35.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.46663251519203186, 0.84375],\n",
      "Time for 1010 iterations: 893.569616317749\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1020 iterations: 902.4146015644073\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 33.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5480259656906128, 0.84375],\n",
      "Time for 1030 iterations: 911.2187449932098\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 33.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1040 iterations: 920.1507730484009\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3977200388908386, 0.84375],\n",
      "Time for 1050 iterations: 928.8200767040253\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 34.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1060 iterations: 937.5625185966492\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 35.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.42831099033355713, 0.875],\n",
      "Time for 1070 iterations: 946.58646941185\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 42.8, previous best: 39.2\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 1080 iterations: 955.9801778793335\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 33.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.43568819761276245, 0.90625],\n",
      "Time for 1090 iterations: 964.7539346218109\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 33.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1100 iterations: 973.6379916667938\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 34.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3937852680683136, 0.90625],\n",
      "Time for 1110 iterations: 982.5435659885406\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1120 iterations: 991.4012155532837\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 33.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4953521490097046, 0.875],\n",
      "Time for 1130 iterations: 1000.1450543403625\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 37.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1140 iterations: 1008.8147876262665\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 33.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.40525171160697937, 0.875],\n",
      "Time for 1150 iterations: 1017.7873842716217\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 34.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1160 iterations: 1026.511565208435\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.574360728263855, 0.84375],\n",
      "Time for 1170 iterations: 1035.342545747757\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 34.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1180 iterations: 1044.1482200622559\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.45870888233184814, 0.84375],\n",
      "Time for 1190 iterations: 1052.9739589691162\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 34.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1200 iterations: 1061.7610881328583\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 34.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5142995119094849, 0.8125],\n",
      "Time for 1210 iterations: 1070.4316456317902\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 37.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1220 iterations: 1079.3529841899872\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5120012760162354, 0.78125],\n",
      "Time for 1230 iterations: 1088.1952760219574\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 35.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1240 iterations: 1097.0718672275543\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 34.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.48701179027557373, 0.78125],\n",
      "Time for 1250 iterations: 1105.9607546329498\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 39.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1260 iterations: 1114.8789768218994\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 35.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.34099364280700684, 0.90625],\n",
      "Time for 1270 iterations: 1123.6369252204895\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 33.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1280 iterations: 1132.4581966400146\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 33.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.36242079734802246, 0.875],\n",
      "Time for 1290 iterations: 1141.246494293213\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1300 iterations: 1150.0051770210266\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4711778461933136, 0.78125],\n",
      "Time for 1310 iterations: 1158.698139667511\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 30.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1320 iterations: 1167.3748288154602\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 39.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.43922102451324463, 0.875],\n",
      "Time for 1330 iterations: 1176.430783033371\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 31.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1340 iterations: 1185.511164188385\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 35.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3996034562587738, 0.875],\n",
      "Time for 1350 iterations: 1194.50585770607\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 41.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1360 iterations: 1203.5720093250275\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 33.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.58837890625, 0.78125],\n",
      "Time for 1370 iterations: 1213.3246867656708\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 36.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1380 iterations: 1223.0205323696136\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3547978699207306, 0.9375],\n",
      "Time for 1390 iterations: 1232.075575351715\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 39.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1400 iterations: 1240.9806582927704\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3774714469909668, 0.84375],\n",
      "Time for 1410 iterations: 1249.8846633434296\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 36.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1420 iterations: 1258.7710664272308\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4195038378238678, 0.875],\n",
      "Time for 1430 iterations: 1267.461452960968\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 44.4, previous best: 42.8\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 1440 iterations: 1276.7776284217834\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 35.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.34277522563934326, 0.9375],\n",
      "Time for 1450 iterations: 1285.5937795639038\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 37.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1460 iterations: 1294.9118604660034\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 35.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.39396044611930847, 0.90625],\n",
      "Time for 1470 iterations: 1303.9461011886597\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1480 iterations: 1312.77010679245\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 36.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4682888090610504, 0.875],\n",
      "Time for 1490 iterations: 1321.616426706314\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1500 iterations: 1330.4922404289246\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 40.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.39319753646850586, 0.875],\n",
      "Time for 1510 iterations: 1339.2536976337433\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1520 iterations: 1348.119417667389\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3147212862968445, 0.90625],\n",
      "Time for 1530 iterations: 1356.914568901062\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 32.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1540 iterations: 1365.7860434055328\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 37.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5839017033576965, 0.84375],\n",
      "Time for 1550 iterations: 1374.7285063266754\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1560 iterations: 1383.8221893310547\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3748832643032074, 0.84375],\n",
      "Time for 1570 iterations: 1392.9709599018097\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 35.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1580 iterations: 1401.9831681251526\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4497426152229309, 0.875],\n",
      "Time for 1590 iterations: 1411.0674550533295\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1600 iterations: 1420.0189156532288\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 34.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5286558270454407, 0.78125],\n",
      "Time for 1610 iterations: 1428.940896987915\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 44.8, previous best: 44.4\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 1620 iterations: 1438.3850603103638\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 34.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.39837056398391724, 0.8125],\n",
      "Time for 1630 iterations: 1447.1725761890411\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 43.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1640 iterations: 1456.0881464481354\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3966369926929474, 0.90625],\n",
      "Time for 1650 iterations: 1465.0316817760468\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 35.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1660 iterations: 1473.8208339214325\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.43730711936950684, 0.90625],\n",
      "Time for 1670 iterations: 1482.7693934440613\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1680 iterations: 1491.881677389145\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 39.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4977262318134308, 0.90625],\n",
      "Time for 1690 iterations: 1500.5823965072632\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 41.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1700 iterations: 1509.4645366668701\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 40.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.44575273990631104, 0.875],\n",
      "Time for 1710 iterations: 1518.2162790298462\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 39.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1720 iterations: 1527.1559319496155\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 40.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4483519494533539, 0.84375],\n",
      "Time for 1730 iterations: 1536.3762238025665\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1740 iterations: 1545.515736579895\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 37.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.40905308723449707, 0.90625],\n",
      "Time for 1750 iterations: 1554.4927637577057\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 34.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1760 iterations: 1563.794604063034\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 39.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3527349829673767, 0.90625],\n",
      "Time for 1770 iterations: 1573.241331577301\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 40.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1780 iterations: 1582.2406697273254\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 36.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4140632152557373, 0.875],\n",
      "Time for 1790 iterations: 1591.4672470092773\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 41.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1800 iterations: 1600.929684638977\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 36.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.500822126865387, 0.84375],\n",
      "Time for 1810 iterations: 1610.4344630241394\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1820 iterations: 1619.5279054641724\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3850651681423187, 0.90625],\n",
      "Time for 1830 iterations: 1628.2688455581665\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 50.0, previous best: 44.8\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 1840 iterations: 1637.4976506233215\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3344821333885193, 0.9375],\n",
      "Time for 1850 iterations: 1646.0817818641663\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 40.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1860 iterations: 1654.7637581825256\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.42428678274154663, 0.84375],\n",
      "Time for 1870 iterations: 1663.5685653686523\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 35.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1880 iterations: 1672.4143342971802\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 50.0, previous best: 50.0\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.32072409987449646, 0.9375],\n",
      "Time for 1890 iterations: 1682.0820705890656\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1900 iterations: 1691.1218118667603\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 37.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.35332340002059937, 0.875],\n",
      "Time for 1910 iterations: 1699.9326009750366\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 37.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1920 iterations: 1708.8479487895966\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 41.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5719105005264282, 0.71875],\n",
      "Time for 1930 iterations: 1717.55339884758\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 39.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1940 iterations: 1726.223209619522\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4146730899810791, 0.84375],\n",
      "Time for 1950 iterations: 1735.2125549316406\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 47.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1960 iterations: 1744.1667943000793\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 40.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.44037729501724243, 0.8125],\n",
      "Time for 1970 iterations: 1753.2390370368958\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 1980 iterations: 1762.2655596733093\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3515525460243225, 0.9375],\n",
      "Time for 1990 iterations: 1771.5415036678314\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 50.4, previous best: 50.0\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 2000 iterations: 1781.7494134902954\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 40.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2821282744407654, 0.9375],\n",
      "Time for 2010 iterations: 1790.6393592357635\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2020 iterations: 1799.2880945205688\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.450250506401062, 0.84375],\n",
      "Time for 2030 iterations: 1808.043681383133\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2040 iterations: 1816.4717900753021\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 39.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.29924553632736206, 0.90625],\n",
      "Time for 2050 iterations: 1824.9935896396637\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2060 iterations: 1833.841688156128\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5611763596534729, 0.71875],\n",
      "Time for 2070 iterations: 1842.9542665481567\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 40.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2080 iterations: 1851.9909238815308\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 39.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3085089325904846, 0.90625],\n",
      "Time for 2090 iterations: 1861.4020137786865\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 47.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2100 iterations: 1870.551777124405\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 47.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2591383457183838, 0.9375],\n",
      "Time for 2110 iterations: 1879.5658507347107\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2120 iterations: 1888.5540218353271\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.6620587110519409, 0.71875],\n",
      "Time for 2130 iterations: 1897.3834764957428\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 38.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2140 iterations: 1906.1622002124786\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 49.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3953026235103607, 0.90625],\n",
      "Time for 2150 iterations: 1915.115352153778\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2160 iterations: 1924.102693080902\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 39.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.35983648896217346, 0.875],\n",
      "Time for 2170 iterations: 1933.0386681556702\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2180 iterations: 1942.1691088676453\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 45.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.36684054136276245, 0.875],\n",
      "Time for 2190 iterations: 1951.3395540714264\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 51.6, previous best: 50.4\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 2200 iterations: 1961.0224182605743\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.32744482159614563, 0.875],\n",
      "Time for 2210 iterations: 1969.9318532943726\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 41.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2220 iterations: 1978.7608020305634\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4397554397583008, 0.875],\n",
      "Time for 2230 iterations: 1987.7982001304626\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 40.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2240 iterations: 1996.7360668182373\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 40.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.33131518959999084, 0.90625],\n",
      "Time for 2250 iterations: 2005.7792155742645\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2260 iterations: 2014.5381214618683\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 52.8, previous best: 51.6\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.39222458004951477, 0.84375],\n",
      "Time for 2270 iterations: 2023.8410897254944\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2280 iterations: 2032.6933896541595\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 45.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4687533378601074, 0.78125],\n",
      "Time for 2290 iterations: 2041.3469660282135\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 40.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2300 iterations: 2050.188409090042\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 47.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3344186544418335, 0.9375],\n",
      "Time for 2310 iterations: 2059.0079321861267\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 41.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2320 iterations: 2067.995309114456\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 37.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.34422731399536133, 0.9375],\n",
      "Time for 2330 iterations: 2076.8954191207886\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 45.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2340 iterations: 2085.8318502902985\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5674742460250854, 0.71875],\n",
      "Time for 2350 iterations: 2094.6883738040924\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 45.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2360 iterations: 2103.561223745346\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.28310686349868774, 0.96875],\n",
      "Time for 2370 iterations: 2112.371586084366\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2380 iterations: 2121.1848106384277\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 41.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.31681153178215027, 0.90625],\n",
      "Time for 2390 iterations: 2130.029586315155\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 43.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2400 iterations: 2138.927598953247\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 47.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4099414050579071, 0.84375],\n",
      "Time for 2410 iterations: 2147.7996537685394\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2420 iterations: 2156.7235531806946\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.511975109577179, 0.84375],\n",
      "Time for 2430 iterations: 2165.7021124362946\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 39.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2440 iterations: 2174.687780380249\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.39775973558425903, 0.8125],\n",
      "Time for 2450 iterations: 2183.4935159683228\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2460 iterations: 2192.2010209560394\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.268325537443161, 0.90625],\n",
      "Time for 2470 iterations: 2200.946925163269\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2480 iterations: 2209.7162318229675\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.392609179019928, 0.90625],\n",
      "Time for 2490 iterations: 2218.5639402866364\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2500 iterations: 2227.3592245578766\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.38425493240356445, 0.84375],\n",
      "Time for 2510 iterations: 2236.1019146442413\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2520 iterations: 2244.976436138153\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 47.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3012545704841614, 0.96875],\n",
      "Time for 2530 iterations: 2253.616916179657\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 41.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2540 iterations: 2262.3675150871277\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.30060532689094543, 0.96875],\n",
      "Time for 2550 iterations: 2270.9985008239746\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 49.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2560 iterations: 2279.855890750885\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3467797338962555, 0.875],\n",
      "Time for 2570 iterations: 2288.518243789673\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2580 iterations: 2297.1740131378174\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20107975602149963, 0.96875],\n",
      "Time for 2590 iterations: 2305.9569289684296\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 47.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2600 iterations: 2314.765805006027\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.33993685245513916, 0.875],\n",
      "Time for 2610 iterations: 2323.4928438663483\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2620 iterations: 2332.1879518032074\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.46068134903907776, 0.8125],\n",
      "Time for 2630 iterations: 2341.0223865509033\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2640 iterations: 2349.659042596817\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 49.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.42539462447166443, 0.75],\n",
      "Time for 2650 iterations: 2358.469515323639\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 45.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2660 iterations: 2367.1431090831757\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 45.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5501977801322937, 0.71875],\n",
      "Time for 2670 iterations: 2375.8923959732056\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2680 iterations: 2384.668886899948\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3339745104312897, 0.9375],\n",
      "Time for 2690 iterations: 2393.31178855896\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2700 iterations: 2402.179310321808\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 41.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2797234058380127, 0.9375],\n",
      "Time for 2710 iterations: 2410.845702648163\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 43.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2720 iterations: 2419.658804178238\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 49.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.36031484603881836, 0.84375],\n",
      "Time for 2730 iterations: 2428.480897426605\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2740 iterations: 2437.47460436821\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.33181124925613403, 0.90625],\n",
      "Time for 2750 iterations: 2446.3923799991608\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2760 iterations: 2455.317704439163\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.30245286226272583, 0.9375],\n",
      "Time for 2770 iterations: 2464.2082555294037\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 49.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2780 iterations: 2473.0428371429443\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 45.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.352191299200058, 0.9375],\n",
      "Time for 2790 iterations: 2481.80233335495\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2800 iterations: 2490.4610681533813\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 47.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.365008145570755, 0.90625],\n",
      "Time for 2810 iterations: 2499.3584797382355\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 45.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2820 iterations: 2508.056207895279\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3244221806526184, 0.9375],\n",
      "Time for 2830 iterations: 2516.9921445846558\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2840 iterations: 2525.7067201137543\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3793674111366272, 0.90625],\n",
      "Time for 2850 iterations: 2534.4978539943695\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 41.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2860 iterations: 2543.207844018936\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 45.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.33064520359039307, 0.90625],\n",
      "Time for 2870 iterations: 2552.2551667690277\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2880 iterations: 2561.3175065517426\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.35886508226394653, 0.84375],\n",
      "Time for 2890 iterations: 2570.160894870758\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2900 iterations: 2579.180806875229\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.35098472237586975, 0.9375],\n",
      "Time for 2910 iterations: 2588.124839067459\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2920 iterations: 2596.94055891037\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 45.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3468688726425171, 0.9375],\n",
      "Time for 2930 iterations: 2605.69696187973\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 45.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2940 iterations: 2614.4720549583435\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3456916809082031, 0.90625],\n",
      "Time for 2950 iterations: 2623.1763212680817\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 2960 iterations: 2632.0017323493958\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.37394487857818604, 0.90625],\n",
      "Time for 2970 iterations: 2640.766832590103\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 55.6, previous best: 52.8\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 2980 iterations: 2650.3850145339966\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3454025685787201, 0.9375],\n",
      "Time for 2990 iterations: 2659.183213710785\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3000 iterations: 2667.959630012512\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2584078311920166, 0.90625],\n",
      "Time for 3010 iterations: 2676.702844142914\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 45.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3020 iterations: 2685.464682817459\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 47.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2848528027534485, 0.9375],\n",
      "Time for 3030 iterations: 2694.216210126877\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3040 iterations: 2702.9021995067596\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 56.8, previous best: 55.6\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.22931940853595734, 1.0],\n",
      "Time for 3050 iterations: 2712.277316093445\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 45.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3060 iterations: 2720.970685005188\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 49.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3456432819366455, 0.84375],\n",
      "Time for 3070 iterations: 2729.896211862564\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3080 iterations: 2738.555276155472\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2889937162399292, 0.90625],\n",
      "Time for 3090 iterations: 2747.3339569568634\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3100 iterations: 2756.059415578842\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3408176898956299, 0.9375],\n",
      "Time for 3110 iterations: 2764.8288114070892\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3120 iterations: 2773.6214888095856\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3644760251045227, 0.84375],\n",
      "Time for 3130 iterations: 2782.549067735672\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 49.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3140 iterations: 2791.3655128479004\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.325160950422287, 0.90625],\n",
      "Time for 3150 iterations: 2800.0666160583496\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 47.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3160 iterations: 2808.9493696689606\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.48161324858665466, 0.8125],\n",
      "Time for 3170 iterations: 2817.7565228939056\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3180 iterations: 2826.663096189499\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 49.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22373703122138977, 0.9375],\n",
      "Time for 3190 iterations: 2835.3008666038513\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3200 iterations: 2844.376059770584\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.427095502614975, 0.875],\n",
      "Time for 3210 iterations: 2853.456642150879\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3220 iterations: 2862.2562341690063\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3546305298805237, 0.90625],\n",
      "Time for 3230 iterations: 2871.156281709671\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3240 iterations: 2879.853480577469\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3692004680633545, 0.875],\n",
      "Time for 3250 iterations: 2888.811110496521\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3260 iterations: 2897.715006351471\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 49.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21603982150554657, 0.96875],\n",
      "Time for 3270 iterations: 2906.6768894195557\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3280 iterations: 2915.725340604782\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 49.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.30226826667785645, 0.9375],\n",
      "Time for 3290 iterations: 2924.772898197174\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3300 iterations: 2933.7155985832214\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2922931909561157, 0.9375],\n",
      "Time for 3310 iterations: 2942.707462310791\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3320 iterations: 2951.6219830513\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3918454945087433, 0.90625],\n",
      "Time for 3330 iterations: 2960.653626680374\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3340 iterations: 2969.777445077896\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2825171947479248, 0.9375],\n",
      "Time for 3350 iterations: 2978.6690537929535\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3360 iterations: 2987.554664850235\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 49.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.45146995782852173, 0.84375],\n",
      "Time for 3370 iterations: 2996.997702598572\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3380 iterations: 3006.434343099594\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.415865421295166, 0.875],\n",
      "Time for 3390 iterations: 3015.395480155945\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3400 iterations: 3024.328535556793\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 43.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.40262293815612793, 0.875],\n",
      "Time for 3410 iterations: 3033.149024248123\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3420 iterations: 3042.2999544143677\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4652215838432312, 0.8125],\n",
      "Time for 3430 iterations: 3051.2304673194885\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3440 iterations: 3059.97212433815\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.40668219327926636, 0.78125],\n",
      "Time for 3450 iterations: 3068.74108171463\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3460 iterations: 3077.4841616153717\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.33184367418289185, 0.90625],\n",
      "Time for 3470 iterations: 3086.3179914951324\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3480 iterations: 3095.2493505477905\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.39683640003204346, 0.875],\n",
      "Time for 3490 iterations: 3104.131742477417\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3500 iterations: 3112.9056866168976\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3297177255153656, 0.90625],\n",
      "Time for 3510 iterations: 3121.7228059768677\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3520 iterations: 3130.392251253128\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4498023986816406, 0.875],\n",
      "Time for 3530 iterations: 3139.347599506378\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3540 iterations: 3148.13875579834\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3294864892959595, 0.9375],\n",
      "Time for 3550 iterations: 3156.7646367549896\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3560 iterations: 3165.622936964035\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 57.2, previous best: 56.8\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.4633418917655945, 0.78125],\n",
      "Time for 3570 iterations: 3174.860043525696\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 57.6, previous best: 57.2\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 3580 iterations: 3184.340922832489\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2916776239871979, 0.875],\n",
      "Time for 3590 iterations: 3193.0587787628174\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3600 iterations: 3201.99552321434\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 61.6, previous best: 57.6\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.28169509768486023, 0.9375],\n",
      "Time for 3610 iterations: 3211.434560060501\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3620 iterations: 3220.420141220093\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3876553177833557, 0.84375],\n",
      "Time for 3630 iterations: 3229.1276388168335\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3640 iterations: 3237.9112498760223\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3707752227783203, 0.84375],\n",
      "Time for 3650 iterations: 3246.7304077148438\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3660 iterations: 3255.4727008342743\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.33856675028800964, 0.9375],\n",
      "Time for 3670 iterations: 3264.251026391983\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3680 iterations: 3273.003449678421\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.33486396074295044, 0.9375],\n",
      "Time for 3690 iterations: 3282.0567121505737\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3700 iterations: 3290.859783411026\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 49.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.36254435777664185, 0.90625],\n",
      "Time for 3710 iterations: 3299.7453994750977\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3720 iterations: 3308.6419591903687\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2596074044704437, 0.9375],\n",
      "Time for 3730 iterations: 3317.747008562088\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3740 iterations: 3327.0697984695435\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.34792208671569824, 0.90625],\n",
      "Time for 3750 iterations: 3336.1192333698273\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3760 iterations: 3345.437665939331\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4329346716403961, 0.84375],\n",
      "Time for 3770 iterations: 3354.1313881874084\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3780 iterations: 3362.8567690849304\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3022657334804535, 0.9375],\n",
      "Time for 3790 iterations: 3371.8020730018616\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3800 iterations: 3380.7414648532867\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.32069170475006104, 0.9375],\n",
      "Time for 3810 iterations: 3389.66112780571\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3820 iterations: 3398.5027730464935\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24743233621120453, 0.96875],\n",
      "Time for 3830 iterations: 3407.337893486023\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3840 iterations: 3416.2161524295807\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.396955668926239, 0.875],\n",
      "Time for 3850 iterations: 3425.075159072876\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3860 iterations: 3433.801003217697\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26666828989982605, 0.875],\n",
      "Time for 3870 iterations: 3442.69483089447\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3880 iterations: 3451.434807062149\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 44.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2528359293937683, 0.96875],\n",
      "Time for 3890 iterations: 3460.3055131435394\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3900 iterations: 3469.05273604393\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 62.0, previous best: 61.6\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.38145798444747925, 0.875],\n",
      "Time for 3910 iterations: 3478.4457714557648\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3920 iterations: 3487.4737918376923\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3807826340198517, 0.8125],\n",
      "Time for 3930 iterations: 3496.5062844753265\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 47.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3940 iterations: 3505.5279397964478\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.29404833912849426, 0.90625],\n",
      "Time for 3950 iterations: 3514.5625727176666\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3960 iterations: 3523.6996824741364\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3054471015930176, 0.90625],\n",
      "Time for 3970 iterations: 3532.8120007514954\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 3980 iterations: 3542.025366306305\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23422083258628845, 0.9375],\n",
      "Time for 3990 iterations: 3551.118716239929\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4000 iterations: 3560.181193113327\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5031710267066956, 0.8125],\n",
      "Time for 4010 iterations: 3569.228175640106\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4020 iterations: 3578.261268377304\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2852264940738678, 0.9375],\n",
      "Time for 4030 iterations: 3587.2690036296844\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4040 iterations: 3596.2500352859497\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.35890519618988037, 0.9375],\n",
      "Time for 4050 iterations: 3605.1429529190063\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4060 iterations: 3614.012358903885\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 62.0, previous best: 62.0\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.44600674510002136, 0.8125],\n",
      "Time for 4070 iterations: 3623.3231921195984\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 46.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4080 iterations: 3632.289894104004\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 49.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20242077112197876, 0.9375],\n",
      "Time for 4090 iterations: 3641.2179243564606\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4100 iterations: 3650.0955510139465\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22415843605995178, 0.96875],\n",
      "Time for 4110 iterations: 3659.16193819046\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4120 iterations: 3668.0395522117615\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21326906979084015, 1.0],\n",
      "Time for 4130 iterations: 3676.93519115448\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4140 iterations: 3685.723049879074\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4206743538379669, 0.84375],\n",
      "Time for 4150 iterations: 3694.55189704895\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4160 iterations: 3703.4345824718475\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.29125839471817017, 0.9375],\n",
      "Time for 4170 iterations: 3712.37841629982\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4180 iterations: 3721.292294025421\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3381201922893524, 0.875],\n",
      "Time for 4190 iterations: 3730.078240633011\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4200 iterations: 3739.0859985351562\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3352569341659546, 0.9375],\n",
      "Time for 4210 iterations: 3747.9090299606323\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 48.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4220 iterations: 3756.688211917877\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.30555641651153564, 0.90625],\n",
      "Time for 4230 iterations: 3765.4261684417725\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4240 iterations: 3774.2495110034943\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.44779524207115173, 0.8125],\n",
      "Time for 4250 iterations: 3783.305595636368\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4260 iterations: 3792.400188446045\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2783889174461365, 0.90625],\n",
      "Time for 4270 iterations: 3801.4873929023743\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4280 iterations: 3810.5484454631805\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22064924240112305, 0.96875],\n",
      "Time for 4290 iterations: 3819.5737595558167\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4300 iterations: 3828.6775097846985\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.38168129324913025, 0.84375],\n",
      "Time for 4310 iterations: 3837.804109811783\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4320 iterations: 3846.922872066498\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24347589910030365, 0.96875],\n",
      "Time for 4330 iterations: 3855.8848683834076\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4340 iterations: 3864.694026708603\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19357463717460632, 1.0],\n",
      "Time for 4350 iterations: 3873.8985171318054\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4360 iterations: 3882.7368907928467\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.35989803075790405, 0.875],\n",
      "Time for 4370 iterations: 3891.7095572948456\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4380 iterations: 3900.715082168579\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2080918550491333, 1.0],\n",
      "Time for 4390 iterations: 3909.573076248169\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4400 iterations: 3918.5692434310913\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.31596386432647705, 0.9375],\n",
      "Time for 4410 iterations: 3927.745717525482\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4420 iterations: 3936.827053308487\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.36090147495269775, 0.875],\n",
      "Time for 4430 iterations: 3945.733305454254\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4440 iterations: 3954.838059425354\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22988273203372955, 1.0],\n",
      "Time for 4450 iterations: 3963.8449923992157\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4460 iterations: 3973.1029164791107\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5099339485168457, 0.84375],\n",
      "Time for 4470 iterations: 3981.9539771080017\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4480 iterations: 3990.933949947357\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1789149045944214, 1.0],\n",
      "Time for 4490 iterations: 3999.7435545921326\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 50.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4500 iterations: 4008.759868144989\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3030216693878174, 0.9375],\n",
      "Time for 4510 iterations: 4017.644424200058\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4520 iterations: 4026.4201769828796\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18423715233802795, 1.0],\n",
      "Time for 4530 iterations: 4035.377494096756\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4540 iterations: 4044.3409900665283\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2680618464946747, 0.9375],\n",
      "Time for 4550 iterations: 4053.6036491394043\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4560 iterations: 4062.5354046821594\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 62.4, previous best: 62.0\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.2268403321504593, 1.0],\n",
      "Time for 4570 iterations: 4072.059275150299\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4580 iterations: 4081.078129529953\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2213056981563568, 0.9375],\n",
      "Time for 4590 iterations: 4090.190727710724\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4600 iterations: 4099.5088312625885\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.29793980717658997, 0.90625],\n",
      "Time for 4610 iterations: 4108.195236206055\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4620 iterations: 4117.66596364975\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2668243646621704, 0.9375],\n",
      "Time for 4630 iterations: 4126.738951444626\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4640 iterations: 4135.583402872086\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3584563732147217, 0.875],\n",
      "Time for 4650 iterations: 4144.481601476669\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4660 iterations: 4153.5781881809235\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3966307044029236, 0.90625],\n",
      "Time for 4670 iterations: 4162.5608768463135\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4680 iterations: 4171.610882759094\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2326684147119522, 0.96875],\n",
      "Time for 4690 iterations: 4180.539653778076\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4700 iterations: 4189.4091556072235\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2353174388408661, 0.9375],\n",
      "Time for 4710 iterations: 4198.45715713501\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4720 iterations: 4207.216646909714\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2230059653520584, 0.96875],\n",
      "Time for 4730 iterations: 4216.1853103637695\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4740 iterations: 4224.9915380477905\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2559506893157959, 0.9375],\n",
      "Time for 4750 iterations: 4233.8975784778595\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4760 iterations: 4242.760654211044\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.30136558413505554, 0.90625],\n",
      "Time for 4770 iterations: 4251.728362321854\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4780 iterations: 4260.5284075737\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 62.8, previous best: 62.4\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.17726346850395203, 0.96875],\n",
      "Time for 4790 iterations: 4269.991473674774\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4800 iterations: 4278.804029226303\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.324222594499588, 0.90625],\n",
      "Time for 4810 iterations: 4287.802417755127\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4820 iterations: 4296.640861034393\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3382348120212555, 0.90625],\n",
      "Time for 4830 iterations: 4305.455674171448\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 66.8, previous best: 62.8\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 4840 iterations: 4315.11568236351\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.28108587861061096, 0.9375],\n",
      "Time for 4850 iterations: 4324.129113435745\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4860 iterations: 4333.080377817154\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2196744978427887, 0.96875],\n",
      "Time for 4870 iterations: 4341.944564580917\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4880 iterations: 4351.06994342804\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.33273231983184814, 0.90625],\n",
      "Time for 4890 iterations: 4360.030648469925\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 68.4, previous best: 66.8\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 4900 iterations: 4369.564304828644\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.34188219904899597, 0.96875],\n",
      "Time for 4910 iterations: 4378.373294591904\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4920 iterations: 4387.197881221771\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4173709750175476, 0.875],\n",
      "Time for 4930 iterations: 4396.265977859497\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4940 iterations: 4405.12014746666\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24029424786567688, 0.96875],\n",
      "Time for 4950 iterations: 4414.228308439255\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4960 iterations: 4423.222870111465\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3681648075580597, 0.90625],\n",
      "Time for 4970 iterations: 4432.450644731522\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 4980 iterations: 4441.2859082221985\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24629032611846924, 0.9375],\n",
      "Time for 4990 iterations: 4450.479989528656\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5000 iterations: 4459.638356208801\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.27251121401786804, 0.9375],\n",
      "Time for 5010 iterations: 4468.55256986618\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5020 iterations: 4477.46741938591\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19892677664756775, 1.0],\n",
      "Time for 5030 iterations: 4486.502480983734\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5040 iterations: 4495.3789966106415\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21707427501678467, 0.96875],\n",
      "Time for 5050 iterations: 4504.207628965378\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5060 iterations: 4513.13888001442\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26887965202331543, 0.96875],\n",
      "Time for 5070 iterations: 4522.120943546295\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5080 iterations: 4531.296450138092\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26618844270706177, 0.96875],\n",
      "Time for 5090 iterations: 4539.938127756119\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5100 iterations: 4548.707005977631\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2970934510231018, 0.9375],\n",
      "Time for 5110 iterations: 4557.355403423309\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5120 iterations: 4566.108725309372\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.25879180431365967, 0.96875],\n",
      "Time for 5130 iterations: 4574.636531591415\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5140 iterations: 4583.25580239296\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3813084363937378, 0.90625],\n",
      "Time for 5150 iterations: 4592.064515829086\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5160 iterations: 4600.850126028061\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3718019127845764, 0.90625],\n",
      "Time for 5170 iterations: 4609.6562530994415\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5180 iterations: 4618.3232963085175\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3397020995616913, 0.90625],\n",
      "Time for 5190 iterations: 4627.0342762470245\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5200 iterations: 4635.680420398712\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.31058067083358765, 0.9375],\n",
      "Time for 5210 iterations: 4644.5284423828125\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5220 iterations: 4653.179337501526\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.30514389276504517, 0.90625],\n",
      "Time for 5230 iterations: 4661.97137928009\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5240 iterations: 4670.633712530136\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2652539610862732, 0.90625],\n",
      "Time for 5250 iterations: 4679.378612756729\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5260 iterations: 4688.239987373352\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.36671972274780273, 0.9375],\n",
      "Time for 5270 iterations: 4696.9379143714905\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5280 iterations: 4705.750664234161\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.30948179960250854, 0.90625],\n",
      "Time for 5290 iterations: 4714.43839931488\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5300 iterations: 4723.789971113205\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21522225439548492, 0.96875],\n",
      "Time for 5310 iterations: 4732.910901784897\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5320 iterations: 4742.089378833771\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23870030045509338, 0.9375],\n",
      "Time for 5330 iterations: 4751.0242602825165\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5340 iterations: 4760.003039360046\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21176406741142273, 0.96875],\n",
      "Time for 5350 iterations: 4768.827058553696\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5360 iterations: 4777.637249708176\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20729725062847137, 0.96875],\n",
      "Time for 5370 iterations: 4786.536548137665\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5380 iterations: 4795.384951353073\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.32907819747924805, 0.875],\n",
      "Time for 5390 iterations: 4804.340983629227\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 51.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5400 iterations: 4813.216526031494\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.37718820571899414, 0.875],\n",
      "Time for 5410 iterations: 4822.175551891327\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5420 iterations: 4831.134133815765\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2058904469013214, 0.96875],\n",
      "Time for 5430 iterations: 4840.308757066727\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5440 iterations: 4849.151289701462\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 52.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24973833560943604, 0.9375],\n",
      "Time for 5450 iterations: 4858.074378013611\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5460 iterations: 4866.940675497055\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.27757197618484497, 0.90625],\n",
      "Time for 5470 iterations: 4875.902055978775\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5480 iterations: 4884.775413274765\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3887450695037842, 0.875],\n",
      "Time for 5490 iterations: 4893.588736534119\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5500 iterations: 4902.494831800461\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4779401123523712, 0.875],\n",
      "Time for 5510 iterations: 4911.481969356537\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5520 iterations: 4920.450852632523\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2365397810935974, 0.96875],\n",
      "Time for 5530 iterations: 4929.22608757019\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5540 iterations: 4938.08925819397\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2564797103404999, 0.9375],\n",
      "Time for 5550 iterations: 4946.929920196533\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5560 iterations: 4955.817587137222\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.44254642724990845, 0.84375],\n",
      "Time for 5570 iterations: 4964.66729927063\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5580 iterations: 4973.408361673355\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 70.0, previous best: 68.4\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.29129743576049805, 0.90625],\n",
      "Time for 5590 iterations: 4982.941310882568\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5600 iterations: 4991.73473739624\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2675403952598572, 0.90625],\n",
      "Time for 5610 iterations: 5000.63837480545\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5620 iterations: 5009.456297636032\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3050670325756073, 0.9375],\n",
      "Time for 5630 iterations: 5018.4564640522\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5640 iterations: 5027.274461269379\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.27963268756866455, 0.875],\n",
      "Time for 5650 iterations: 5036.191313505173\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5660 iterations: 5045.108238220215\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20344436168670654, 0.96875],\n",
      "Time for 5670 iterations: 5054.069437742233\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5680 iterations: 5062.917217493057\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.40193650126457214, 0.875],\n",
      "Time for 5690 iterations: 5071.669972896576\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5700 iterations: 5080.545122861862\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18446549773216248, 0.96875],\n",
      "Time for 5710 iterations: 5089.345808267593\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5720 iterations: 5098.315906047821\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20615962147712708, 0.96875],\n",
      "Time for 5730 iterations: 5107.197958707809\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5740 iterations: 5116.068631410599\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2624034881591797, 0.90625],\n",
      "Time for 5750 iterations: 5124.892666101456\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5760 iterations: 5133.787977218628\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.38045865297317505, 0.84375],\n",
      "Time for 5770 iterations: 5142.656737327576\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5780 iterations: 5151.41667175293\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18126127123832703, 0.9375],\n",
      "Time for 5790 iterations: 5160.462830543518\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5800 iterations: 5169.208604097366\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26011818647384644, 0.9375],\n",
      "Time for 5810 iterations: 5178.058876514435\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5820 iterations: 5186.872389554977\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.40459030866622925, 0.90625],\n",
      "Time for 5830 iterations: 5195.795313835144\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5840 iterations: 5204.782794237137\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.40997350215911865, 0.84375],\n",
      "Time for 5850 iterations: 5213.681783437729\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5860 iterations: 5222.4745898246765\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.193268820643425, 1.0],\n",
      "Time for 5870 iterations: 5231.424067735672\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5880 iterations: 5240.169381380081\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4385506510734558, 0.875],\n",
      "Time for 5890 iterations: 5248.949305772781\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5900 iterations: 5257.866765975952\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20350021123886108, 0.96875],\n",
      "Time for 5910 iterations: 5266.606661081314\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5920 iterations: 5275.472548723221\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2714487910270691, 0.9375],\n",
      "Time for 5930 iterations: 5284.239345550537\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5940 iterations: 5293.110406398773\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3091374933719635, 0.90625],\n",
      "Time for 5950 iterations: 5302.084547281265\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5960 iterations: 5310.964267253876\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17110878229141235, 1.0],\n",
      "Time for 5970 iterations: 5319.769738197327\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 5980 iterations: 5328.60111117363\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5065197944641113, 0.84375],\n",
      "Time for 5990 iterations: 5337.429246664047\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6000 iterations: 5346.445238113403\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15638107061386108, 1.0],\n",
      "Time for 6010 iterations: 5355.344178438187\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6020 iterations: 5364.112172365189\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.25951603055000305, 0.9375],\n",
      "Time for 6030 iterations: 5372.979964971542\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6040 iterations: 5381.810787439346\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.25259605050086975, 0.96875],\n",
      "Time for 6050 iterations: 5390.729319572449\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6060 iterations: 5399.490873336792\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17900007963180542, 1.0],\n",
      "Time for 6070 iterations: 5408.369331598282\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6080 iterations: 5417.211898326874\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2683003842830658, 0.9375],\n",
      "Time for 6090 iterations: 5426.137177705765\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6100 iterations: 5434.945599794388\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2643275260925293, 0.96875],\n",
      "Time for 6110 iterations: 5443.9383952617645\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6120 iterations: 5453.079557657242\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4029061794281006, 0.90625],\n",
      "Time for 6130 iterations: 5462.040692329407\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6140 iterations: 5471.163035392761\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.28987622261047363, 0.96875],\n",
      "Time for 6150 iterations: 5479.996833562851\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6160 iterations: 5488.75358247757\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.44313469529151917, 0.9375],\n",
      "Time for 6170 iterations: 5497.269582033157\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6180 iterations: 5505.861792087555\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24013376235961914, 0.96875],\n",
      "Time for 6190 iterations: 5514.385730981827\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6200 iterations: 5522.955128669739\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.240651935338974, 0.96875],\n",
      "Time for 6210 iterations: 5531.475692033768\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6220 iterations: 5539.904128551483\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2631842792034149, 0.90625],\n",
      "Time for 6230 iterations: 5548.553087949753\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6240 iterations: 5557.056565761566\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2162054181098938, 0.96875],\n",
      "Time for 6250 iterations: 5565.659771203995\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6260 iterations: 5574.152415752411\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2425786703824997, 0.96875],\n",
      "Time for 6270 iterations: 5582.825229167938\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6280 iterations: 5591.3093094825745\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.33173730969429016, 0.875],\n",
      "Time for 6290 iterations: 5599.894952058792\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6300 iterations: 5608.37611746788\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3940224051475525, 0.875],\n",
      "Time for 6310 iterations: 5617.002841234207\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6320 iterations: 5625.570831775665\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5106765031814575, 0.84375],\n",
      "Time for 6330 iterations: 5634.008556365967\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6340 iterations: 5642.598490476608\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24574005603790283, 0.96875],\n",
      "Time for 6350 iterations: 5651.061044931412\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6360 iterations: 5659.642410039902\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26621872186660767, 1.0],\n",
      "Time for 6370 iterations: 5668.1632969379425\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6380 iterations: 5676.871178388596\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1823434829711914, 0.96875],\n",
      "Time for 6390 iterations: 5685.37638425827\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6400 iterations: 5693.9340443611145\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21107754111289978, 0.96875],\n",
      "Time for 6410 iterations: 5702.467926979065\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 53.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6420 iterations: 5710.959851026535\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19900450110435486, 1.0],\n",
      "Time for 6430 iterations: 5719.6019060611725\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6440 iterations: 5728.091079711914\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.46385249495506287, 0.84375],\n",
      "Time for 6450 iterations: 5736.667052030563\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6460 iterations: 5745.164348602295\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19610083103179932, 0.96875],\n",
      "Time for 6470 iterations: 5753.753130912781\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6480 iterations: 5762.238684177399\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.369869589805603, 0.875],\n",
      "Time for 6490 iterations: 5770.961340665817\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6500 iterations: 5779.468183755875\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26160183548927307, 0.9375],\n",
      "Time for 6510 iterations: 5788.038382768631\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6520 iterations: 5796.51625919342\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2606283724308014, 0.9375],\n",
      "Time for 6530 iterations: 5804.993408679962\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6540 iterations: 5813.685567855835\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3792814314365387, 0.9375],\n",
      "Time for 6550 iterations: 5822.161292791367\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6560 iterations: 5830.728888511658\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20978151261806488, 0.96875],\n",
      "Time for 6570 iterations: 5839.257593631744\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6580 iterations: 5847.811891794205\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.25948774814605713, 0.9375],\n",
      "Time for 6590 iterations: 5856.319437742233\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6600 iterations: 5864.966535568237\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.32962167263031006, 0.90625],\n",
      "Time for 6610 iterations: 5873.472983121872\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6620 iterations: 5882.0269594192505\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2859395742416382, 0.90625],\n",
      "Time for 6630 iterations: 5890.532689809799\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6640 iterations: 5898.985489845276\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.32997816801071167, 0.90625],\n",
      "Time for 6650 iterations: 5907.560654401779\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6660 iterations: 5916.152989864349\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.35621756315231323, 0.90625],\n",
      "Time for 6670 iterations: 5924.808178424835\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6680 iterations: 5933.3190994262695\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.186843141913414, 0.96875],\n",
      "Time for 6690 iterations: 5941.941219091415\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6700 iterations: 5950.442630767822\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.246412992477417, 0.9375],\n",
      "Time for 6710 iterations: 5959.099892377853\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6720 iterations: 5967.6056072711945\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.25724196434020996, 0.9375],\n",
      "Time for 6730 iterations: 5976.127104520798\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6740 iterations: 5984.62979054451\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21553394198417664, 0.9375],\n",
      "Time for 6750 iterations: 5993.083060264587\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6760 iterations: 6001.658660650253\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22696372866630554, 1.0],\n",
      "Time for 6770 iterations: 6010.253780126572\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6780 iterations: 6018.857837677002\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2536986470222473, 0.9375],\n",
      "Time for 6790 iterations: 6027.355326652527\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6800 iterations: 6035.934007406235\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1925075352191925, 1.0],\n",
      "Time for 6810 iterations: 6044.4610850811005\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6820 iterations: 6053.085788726807\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21819519996643066, 0.96875],\n",
      "Time for 6830 iterations: 6061.609151124954\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6840 iterations: 6070.152170658112\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2184656262397766, 0.96875],\n",
      "Time for 6850 iterations: 6078.676635026932\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6860 iterations: 6087.131266593933\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2648197114467621, 0.96875],\n",
      "Time for 6870 iterations: 6095.704018354416\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6880 iterations: 6104.3186683654785\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26677775382995605, 0.90625],\n",
      "Time for 6890 iterations: 6112.910321474075\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6900 iterations: 6121.403512954712\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23785285651683807, 0.9375],\n",
      "Time for 6910 iterations: 6129.97642326355\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6920 iterations: 6138.483553886414\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2937895953655243, 0.9375],\n",
      "Time for 6930 iterations: 6147.1529841423035\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6940 iterations: 6155.651478052139\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20499905943870544, 0.9375],\n",
      "Time for 6950 iterations: 6164.2505078315735\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6960 iterations: 6172.706298351288\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2198936492204666, 0.96875],\n",
      "Time for 6970 iterations: 6181.1869328022\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 6980 iterations: 6189.758398532867\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21560193598270416, 0.96875],\n",
      "Time for 6990 iterations: 6198.392590761185\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7000 iterations: 6206.997491836548\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2635253071784973, 0.9375],\n",
      "Time for 7010 iterations: 6215.507433176041\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7020 iterations: 6224.088578224182\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19888438284397125, 0.9375],\n",
      "Time for 7030 iterations: 6232.57510972023\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7040 iterations: 6241.209715604782\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19100171327590942, 0.96875],\n",
      "Time for 7050 iterations: 6249.733378410339\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7060 iterations: 6258.176713466644\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19476234912872314, 1.0],\n",
      "Time for 7070 iterations: 6266.75249004364\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7080 iterations: 6275.222727537155\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24718023836612701, 0.9375],\n",
      "Time for 7090 iterations: 6283.821182489395\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7100 iterations: 6292.443512201309\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16577240824699402, 1.0],\n",
      "Time for 7110 iterations: 6301.046083688736\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7120 iterations: 6309.5402801036835\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3438796401023865, 0.90625],\n",
      "Time for 7130 iterations: 6318.131764411926\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7140 iterations: 6326.61917090416\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.31649771332740784, 0.875],\n",
      "Time for 7150 iterations: 6335.264872074127\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7160 iterations: 6343.802018165588\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.291890025138855, 0.9375],\n",
      "Time for 7170 iterations: 6352.309851169586\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7180 iterations: 6360.969253540039\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23468941450119019, 0.9375],\n",
      "Time for 7190 iterations: 6369.45822763443\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7200 iterations: 6378.024869441986\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1801621913909912, 1.0],\n",
      "Time for 7210 iterations: 6386.620425224304\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7220 iterations: 6395.205369710922\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23270989954471588, 0.9375],\n",
      "Time for 7230 iterations: 6403.690793991089\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7240 iterations: 6412.294317960739\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2943100929260254, 0.96875],\n",
      "Time for 7250 iterations: 6420.80575466156\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7260 iterations: 6429.434685945511\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14817148447036743, 1.0],\n",
      "Time for 7270 iterations: 6437.996860265732\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7280 iterations: 6446.524130821228\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.45047539472579956, 0.875],\n",
      "Time for 7290 iterations: 6455.096282482147\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7300 iterations: 6463.613653421402\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16802652180194855, 1.0],\n",
      "Time for 7310 iterations: 6472.224264621735\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7320 iterations: 6480.721461057663\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2560057044029236, 0.9375],\n",
      "Time for 7330 iterations: 6489.321742534637\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7340 iterations: 6497.828140497208\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 54.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.31227365136146545, 0.90625],\n",
      "Time for 7350 iterations: 6506.426895618439\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7360 iterations: 6514.925178766251\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18479502201080322, 1.0],\n",
      "Time for 7370 iterations: 6523.644793510437\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7380 iterations: 6532.152596950531\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.43508780002593994, 0.84375],\n",
      "Time for 7390 iterations: 6540.630685567856\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7400 iterations: 6549.178725719452\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2275552749633789, 0.9375],\n",
      "Time for 7410 iterations: 6557.659499406815\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7420 iterations: 6566.325740814209\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20505613088607788, 0.9375],\n",
      "Time for 7430 iterations: 6574.804876565933\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7440 iterations: 6583.373109340668\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2490866631269455, 0.96875],\n",
      "Time for 7450 iterations: 6591.860329866409\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7460 iterations: 6600.452125072479\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.506842851638794, 0.84375],\n",
      "Time for 7470 iterations: 6608.9688103199005\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7480 iterations: 6617.661903619766\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15291789174079895, 1.0],\n",
      "Time for 7490 iterations: 6626.173690795898\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7500 iterations: 6634.637437343597\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3424079716205597, 0.96875],\n",
      "Time for 7510 iterations: 6643.231570243835\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7520 iterations: 6651.741040945053\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2833269238471985, 0.96875],\n",
      "Time for 7530 iterations: 6660.4021372795105\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7540 iterations: 6668.864182710648\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26562586426734924, 0.9375],\n",
      "Time for 7550 iterations: 6677.45677781105\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7560 iterations: 6685.973093032837\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.37540578842163086, 0.9375],\n",
      "Time for 7570 iterations: 6694.565832614899\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7580 iterations: 6703.0777814388275\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21986067295074463, 0.96875],\n",
      "Time for 7590 iterations: 6711.795832633972\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 73.2, previous best: 70.0\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 7600 iterations: 6720.864259719849\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23851686716079712, 0.9375],\n",
      "Time for 7610 iterations: 6729.330930709839\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7620 iterations: 6737.922325372696\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20390574634075165, 0.96875],\n",
      "Time for 7630 iterations: 6746.468475580215\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7640 iterations: 6755.107824802399\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 56.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2747139632701874, 0.96875],\n",
      "Time for 7650 iterations: 6763.660062551498\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7660 iterations: 6772.240873813629\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22857797145843506, 0.9375],\n",
      "Time for 7670 iterations: 6780.756211042404\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7680 iterations: 6789.303388118744\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2217777669429779, 0.9375],\n",
      "Time for 7690 iterations: 6797.810437202454\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7700 iterations: 6806.392131567001\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1982772946357727, 0.96875],\n",
      "Time for 7710 iterations: 6814.9665904045105\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7720 iterations: 6823.414422035217\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.28644800186157227, 0.90625],\n",
      "Time for 7730 iterations: 6831.982819080353\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7740 iterations: 6840.448577642441\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19143813848495483, 1.0],\n",
      "Time for 7750 iterations: 6849.088675260544\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7760 iterations: 6857.600597143173\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1990894079208374, 0.96875],\n",
      "Time for 7770 iterations: 6866.175966501236\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7780 iterations: 6874.65931558609\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2666066288948059, 0.9375],\n",
      "Time for 7790 iterations: 6883.23668050766\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7800 iterations: 6891.7618317604065\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1571444720029831, 0.96875],\n",
      "Time for 7810 iterations: 6900.327390670776\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7820 iterations: 6908.966500282288\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2575151324272156, 0.96875],\n",
      "Time for 7830 iterations: 6917.41312623024\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7840 iterations: 6926.003986597061\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2138882279396057, 1.0],\n",
      "Time for 7850 iterations: 6934.495596885681\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7860 iterations: 6943.1547203063965\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2045576572418213, 1.0],\n",
      "Time for 7870 iterations: 6951.629540681839\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7880 iterations: 6960.237837076187\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.32746079564094543, 0.90625],\n",
      "Time for 7890 iterations: 6968.745175123215\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7900 iterations: 6977.294375181198\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.256355881690979, 0.96875],\n",
      "Time for 7910 iterations: 6985.818762779236\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7920 iterations: 6994.416663646698\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2481406331062317, 0.96875],\n",
      "Time for 7930 iterations: 7002.981524944305\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7940 iterations: 7011.475610733032\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2958795726299286, 0.9375],\n",
      "Time for 7950 iterations: 7020.072661399841\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7960 iterations: 7028.583325386047\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13374409079551697, 1.0],\n",
      "Time for 7970 iterations: 7037.263109445572\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 7980 iterations: 7045.80247092247\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.38798150420188904, 0.9375],\n",
      "Time for 7990 iterations: 7054.430336475372\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8000 iterations: 7062.922003984451\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15486395359039307, 1.0],\n",
      "Time for 8010 iterations: 7071.5325763225555\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8020 iterations: 7080.093903541565\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.43516865372657776, 0.84375],\n",
      "Time for 8030 iterations: 7088.678026676178\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8040 iterations: 7097.255557537079\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1858183741569519, 1.0],\n",
      "Time for 8050 iterations: 7105.779372215271\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8060 iterations: 7114.355527877808\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3587135076522827, 0.875],\n",
      "Time for 8070 iterations: 7122.880596876144\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8080 iterations: 7131.551115274429\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.36917951703071594, 0.84375],\n",
      "Time for 8090 iterations: 7140.006690502167\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8100 iterations: 7148.5261907577515\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22781379520893097, 0.96875],\n",
      "Time for 8110 iterations: 7157.035655260086\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8120 iterations: 7165.665128707886\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.32960793375968933, 0.875],\n",
      "Time for 8130 iterations: 7174.216495037079\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8140 iterations: 7182.8153240680695\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15023258328437805, 1.0],\n",
      "Time for 8150 iterations: 7191.411037683487\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8160 iterations: 7199.886739492416\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2193748503923416, 0.96875],\n",
      "Time for 8170 iterations: 7208.482800960541\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8180 iterations: 7216.993506669998\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2340858280658722, 0.96875],\n",
      "Time for 8190 iterations: 7225.654021978378\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8200 iterations: 7234.158827781677\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1929190456867218, 0.96875],\n",
      "Time for 8210 iterations: 7242.755695819855\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8220 iterations: 7251.275493621826\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.34384727478027344, 0.90625],\n",
      "Time for 8230 iterations: 7259.886499404907\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8240 iterations: 7268.411744832993\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 74.8, previous best: 73.2\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.27109962701797485, 0.9375],\n",
      "Time for 8250 iterations: 7277.5307676792145\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8260 iterations: 7286.14186668396\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2085285186767578, 1.0],\n",
      "Time for 8270 iterations: 7294.67645740509\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8280 iterations: 7303.313055515289\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.27159014344215393, 0.9375],\n",
      "Time for 8290 iterations: 7311.816694736481\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8300 iterations: 7320.440207242966\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.232847660779953, 0.9375],\n",
      "Time for 8310 iterations: 7328.942864894867\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8320 iterations: 7337.515233755112\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3060036897659302, 0.90625],\n",
      "Time for 8330 iterations: 7346.066755771637\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8340 iterations: 7354.559087514877\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2362757921218872, 0.96875],\n",
      "Time for 8350 iterations: 7363.136614322662\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8360 iterations: 7371.700644016266\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18809616565704346, 1.0],\n",
      "Time for 8370 iterations: 7380.28120303154\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8380 iterations: 7388.782329797745\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23301199078559875, 0.9375],\n",
      "Time for 8390 iterations: 7397.399861574173\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8400 iterations: 7405.93071603775\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.31636911630630493, 0.9375],\n",
      "Time for 8410 iterations: 7414.611167430878\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8420 iterations: 7423.09626865387\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24088013172149658, 0.9375],\n",
      "Time for 8430 iterations: 7431.685299396515\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8440 iterations: 7440.217802762985\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1863231509923935, 0.96875],\n",
      "Time for 8450 iterations: 7448.666246652603\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8460 iterations: 7457.338433742523\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17804113030433655, 1.0],\n",
      "Time for 8470 iterations: 7465.825993299484\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8480 iterations: 7474.438263654709\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20076370239257812, 1.0],\n",
      "Time for 8490 iterations: 7482.929656744003\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8500 iterations: 7491.530769586563\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17489999532699585, 0.96875],\n",
      "Time for 8510 iterations: 7500.0501573085785\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8520 iterations: 7508.812767505646\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18836799263954163, 0.96875],\n",
      "Time for 8530 iterations: 7517.2929792404175\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8540 iterations: 7525.867564916611\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22188735008239746, 0.96875],\n",
      "Time for 8550 iterations: 7534.379042625427\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8560 iterations: 7542.84450507164\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26628994941711426, 0.9375],\n",
      "Time for 8570 iterations: 7551.490135908127\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8580 iterations: 7559.982125997543\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.36590367555618286, 0.875],\n",
      "Time for 8590 iterations: 7568.552779436111\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8600 iterations: 7577.085773229599\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4125547409057617, 0.875],\n",
      "Time for 8610 iterations: 7585.686809301376\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8620 iterations: 7594.186794519424\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2699422836303711, 0.96875],\n",
      "Time for 8630 iterations: 7602.918997049332\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8640 iterations: 7611.449611902237\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.34145963191986084, 0.90625],\n",
      "Time for 8650 iterations: 7620.032337665558\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8660 iterations: 7628.577799081802\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2621212303638458, 0.9375],\n",
      "Time for 8670 iterations: 7637.0721783638\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8680 iterations: 7645.686608314514\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19092132151126862, 1.0],\n",
      "Time for 8690 iterations: 7654.222542524338\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8700 iterations: 7662.808430194855\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16212579607963562, 1.0],\n",
      "Time for 8710 iterations: 7671.3222687244415\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8720 iterations: 7679.90647983551\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2863520085811615, 0.9375],\n",
      "Time for 8730 iterations: 7688.37441110611\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8740 iterations: 7697.072863340378\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.25689876079559326, 0.90625],\n",
      "Time for 8750 iterations: 7705.597356081009\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8760 iterations: 7714.1807379722595\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2179114818572998, 0.9375],\n",
      "Time for 8770 iterations: 7722.678688526154\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8780 iterations: 7731.142799139023\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.32257580757141113, 0.90625],\n",
      "Time for 8790 iterations: 7739.791133880615\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8800 iterations: 7748.263319730759\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19277620315551758, 1.0],\n",
      "Time for 8810 iterations: 7756.853044271469\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8820 iterations: 7765.340022325516\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 55.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17436140775680542, 1.0],\n",
      "Time for 8830 iterations: 7773.917157173157\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8840 iterations: 7782.437399625778\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3282935917377472, 0.90625],\n",
      "Time for 8850 iterations: 7791.162051916122\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8860 iterations: 7799.709619045258\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.6521155834197998, 0.875],\n",
      "Time for 8870 iterations: 7808.328097343445\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8880 iterations: 7816.802396535873\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.45680373907089233, 0.875],\n",
      "Time for 8890 iterations: 7825.263776302338\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8900 iterations: 7833.887729883194\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20923861861228943, 0.96875],\n",
      "Time for 8910 iterations: 7842.384818077087\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8920 iterations: 7850.985609292984\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21299484372138977, 0.96875],\n",
      "Time for 8930 iterations: 7859.518696784973\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8940 iterations: 7868.059705257416\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1881491243839264, 0.96875],\n",
      "Time for 8950 iterations: 7876.574543476105\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8960 iterations: 7885.283668518066\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.28045764565467834, 0.875],\n",
      "Time for 8970 iterations: 7893.808673620224\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 8980 iterations: 7902.265620231628\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16121146082878113, 1.0],\n",
      "Time for 8990 iterations: 7910.841716527939\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9000 iterations: 7919.316550254822\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3233073055744171, 0.9375],\n",
      "Time for 9010 iterations: 7927.906228542328\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9020 iterations: 7936.404468297958\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22637173533439636, 0.96875],\n",
      "Time for 9030 iterations: 7944.993635892868\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9040 iterations: 7953.459778547287\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 75.2, previous best: 74.8\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.19871141016483307, 0.96875],\n",
      "Time for 9050 iterations: 7962.628704786301\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9060 iterations: 7971.15801692009\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3337433338165283, 0.875],\n",
      "Time for 9070 iterations: 7979.877425670624\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9080 iterations: 7988.363343238831\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21113435924053192, 0.9375],\n",
      "Time for 9090 iterations: 7996.847636222839\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9100 iterations: 8005.514900684357\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21639016270637512, 0.96875],\n",
      "Time for 9110 iterations: 8014.001169681549\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9120 iterations: 8022.638304948807\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2504608631134033, 0.96875],\n",
      "Time for 9130 iterations: 8031.133438110352\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9140 iterations: 8039.737017631531\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2648281455039978, 0.9375],\n",
      "Time for 9150 iterations: 8048.227640628815\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9160 iterations: 8056.81346988678\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2803620994091034, 0.96875],\n",
      "Time for 9170 iterations: 8065.318807601929\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9180 iterations: 8074.032894611359\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.39779162406921387, 0.875],\n",
      "Time for 9190 iterations: 8082.567124843597\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9200 iterations: 8091.051383972168\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 75.2, previous best: 75.2\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.3210095763206482, 0.875],\n",
      "Time for 9210 iterations: 8100.209596633911\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9220 iterations: 8108.715683937073\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4007876217365265, 0.875],\n",
      "Time for 9230 iterations: 8117.341856241226\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9240 iterations: 8125.850426197052\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.210889995098114, 0.96875],\n",
      "Time for 9250 iterations: 8134.4587507247925\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9260 iterations: 8142.950203180313\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19057989120483398, 0.96875],\n",
      "Time for 9270 iterations: 8151.523729324341\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9280 iterations: 8160.02102804184\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23338386416435242, 0.9375],\n",
      "Time for 9290 iterations: 8168.675384283066\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9300 iterations: 8177.167111158371\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.39284998178482056, 0.875],\n",
      "Time for 9310 iterations: 8185.6467752456665\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9320 iterations: 8194.216459989548\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.28858229517936707, 0.9375],\n",
      "Time for 9330 iterations: 8202.692087173462\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9340 iterations: 8211.346866369247\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5809398293495178, 0.875],\n",
      "Time for 9350 iterations: 8219.821053743362\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9360 iterations: 8228.394313097\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2387804388999939, 0.96875],\n",
      "Time for 9370 iterations: 8236.8924202919\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9380 iterations: 8245.523671865463\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17418059706687927, 1.0],\n",
      "Time for 9390 iterations: 8254.04666686058\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9400 iterations: 8262.7111120224\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2789403796195984, 0.9375],\n",
      "Time for 9410 iterations: 8271.270962715149\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9420 iterations: 8279.749029159546\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23785682022571564, 0.96875],\n",
      "Time for 9430 iterations: 8288.354683160782\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9440 iterations: 8296.877500295639\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4317684471607208, 0.90625],\n",
      "Time for 9450 iterations: 8305.436430215836\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9460 iterations: 8314.041112661362\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2126190960407257, 0.96875],\n",
      "Time for 9470 iterations: 8322.63781118393\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9480 iterations: 8331.11658358574\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.39415356516838074, 0.875],\n",
      "Time for 9490 iterations: 8339.695096492767\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9500 iterations: 8348.246688127518\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24856334924697876, 0.96875],\n",
      "Time for 9510 iterations: 8356.925157546997\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9520 iterations: 8365.456868886948\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21895436942577362, 0.9375],\n",
      "Time for 9530 iterations: 8373.906046152115\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9540 iterations: 8382.51425409317\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16978490352630615, 0.96875],\n",
      "Time for 9550 iterations: 8391.02591252327\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9560 iterations: 8399.711908817291\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20563046634197235, 1.0],\n",
      "Time for 9570 iterations: 8408.221419334412\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9580 iterations: 8416.847806215286\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.29899853467941284, 0.875],\n",
      "Time for 9590 iterations: 8425.336076021194\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9600 iterations: 8433.904294252396\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26533132791519165, 0.9375],\n",
      "Time for 9610 iterations: 8442.403941869736\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9620 iterations: 8450.991568803787\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23061014711856842, 0.96875],\n",
      "Time for 9630 iterations: 8459.61175584793\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9640 iterations: 8468.063081026077\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2682758867740631, 0.90625],\n",
      "Time for 9650 iterations: 8476.69662642479\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9660 iterations: 8485.151971578598\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3141617476940155, 0.9375],\n",
      "Time for 9670 iterations: 8493.81977391243\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9680 iterations: 8502.341805458069\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2260078638792038, 0.96875],\n",
      "Time for 9690 iterations: 8510.963964939117\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9700 iterations: 8519.494146347046\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22166606783866882, 0.96875],\n",
      "Time for 9710 iterations: 8528.1067507267\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9720 iterations: 8536.639786481857\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18946465849876404, 0.96875],\n",
      "Time for 9730 iterations: 8545.265667438507\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9740 iterations: 8553.91230750084\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.36751505732536316, 0.875],\n",
      "Time for 9750 iterations: 8562.391963481903\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9760 iterations: 8570.949617147446\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4227388799190521, 0.875],\n",
      "Time for 9770 iterations: 8579.44921541214\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9780 iterations: 8588.050889730453\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22301575541496277, 1.0],\n",
      "Time for 9790 iterations: 8596.58416891098\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9800 iterations: 8605.164731264114\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3146367073059082, 0.90625],\n",
      "Time for 9810 iterations: 8613.636406183243\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9820 iterations: 8622.247646808624\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.35825905203819275, 0.875],\n",
      "Time for 9830 iterations: 8630.760833024979\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9840 iterations: 8639.347578525543\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18700546026229858, 0.96875],\n",
      "Time for 9850 iterations: 8647.944306612015\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9860 iterations: 8656.461895227432\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14932814240455627, 1.0],\n",
      "Time for 9870 iterations: 8665.020047426224\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9880 iterations: 8673.543825864792\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20806510746479034, 0.96875],\n",
      "Time for 9890 iterations: 8682.223595380783\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9900 iterations: 8690.759490728378\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2953373193740845, 0.90625],\n",
      "Time for 9910 iterations: 8699.346160173416\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9920 iterations: 8707.870384454727\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.37135961651802063, 0.90625],\n",
      "Time for 9930 iterations: 8716.472733259201\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9940 iterations: 8724.98309636116\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22330865263938904, 0.96875],\n",
      "Time for 9950 iterations: 8733.586700201035\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9960 iterations: 8742.18660235405\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.25696495175361633, 0.9375],\n",
      "Time for 9970 iterations: 8750.7141726017\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 9980 iterations: 8759.301134109497\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.35360074043273926, 0.875],\n",
      "Time for 9990 iterations: 8767.80616903305\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10000 iterations: 8776.45419216156\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19556424021720886, 0.96875],\n",
      "Time for 10010 iterations: 8784.942690372467\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10020 iterations: 8793.499985218048\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.31268811225891113, 0.90625],\n",
      "Time for 10030 iterations: 8802.033448457718\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10040 iterations: 8810.599843263626\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.29173728823661804, 0.96875],\n",
      "Time for 10050 iterations: 8819.082997083664\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10060 iterations: 8827.684678792953\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.49577346444129944, 0.90625],\n",
      "Time for 10070 iterations: 8836.27563548088\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10080 iterations: 8844.778083324432\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3342226445674896, 0.84375],\n",
      "Time for 10090 iterations: 8853.400226831436\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10100 iterations: 8861.916755437851\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.35069411993026733, 0.875],\n",
      "Time for 10110 iterations: 8870.577741384506\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10120 iterations: 8879.087588310242\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16896134614944458, 0.96875],\n",
      "Time for 10130 iterations: 8887.64659190178\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 75.2, previous best: 75.2\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 10140 iterations: 8896.807998418808\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17123566567897797, 0.96875],\n",
      "Time for 10150 iterations: 8905.43986082077\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10160 iterations: 8913.897153377533\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18874050676822662, 1.0],\n",
      "Time for 10170 iterations: 8922.450178146362\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10180 iterations: 8931.042344808578\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17433109879493713, 0.96875],\n",
      "Time for 10190 iterations: 8939.55661869049\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10200 iterations: 8948.169226884842\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3166438937187195, 0.90625],\n",
      "Time for 10210 iterations: 8956.684604883194\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10220 iterations: 8965.328558206558\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.585066556930542, 0.875],\n",
      "Time for 10230 iterations: 8973.852568626404\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10240 iterations: 8982.442318439484\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.28516775369644165, 0.90625],\n",
      "Time for 10250 iterations: 8990.979521274567\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 58.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10260 iterations: 8999.485160827637\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.34290361404418945, 0.9375],\n",
      "Time for 10270 iterations: 9008.05773305893\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10280 iterations: 9016.6488904953\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2284843623638153, 0.96875],\n",
      "Time for 10290 iterations: 9025.211124897003\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10300 iterations: 9033.696937084198\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.38959643244743347, 0.90625],\n",
      "Time for 10310 iterations: 9042.347364902496\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10320 iterations: 9050.829287052155\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2938905954360962, 0.9375],\n",
      "Time for 10330 iterations: 9059.537892103195\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10340 iterations: 9068.07153391838\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.31531086564064026, 0.90625],\n",
      "Time for 10350 iterations: 9076.68406033516\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10360 iterations: 9085.198141813278\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2765764594078064, 0.9375],\n",
      "Time for 10370 iterations: 9093.643556118011\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10380 iterations: 9102.286113977432\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16711276769638062, 1.0],\n",
      "Time for 10390 iterations: 9110.886443376541\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10400 iterations: 9119.470194101334\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.30761438608169556, 0.96875],\n",
      "Time for 10410 iterations: 9127.924623966217\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10420 iterations: 9136.541171312332\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1468011438846588, 1.0],\n",
      "Time for 10430 iterations: 9145.024416208267\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10440 iterations: 9153.690676927567\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18777744472026825, 0.96875],\n",
      "Time for 10450 iterations: 9162.227293491364\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10460 iterations: 9170.776194095612\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14970415830612183, 1.0],\n",
      "Time for 10470 iterations: 9179.32542181015\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10480 iterations: 9187.820343732834\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2863096594810486, 0.90625],\n",
      "Time for 10490 iterations: 9196.458966255188\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10500 iterations: 9205.023562908173\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2738400995731354, 0.9375],\n",
      "Time for 10510 iterations: 9213.622676372528\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10520 iterations: 9222.121537685394\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 60.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.32985377311706543, 0.96875],\n",
      "Time for 10530 iterations: 9230.704794168472\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10540 iterations: 9239.217652320862\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22671180963516235, 0.96875],\n",
      "Time for 10550 iterations: 9247.838307857513\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10560 iterations: 9256.364033460617\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.28317368030548096, 0.96875],\n",
      "Time for 10570 iterations: 9264.92909169197\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10580 iterations: 9273.425748348236\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2624519467353821, 0.9375],\n",
      "Time for 10590 iterations: 9281.909576177597\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10600 iterations: 9290.50996851921\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.227690190076828, 0.96875],\n",
      "Time for 10610 iterations: 9299.10107755661\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10620 iterations: 9307.7053668499\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1753641664981842, 1.0],\n",
      "Time for 10630 iterations: 9316.215823888779\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10640 iterations: 9324.783602714539\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 76.4, previous best: 75.2\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.18848444521427155, 0.9375],\n",
      "Time for 10650 iterations: 9333.860548734665\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10660 iterations: 9342.48248910904\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2841874361038208, 0.9375],\n",
      "Time for 10670 iterations: 9350.973015069962\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10680 iterations: 9359.575209617615\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26731204986572266, 0.9375],\n",
      "Time for 10690 iterations: 9368.101182937622\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10700 iterations: 9376.575990915298\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2570483982563019, 0.9375],\n",
      "Time for 10710 iterations: 9385.102129459381\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10720 iterations: 9393.74951505661\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4752834439277649, 0.875],\n",
      "Time for 10730 iterations: 9402.388186454773\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10740 iterations: 9410.877900123596\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18001174926757812, 0.96875],\n",
      "Time for 10750 iterations: 9419.49271607399\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10760 iterations: 9427.940464019775\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2349858582019806, 0.96875],\n",
      "Time for 10770 iterations: 9436.608086109161\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10780 iterations: 9445.105301618576\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19687378406524658, 0.96875],\n",
      "Time for 10790 iterations: 9453.729001283646\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10800 iterations: 9462.226428747177\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2849373519420624, 0.96875],\n",
      "Time for 10810 iterations: 9470.688817501068\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10820 iterations: 9479.255147695541\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3486975431442261, 0.90625],\n",
      "Time for 10830 iterations: 9487.90094590187\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10840 iterations: 9496.46962094307\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2687680125236511, 0.90625],\n",
      "Time for 10850 iterations: 9504.989241838455\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10860 iterations: 9513.541623830795\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.29287466406822205, 0.90625],\n",
      "Time for 10870 iterations: 9522.047065019608\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10880 iterations: 9530.746754646301\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21417829394340515, 0.9375],\n",
      "Time for 10890 iterations: 9539.275678873062\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10900 iterations: 9547.745352983475\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20979659259319305, 1.0],\n",
      "Time for 10910 iterations: 9556.369418859482\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10920 iterations: 9564.806936979294\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.30131232738494873, 0.9375],\n",
      "Time for 10930 iterations: 9573.377247333527\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10940 iterations: 9581.97848200798\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20889204740524292, 0.96875],\n",
      "Time for 10950 iterations: 9590.602202415466\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10960 iterations: 9599.084492444992\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17019671201705933, 1.0],\n",
      "Time for 10970 iterations: 9607.665008068085\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 10980 iterations: 9616.189435005188\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26885414123535156, 0.9375],\n",
      "Time for 10990 iterations: 9624.877629995346\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11000 iterations: 9633.636122226715\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21218203008174896, 1.0],\n",
      "Time for 11010 iterations: 9642.09201335907\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11020 iterations: 9650.738557338715\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.30864328145980835, 0.90625],\n",
      "Time for 11030 iterations: 9659.20730638504\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11040 iterations: 9667.782203435898\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16522206366062164, 1.0],\n",
      "Time for 11050 iterations: 9676.42443227768\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11060 iterations: 9684.991626262665\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.25055593252182007, 0.96875],\n",
      "Time for 11070 iterations: 9693.49488902092\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11080 iterations: 9702.092577457428\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1512579321861267, 1.0],\n",
      "Time for 11090 iterations: 9710.594965934753\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11100 iterations: 9719.275446176529\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19048234820365906, 0.96875],\n",
      "Time for 11110 iterations: 9727.786525249481\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11120 iterations: 9736.254710197449\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3216043710708618, 0.9375],\n",
      "Time for 11130 iterations: 9744.844473838806\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11140 iterations: 9753.315693378448\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.208545982837677, 0.96875],\n",
      "Time for 11150 iterations: 9761.994271755219\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11160 iterations: 9770.621317148209\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3223062753677368, 0.9375],\n",
      "Time for 11170 iterations: 9779.255259752274\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11180 iterations: 9787.772298336029\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17093229293823242, 0.96875],\n",
      "Time for 11190 iterations: 9796.351109266281\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11200 iterations: 9804.841274499893\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4022669494152069, 0.875],\n",
      "Time for 11210 iterations: 9813.416029453278\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11220 iterations: 9822.071640014648\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.271462082862854, 0.96875],\n",
      "Time for 11230 iterations: 9830.542553186417\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11240 iterations: 9839.130755662918\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2573121190071106, 0.9375],\n",
      "Time for 11250 iterations: 9847.634717464447\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11260 iterations: 9856.210860013962\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22182536125183105, 0.9375],\n",
      "Time for 11270 iterations: 9864.762606620789\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11280 iterations: 9873.340792179108\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13503265380859375, 1.0],\n",
      "Time for 11290 iterations: 9881.864346027374\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11300 iterations: 9890.46171092987\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2805618941783905, 0.9375],\n",
      "Time for 11310 iterations: 9898.992091655731\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11320 iterations: 9907.566232442856\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.172750324010849, 1.0],\n",
      "Time for 11330 iterations: 9916.18806219101\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11340 iterations: 9924.645750045776\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16214442253112793, 1.0],\n",
      "Time for 11350 iterations: 9933.198607206345\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11360 iterations: 9941.713887691498\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20736351609230042, 0.96875],\n",
      "Time for 11370 iterations: 9950.290312051773\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11380 iterations: 9958.891697406769\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3618229031562805, 0.90625],\n",
      "Time for 11390 iterations: 9967.49777674675\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11400 iterations: 9976.021136760712\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17175300419330597, 0.96875],\n",
      "Time for 11410 iterations: 9984.62847828865\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11420 iterations: 9993.111214637756\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2203875482082367, 0.96875],\n",
      "Time for 11430 iterations: 10001.724672317505\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11440 iterations: 10010.352441549301\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1755387783050537, 0.96875],\n",
      "Time for 11450 iterations: 10018.83396744728\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11460 iterations: 10027.408599853516\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17272359132766724, 1.0],\n",
      "Time for 11470 iterations: 10035.908377170563\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11480 iterations: 10044.524726629257\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.25260069966316223, 0.9375],\n",
      "Time for 11490 iterations: 10053.08740568161\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11500 iterations: 10061.676368713379\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24964524805545807, 0.96875],\n",
      "Time for 11510 iterations: 10070.18192601204\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11520 iterations: 10078.791573762894\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20722931623458862, 0.96875],\n",
      "Time for 11530 iterations: 10087.283047437668\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11540 iterations: 10095.756036996841\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24504172801971436, 0.90625],\n",
      "Time for 11550 iterations: 10104.534241437912\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11560 iterations: 10113.000870227814\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22461146116256714, 0.96875],\n",
      "Time for 11570 iterations: 10121.619642019272\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11580 iterations: 10130.14506983757\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26104089617729187, 0.9375],\n",
      "Time for 11590 iterations: 10138.748500823975\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11600 iterations: 10147.34838628769\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21031543612480164, 1.0],\n",
      "Time for 11610 iterations: 10155.926706075668\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11620 iterations: 10164.449738502502\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2312767505645752, 0.96875],\n",
      "Time for 11630 iterations: 10173.028880119324\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11640 iterations: 10181.60576081276\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18944568932056427, 1.0],\n",
      "Time for 11650 iterations: 10190.045862197876\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11660 iterations: 10198.880227327347\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19986295700073242, 1.0],\n",
      "Time for 11670 iterations: 10207.343202829361\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11680 iterations: 10215.892838001251\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16484755277633667, 1.0],\n",
      "Time for 11690 iterations: 10224.386964082718\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11700 iterations: 10232.996425390244\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1535080075263977, 1.0],\n",
      "Time for 11710 iterations: 10241.570219993591\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 61.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11720 iterations: 10250.149161815643\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16628554463386536, 1.0],\n",
      "Time for 11730 iterations: 10258.690960884094\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11740 iterations: 10267.258791446686\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.36542195081710815, 0.84375],\n",
      "Time for 11750 iterations: 10275.777556180954\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11760 iterations: 10284.228689908981\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.191229909658432, 0.96875],\n",
      "Time for 11770 iterations: 10292.96012544632\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11780 iterations: 10301.470421552658\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18376041948795319, 0.96875],\n",
      "Time for 11790 iterations: 10310.051857471466\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11800 iterations: 10318.622145175934\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13691680133342743, 1.0],\n",
      "Time for 11810 iterations: 10327.209019422531\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11820 iterations: 10335.80337357521\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.35313940048217773, 0.9375],\n",
      "Time for 11830 iterations: 10344.441334962845\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11840 iterations: 10352.891243696213\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22493714094161987, 0.96875],\n",
      "Time for 11850 iterations: 10361.496794939041\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11860 iterations: 10370.03839802742\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3484240770339966, 0.875],\n",
      "Time for 11870 iterations: 10378.555138111115\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11880 iterations: 10387.227944612503\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22731709480285645, 0.96875],\n",
      "Time for 11890 iterations: 10395.696835279465\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11900 iterations: 10404.284356594086\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21837735176086426, 0.96875],\n",
      "Time for 11910 iterations: 10412.801245689392\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11920 iterations: 10421.412446022034\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18278518319129944, 1.0],\n",
      "Time for 11930 iterations: 10429.981498003006\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11940 iterations: 10438.563473701477\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.29796773195266724, 0.9375],\n",
      "Time for 11950 iterations: 10447.067500829697\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11960 iterations: 10455.65295124054\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18240469694137573, 0.96875],\n",
      "Time for 11970 iterations: 10464.171685934067\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 11980 iterations: 10472.65414237976\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17419327795505524, 1.0],\n",
      "Time for 11990 iterations: 10481.377888202667\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12000 iterations: 10489.888205766678\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22349867224693298, 0.9375],\n",
      "Time for 12010 iterations: 10498.47529911995\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12020 iterations: 10506.98937010765\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16917169094085693, 0.96875],\n",
      "Time for 12030 iterations: 10515.545456886292\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12040 iterations: 10523.993633031845\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.47006064653396606, 0.90625],\n",
      "Time for 12050 iterations: 10532.711115121841\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12060 iterations: 10541.21293091774\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21004298329353333, 0.96875],\n",
      "Time for 12070 iterations: 10549.82973909378\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12080 iterations: 10558.303431034088\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24655595421791077, 0.96875],\n",
      "Time for 12090 iterations: 10566.755521297455\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12100 iterations: 10575.444203138351\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.185933917760849, 0.9375],\n",
      "Time for 12110 iterations: 10583.958330869675\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12120 iterations: 10592.527931928635\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24629396200180054, 0.9375],\n",
      "Time for 12130 iterations: 10601.023880958557\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12140 iterations: 10609.59006524086\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.30082398653030396, 0.9375],\n",
      "Time for 12150 iterations: 10618.097427606583\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12160 iterations: 10626.817297697067\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19389912486076355, 0.96875],\n",
      "Time for 12170 iterations: 10635.33679318428\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12180 iterations: 10643.760241031647\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17072121798992157, 0.96875],\n",
      "Time for 12190 iterations: 10652.352946519852\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12200 iterations: 10660.801433801651\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2228030115365982, 0.9375],\n",
      "Time for 12210 iterations: 10669.45944738388\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 76.4, previous best: 76.4\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 12220 iterations: 10678.582299947739\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13753265142440796, 1.0],\n",
      "Time for 12230 iterations: 10687.190064430237\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12240 iterations: 10695.738099098206\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19773441553115845, 0.96875],\n",
      "Time for 12250 iterations: 10704.355338096619\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12260 iterations: 10712.869606494904\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3412240743637085, 0.9375],\n",
      "Time for 12270 iterations: 10721.565051078796\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12280 iterations: 10730.082558870316\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1850230097770691, 0.96875],\n",
      "Time for 12290 iterations: 10738.538526296616\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12300 iterations: 10747.205186128616\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26310068368911743, 0.9375],\n",
      "Time for 12310 iterations: 10755.696808815002\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12320 iterations: 10764.351432561874\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23110896348953247, 0.90625],\n",
      "Time for 12330 iterations: 10772.868609666824\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12340 iterations: 10781.4644613266\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1450100839138031, 1.0],\n",
      "Time for 12350 iterations: 10789.964782953262\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12360 iterations: 10798.585550546646\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24295154213905334, 0.90625],\n",
      "Time for 12370 iterations: 10807.086663484573\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12380 iterations: 10815.705058574677\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.30822181701660156, 0.96875],\n",
      "Time for 12390 iterations: 10824.258469104767\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12400 iterations: 10832.75090098381\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15886804461479187, 1.0],\n",
      "Time for 12410 iterations: 10841.323198318481\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12420 iterations: 10849.800480127335\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2767592966556549, 0.9375],\n",
      "Time for 12430 iterations: 10858.463136911392\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12440 iterations: 10866.961868286133\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.172506183385849, 1.0],\n",
      "Time for 12450 iterations: 10875.626464128494\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12460 iterations: 10884.15503501892\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2216995358467102, 0.96875],\n",
      "Time for 12470 iterations: 10892.735026359558\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12480 iterations: 10901.274163484573\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3445645570755005, 0.90625],\n",
      "Time for 12490 iterations: 10909.997638702393\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12500 iterations: 10918.533371210098\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.187982976436615, 0.96875],\n",
      "Time for 12510 iterations: 10926.973998785019\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12520 iterations: 10935.567805290222\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24153074622154236, 0.96875],\n",
      "Time for 12530 iterations: 10944.022670269012\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12540 iterations: 10952.736407756805\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2114938348531723, 0.9375],\n",
      "Time for 12550 iterations: 10961.251075029373\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12560 iterations: 10969.849645614624\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.33139848709106445, 0.96875],\n",
      "Time for 12570 iterations: 10978.368287563324\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12580 iterations: 10986.95740699768\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.34293463826179504, 0.9375],\n",
      "Time for 12590 iterations: 10995.495802640915\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12600 iterations: 11004.226234197617\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17217858135700226, 1.0],\n",
      "Time for 12610 iterations: 11012.747184515\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12620 iterations: 11021.203788757324\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.28670090436935425, 0.9375],\n",
      "Time for 12630 iterations: 11029.82045173645\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12640 iterations: 11038.335371255875\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1314462125301361, 1.0],\n",
      "Time for 12650 iterations: 11047.083808422089\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12660 iterations: 11055.593631029129\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16220778226852417, 0.96875],\n",
      "Time for 12670 iterations: 11064.177566766739\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12680 iterations: 11072.680814266205\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5592652559280396, 0.875],\n",
      "Time for 12690 iterations: 11081.28491640091\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12700 iterations: 11089.803928852081\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2681400775909424, 0.90625],\n",
      "Time for 12710 iterations: 11098.542264699936\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12720 iterations: 11107.025219917297\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18911992013454437, 1.0],\n",
      "Time for 12730 iterations: 11115.486583471298\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12740 iterations: 11124.076428174973\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19460301101207733, 1.0],\n",
      "Time for 12750 iterations: 11132.590576410294\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12760 iterations: 11141.294329166412\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23502612113952637, 0.96875],\n",
      "Time for 12770 iterations: 11149.792084932327\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12780 iterations: 11158.406341075897\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2642326056957245, 0.9375],\n",
      "Time for 12790 iterations: 11166.911785125732\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12800 iterations: 11175.505051374435\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19610708951950073, 0.96875],\n",
      "Time for 12810 iterations: 11184.03754067421\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12820 iterations: 11192.64579963684\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1974806785583496, 0.96875],\n",
      "Time for 12830 iterations: 11201.216142177582\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12840 iterations: 11209.65072107315\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21235576272010803, 0.9375],\n",
      "Time for 12850 iterations: 11218.283658742905\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12860 iterations: 11226.80123090744\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.28343772888183594, 0.9375],\n",
      "Time for 12870 iterations: 11235.465646505356\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12880 iterations: 11243.960546970367\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2632664442062378, 0.96875],\n",
      "Time for 12890 iterations: 11252.567346572876\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12900 iterations: 11261.078599214554\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21808740496635437, 0.90625],\n",
      "Time for 12910 iterations: 11269.627912044525\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12920 iterations: 11278.145613193512\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18085236847400665, 1.0],\n",
      "Time for 12930 iterations: 11286.69630241394\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12940 iterations: 11295.36948633194\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21032336354255676, 0.9375],\n",
      "Time for 12950 iterations: 11303.826792955399\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12960 iterations: 11312.408348560333\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1748645007610321, 0.96875],\n",
      "Time for 12970 iterations: 11320.929827451706\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 12980 iterations: 11329.586155414581\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18711930513381958, 0.96875],\n",
      "Time for 12990 iterations: 11338.11898636818\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13000 iterations: 11346.715536117554\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2527253329753876, 0.9375],\n",
      "Time for 13010 iterations: 11355.248281240463\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13020 iterations: 11363.814699172974\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22931860387325287, 0.9375],\n",
      "Time for 13030 iterations: 11372.331597089767\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13040 iterations: 11380.898982524872\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2684047520160675, 0.96875],\n",
      "Time for 13050 iterations: 11389.46352314949\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13060 iterations: 11397.922654628754\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.294784277677536, 0.90625],\n",
      "Time for 13070 iterations: 11406.507124185562\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13080 iterations: 11415.056205749512\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2397548258304596, 0.9375],\n",
      "Time for 13090 iterations: 11423.756243467331\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13100 iterations: 11432.242905378342\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18712344765663147, 0.9375],\n",
      "Time for 13110 iterations: 11440.812106370926\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13120 iterations: 11449.349769830704\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1625066101551056, 1.0],\n",
      "Time for 13130 iterations: 11457.928810358047\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13140 iterations: 11466.468775749207\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2543429136276245, 0.9375],\n",
      "Time for 13150 iterations: 11475.061623096466\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13160 iterations: 11483.659742116928\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2645207643508911, 0.96875],\n",
      "Time for 13170 iterations: 11492.159797906876\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13180 iterations: 11500.791348218918\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16714687645435333, 1.0],\n",
      "Time for 13190 iterations: 11509.265365362167\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13200 iterations: 11517.950675010681\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4749280512332916, 0.9375],\n",
      "Time for 13210 iterations: 11526.486604213715\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13220 iterations: 11535.087596654892\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17263278365135193, 1.0],\n",
      "Time for 13230 iterations: 11543.606592655182\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13240 iterations: 11552.169831752777\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1552710384130478, 1.0],\n",
      "Time for 13250 iterations: 11560.745141029358\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13260 iterations: 11569.32205414772\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2849506139755249, 0.9375],\n",
      "Time for 13270 iterations: 11577.965090990067\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13280 iterations: 11586.46731376648\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.29878079891204834, 0.90625],\n",
      "Time for 13290 iterations: 11595.073187112808\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13300 iterations: 11603.564499378204\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19384939968585968, 0.96875],\n",
      "Time for 13310 iterations: 11612.246428012848\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 63.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13320 iterations: 11620.776860237122\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2575691342353821, 0.9375],\n",
      "Time for 13330 iterations: 11629.373977184296\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13340 iterations: 11637.923468589783\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.4279087781906128, 0.9375],\n",
      "Time for 13350 iterations: 11646.569694042206\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13360 iterations: 11655.027972459793\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26774245500564575, 0.9375],\n",
      "Time for 13370 iterations: 11663.602980613708\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13380 iterations: 11672.175733804703\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 59.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2299322485923767, 0.96875],\n",
      "Time for 13390 iterations: 11680.678535699844\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13400 iterations: 11689.258003950119\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2123776078224182, 0.96875],\n",
      "Time for 13410 iterations: 11697.792434453964\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13420 iterations: 11706.452830553055\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5238030552864075, 0.875],\n",
      "Time for 13430 iterations: 11714.982192516327\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13440 iterations: 11723.532467842102\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16444845497608185, 1.0],\n",
      "Time for 13450 iterations: 11732.072012424469\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13460 iterations: 11740.562757968903\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3247886300086975, 0.96875],\n",
      "Time for 13470 iterations: 11749.16433095932\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13480 iterations: 11757.65491271019\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.28613078594207764, 0.9375],\n",
      "Time for 13490 iterations: 11766.233375787735\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13500 iterations: 11774.681683778763\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23540076613426208, 0.96875],\n",
      "Time for 13510 iterations: 11783.27874803543\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13520 iterations: 11791.794137001038\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3325149416923523, 0.90625],\n",
      "Time for 13530 iterations: 11800.503466129303\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13540 iterations: 11808.964457988739\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5104453563690186, 0.875],\n",
      "Time for 13550 iterations: 11817.549599170685\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13560 iterations: 11826.08208489418\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.25555408000946045, 0.96875],\n",
      "Time for 13570 iterations: 11834.556159734726\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13580 iterations: 11843.253072500229\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18378853797912598, 0.96875],\n",
      "Time for 13590 iterations: 11851.75197649002\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13600 iterations: 11860.306595802307\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.29238057136535645, 0.90625],\n",
      "Time for 13610 iterations: 11868.810739994049\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13620 iterations: 11877.411816120148\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21753905713558197, 0.96875],\n",
      "Time for 13630 iterations: 11885.936113834381\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13640 iterations: 11894.681901693344\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15629522502422333, 1.0],\n",
      "Time for 13650 iterations: 11903.158902645111\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13660 iterations: 11911.747411489487\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1901237666606903, 0.96875],\n",
      "Time for 13670 iterations: 11920.304374456406\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13680 iterations: 11928.737124919891\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13975366950035095, 1.0],\n",
      "Time for 13690 iterations: 11937.387148618698\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13700 iterations: 11945.826548576355\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20610597729682922, 0.96875],\n",
      "Time for 13710 iterations: 11954.454446554184\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13720 iterations: 11962.928400278091\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20077785849571228, 0.96875],\n",
      "Time for 13730 iterations: 11971.501086711884\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13740 iterations: 11980.04854631424\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 77.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 77.2, previous best: 76.4\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.19958090782165527, 1.0],\n",
      "Time for 13750 iterations: 11989.274450302124\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13760 iterations: 11997.8031103611\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18434664607048035, 1.0],\n",
      "Time for 13770 iterations: 12006.40148115158\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13780 iterations: 12014.921063899994\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22996321320533752, 0.9375],\n",
      "Time for 13790 iterations: 12023.410167694092\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13800 iterations: 12032.009628534317\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1818561553955078, 1.0],\n",
      "Time for 13810 iterations: 12040.64491391182\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13820 iterations: 12049.228644132614\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17094498872756958, 0.96875],\n",
      "Time for 13830 iterations: 12057.732966423035\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13840 iterations: 12066.331379652023\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22205613553524017, 0.96875],\n",
      "Time for 13850 iterations: 12074.806821107864\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13860 iterations: 12083.446985721588\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2456936538219452, 0.90625],\n",
      "Time for 13870 iterations: 12092.03102517128\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13880 iterations: 12100.589524269104\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24587741494178772, 0.9375],\n",
      "Time for 13890 iterations: 12109.051666498184\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13900 iterations: 12117.587220430374\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1772434115409851, 1.0],\n",
      "Time for 13910 iterations: 12126.152505397797\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13920 iterations: 12134.789358615875\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2110576331615448, 0.96875],\n",
      "Time for 13930 iterations: 12143.378165006638\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13940 iterations: 12151.887596607208\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.29956021904945374, 0.90625],\n",
      "Time for 13950 iterations: 12160.465482711792\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13960 iterations: 12168.963728189468\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17128577828407288, 1.0],\n",
      "Time for 13970 iterations: 12177.624986410141\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 13980 iterations: 12186.156799077988\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19932478666305542, 0.9375],\n",
      "Time for 13990 iterations: 12194.744840860367\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14000 iterations: 12203.237118721008\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2938525378704071, 0.96875],\n",
      "Time for 14010 iterations: 12211.688145399094\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14020 iterations: 12220.281158208847\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21174943447113037, 0.96875],\n",
      "Time for 14030 iterations: 12228.926186800003\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14040 iterations: 12237.547175884247\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1924053281545639, 0.96875],\n",
      "Time for 14050 iterations: 12246.036890506744\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14060 iterations: 12254.683283090591\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.42940711975097656, 0.875],\n",
      "Time for 14070 iterations: 12263.174059867859\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14080 iterations: 12271.814728021622\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20517325401306152, 0.96875],\n",
      "Time for 14090 iterations: 12280.31470322609\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14100 iterations: 12288.760552406311\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16448083519935608, 1.0],\n",
      "Time for 14110 iterations: 12297.33389878273\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14120 iterations: 12305.80179643631\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3516019284725189, 0.90625],\n",
      "Time for 14130 iterations: 12314.399918079376\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14140 iterations: 12322.984877347946\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1851445436477661, 0.96875],\n",
      "Time for 14150 iterations: 12331.591291189194\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14160 iterations: 12340.072960853577\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.32163184881210327, 0.84375],\n",
      "Time for 14170 iterations: 12348.628809213638\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14180 iterations: 12357.138046503067\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19596293568611145, 0.9375],\n",
      "Time for 14190 iterations: 12365.811683416367\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14200 iterations: 12374.350963830948\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3057321310043335, 0.84375],\n",
      "Time for 14210 iterations: 12382.765432357788\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14220 iterations: 12391.420528173447\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2585933804512024, 0.90625],\n",
      "Time for 14230 iterations: 12399.89580130577\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14240 iterations: 12408.482949495316\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2548638582229614, 0.96875],\n",
      "Time for 14250 iterations: 12417.120032072067\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14260 iterations: 12425.704333305359\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14841343462467194, 1.0],\n",
      "Time for 14270 iterations: 12434.216317176819\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14280 iterations: 12442.815415143967\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20556487143039703, 0.96875],\n",
      "Time for 14290 iterations: 12451.37794470787\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14300 iterations: 12460.032593727112\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2605249285697937, 0.96875],\n",
      "Time for 14310 iterations: 12468.53165102005\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14320 iterations: 12477.016829490662\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.30316537618637085, 0.90625],\n",
      "Time for 14330 iterations: 12485.597830057144\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14340 iterations: 12494.078706979752\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.36084577441215515, 0.96875],\n",
      "Time for 14350 iterations: 12502.69898891449\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14360 iterations: 12511.290738344193\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17823942005634308, 1.0],\n",
      "Time for 14370 iterations: 12519.868211746216\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14380 iterations: 12528.397496938705\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3146572709083557, 0.9375],\n",
      "Time for 14390 iterations: 12536.987671852112\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14400 iterations: 12545.543111085892\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20540830492973328, 0.96875],\n",
      "Time for 14410 iterations: 12554.200239181519\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14420 iterations: 12562.712904453278\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22657379508018494, 0.96875],\n",
      "Time for 14430 iterations: 12571.184147834778\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14440 iterations: 12579.766999959946\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20223870873451233, 0.96875],\n",
      "Time for 14450 iterations: 12588.243877410889\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14460 iterations: 12596.81758260727\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24028624594211578, 0.9375],\n",
      "Time for 14470 iterations: 12605.452127218246\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14480 iterations: 12614.031475067139\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17168432474136353, 1.0],\n",
      "Time for 14490 iterations: 12622.556062221527\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14500 iterations: 12631.150657176971\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14394685626029968, 1.0],\n",
      "Time for 14510 iterations: 12639.69547367096\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14520 iterations: 12648.342962265015\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1931862235069275, 0.96875],\n",
      "Time for 14530 iterations: 12656.924713373184\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14540 iterations: 12665.398274898529\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24305479228496552, 0.9375],\n",
      "Time for 14550 iterations: 12674.02619767189\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14560 iterations: 12682.525886297226\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17109453678131104, 0.96875],\n",
      "Time for 14570 iterations: 12691.15712761879\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14580 iterations: 12699.75756239891\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18769872188568115, 0.96875],\n",
      "Time for 14590 iterations: 12708.348521232605\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14600 iterations: 12716.846522808075\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19188088178634644, 0.96875],\n",
      "Time for 14610 iterations: 12725.432757139206\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14620 iterations: 12733.979672670364\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1730543076992035, 0.96875],\n",
      "Time for 14630 iterations: 12742.597393512726\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14640 iterations: 12751.226172447205\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14097964763641357, 1.0],\n",
      "Time for 14650 iterations: 12759.674906730652\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14660 iterations: 12768.270773649216\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21086722612380981, 0.96875],\n",
      "Time for 14670 iterations: 12776.808252334595\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14680 iterations: 12785.405192136765\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.39541834592819214, 0.875],\n",
      "Time for 14690 iterations: 12793.936342477798\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14700 iterations: 12802.530647039413\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.31757745146751404, 0.90625],\n",
      "Time for 14710 iterations: 12811.068473100662\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14720 iterations: 12819.68957567215\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16398382186889648, 1.0],\n",
      "Time for 14730 iterations: 12828.224390029907\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14740 iterations: 12836.696814537048\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14937160909175873, 1.0],\n",
      "Time for 14750 iterations: 12845.407569169998\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14760 iterations: 12853.88333106041\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1845054030418396, 1.0],\n",
      "Time for 14770 iterations: 12862.465867519379\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14780 iterations: 12870.9613904953\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2719835937023163, 0.90625],\n",
      "Time for 14790 iterations: 12879.572848796844\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14800 iterations: 12888.128366708755\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.12054673582315445, 1.0],\n",
      "Time for 14810 iterations: 12896.72842001915\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14820 iterations: 12905.2473590374\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19366317987442017, 0.96875],\n",
      "Time for 14830 iterations: 12913.814941644669\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 77.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 77.6, previous best: 77.2\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "Time for 14840 iterations: 12922.897608041763\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15832139551639557, 1.0],\n",
      "Time for 14850 iterations: 12931.413677930832\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14860 iterations: 12940.172068119049\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15503104031085968, 1.0],\n",
      "Time for 14870 iterations: 12948.66670370102\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14880 iterations: 12957.275185585022\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.25216177105903625, 0.96875],\n",
      "Time for 14890 iterations: 12965.754750967026\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14900 iterations: 12974.33645248413\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17417585849761963, 1.0],\n",
      "Time for 14910 iterations: 12982.922738552094\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14920 iterations: 12991.504491090775\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1986771523952484, 0.96875],\n",
      "Time for 14930 iterations: 13000.034606695175\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14940 iterations: 13008.625854253769\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 80.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Current best: 80.0, previous best: 77.6\n",
      "Saving weights to: /home/sina/Desktop/omniglot/model_omniglot_weights.h5 \n",
      "\n",
      "training loss: [0.16297344863414764, 1.0],\n",
      "Time for 14950 iterations: 13017.723225355148\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14960 iterations: 13026.163495540619\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2531549334526062, 0.96875],\n",
      "Time for 14970 iterations: 13034.904261112213\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 14980 iterations: 13043.410633802414\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 62.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3440813422203064, 0.9375],\n",
      "Time for 14990 iterations: 13051.983674049377\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15000 iterations: 13060.500085830688\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14638079702854156, 1.0],\n",
      "Time for 15010 iterations: 13069.089643001556\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15020 iterations: 13077.690423965454\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26980164647102356, 0.90625],\n",
      "Time for 15030 iterations: 13086.284786939621\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15040 iterations: 13094.82961320877\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14625295996665955, 1.0],\n",
      "Time for 15050 iterations: 13103.437452316284\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15060 iterations: 13112.021133422852\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2162904590368271, 1.0],\n",
      "Time for 15070 iterations: 13120.485300302505\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15080 iterations: 13129.225327968597\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1480690836906433, 1.0],\n",
      "Time for 15090 iterations: 13137.721112728119\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15100 iterations: 13146.291256666183\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17177245020866394, 1.0],\n",
      "Time for 15110 iterations: 13154.796586036682\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15120 iterations: 13163.428533792496\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.27682065963745117, 0.96875],\n",
      "Time for 15130 iterations: 13172.036107301712\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15140 iterations: 13180.621201515198\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.34246474504470825, 0.90625],\n",
      "Time for 15150 iterations: 13189.12963104248\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15160 iterations: 13197.679490089417\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1890057623386383, 0.96875],\n",
      "Time for 15170 iterations: 13206.205204725266\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15180 iterations: 13214.678967237473\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.37049779295921326, 0.9375],\n",
      "Time for 15190 iterations: 13223.420417785645\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15200 iterations: 13231.909167528152\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2826554477214813, 0.90625],\n",
      "Time for 15210 iterations: 13240.573954820633\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15220 iterations: 13249.01455283165\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18754354119300842, 0.96875],\n",
      "Time for 15230 iterations: 13257.629140377045\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15240 iterations: 13266.167934656143\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3218506872653961, 0.875],\n",
      "Time for 15250 iterations: 13274.762159109116\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15260 iterations: 13283.337287664413\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13483348488807678, 1.0],\n",
      "Time for 15270 iterations: 13291.972938537598\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15280 iterations: 13300.421750068665\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18013940751552582, 0.96875],\n",
      "Time for 15290 iterations: 13308.88439321518\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15300 iterations: 13317.577711343765\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24167490005493164, 0.96875],\n",
      "Time for 15310 iterations: 13326.018753051758\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15320 iterations: 13334.595546007156\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1443096101284027, 1.0],\n",
      "Time for 15330 iterations: 13343.0444252491\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15340 iterations: 13351.717567920685\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14730960130691528, 1.0],\n",
      "Time for 15350 iterations: 13360.298237800598\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15360 iterations: 13368.879373550415\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16100972890853882, 0.96875],\n",
      "Time for 15370 iterations: 13377.408500432968\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15380 iterations: 13385.868782520294\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15724840760231018, 1.0],\n",
      "Time for 15390 iterations: 13394.49467587471\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15400 iterations: 13402.92331957817\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1541265845298767, 1.0],\n",
      "Time for 15410 iterations: 13411.631436347961\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15420 iterations: 13420.072094678879\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.25497764348983765, 0.96875],\n",
      "Time for 15430 iterations: 13428.646003484726\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15440 iterations: 13437.192195892334\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2729320526123047, 0.9375],\n",
      "Time for 15450 iterations: 13445.754398107529\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15460 iterations: 13454.384207725525\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1566963791847229, 1.0],\n",
      "Time for 15470 iterations: 13462.999907493591\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15480 iterations: 13471.533611774445\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15806803107261658, 1.0],\n",
      "Time for 15490 iterations: 13480.022715568542\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15500 iterations: 13488.68028831482\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.25609639286994934, 0.96875],\n",
      "Time for 15510 iterations: 13497.150032997131\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15520 iterations: 13505.828613042831\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1376599222421646, 1.0],\n",
      "Time for 15530 iterations: 13514.3471596241\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15540 iterations: 13522.94950222969\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13660334050655365, 1.0],\n",
      "Time for 15550 iterations: 13531.447563171387\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15560 iterations: 13539.993108034134\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15065675973892212, 1.0],\n",
      "Time for 15570 iterations: 13548.602650403976\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15580 iterations: 13557.18459033966\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.31206315755844116, 0.9375],\n",
      "Time for 15590 iterations: 13565.703892230988\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15600 iterations: 13574.152987957\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22203344106674194, 0.96875],\n",
      "Time for 15610 iterations: 13582.745597839355\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15620 iterations: 13591.219648122787\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13522636890411377, 1.0],\n",
      "Time for 15630 iterations: 13599.948120117188\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15640 iterations: 13608.447253704071\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.39158564805984497, 0.84375],\n",
      "Time for 15650 iterations: 13617.038397073746\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15660 iterations: 13625.516186237335\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21910658478736877, 0.9375],\n",
      "Time for 15670 iterations: 13634.070431470871\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15680 iterations: 13642.588339328766\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13993966579437256, 1.0],\n",
      "Time for 15690 iterations: 13651.276816368103\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15700 iterations: 13659.795861244202\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21114836633205414, 1.0],\n",
      "Time for 15710 iterations: 13668.266008377075\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15720 iterations: 13676.87877869606\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18017014861106873, 1.0],\n",
      "Time for 15730 iterations: 13685.404981136322\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15740 iterations: 13694.148263454437\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20416593551635742, 0.96875],\n",
      "Time for 15750 iterations: 13702.665786027908\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15760 iterations: 13711.27353477478\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15954846143722534, 1.0],\n",
      "Time for 15770 iterations: 13719.750534057617\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15780 iterations: 13728.360654830933\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19193783402442932, 0.96875],\n",
      "Time for 15790 iterations: 13736.961183547974\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15800 iterations: 13745.562150001526\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20445317029953003, 1.0],\n",
      "Time for 15810 iterations: 13754.304373025894\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15820 iterations: 13762.874152183533\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13007432222366333, 1.0],\n",
      "Time for 15830 iterations: 13771.568749427795\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15840 iterations: 13780.05157661438\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20823583006858826, 1.0],\n",
      "Time for 15850 iterations: 13788.773320913315\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15860 iterations: 13797.283676862717\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 77.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16571171581745148, 1.0],\n",
      "Time for 15870 iterations: 13805.829595327377\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15880 iterations: 13814.351788043976\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16385625302791595, 1.0],\n",
      "Time for 15890 iterations: 13822.956403493881\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15900 iterations: 13831.593973875046\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18635818362236023, 0.96875],\n",
      "Time for 15910 iterations: 13840.22544503212\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15920 iterations: 13848.704685688019\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17196959257125854, 1.0],\n",
      "Time for 15930 iterations: 13857.109295368195\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15940 iterations: 13865.695968151093\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1615857183933258, 1.0],\n",
      "Time for 15950 iterations: 13874.2114007473\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15960 iterations: 13882.950052976608\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2220383584499359, 0.9375],\n",
      "Time for 15970 iterations: 13891.43386554718\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 15980 iterations: 13900.007608652115\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24398094415664673, 0.9375],\n",
      "Time for 15990 iterations: 13908.510056972504\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16000 iterations: 13917.115840435028\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 78.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.141200989484787, 1.0],\n",
      "Time for 16010 iterations: 13925.70445895195\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16020 iterations: 13934.15727853775\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16005760431289673, 1.0],\n",
      "Time for 16030 iterations: 13942.796662092209\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16040 iterations: 13951.319838047028\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20046338438987732, 0.96875],\n",
      "Time for 16050 iterations: 13959.889686584473\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 78.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16060 iterations: 13968.382805347443\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24232301115989685, 0.96875],\n",
      "Time for 16070 iterations: 13977.119986772537\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16080 iterations: 13985.623064279556\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23524150252342224, 0.96875],\n",
      "Time for 16090 iterations: 13994.190019130707\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16100 iterations: 14002.703400850296\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20228932797908783, 0.9375],\n",
      "Time for 16110 iterations: 14011.292769432068\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16120 iterations: 14019.892496347427\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1986411064863205, 0.9375],\n",
      "Time for 16130 iterations: 14028.361204624176\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16140 iterations: 14037.016467809677\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18326199054718018, 1.0],\n",
      "Time for 16150 iterations: 14045.441100358963\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16160 iterations: 14054.002387285233\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2635805606842041, 0.9375],\n",
      "Time for 16170 iterations: 14062.505695819855\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16180 iterations: 14071.264307498932\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20632579922676086, 0.96875],\n",
      "Time for 16190 iterations: 14079.747802495956\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16200 iterations: 14088.320312023163\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2060873806476593, 0.9375],\n",
      "Time for 16210 iterations: 14096.839314699173\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16220 iterations: 14105.389771223068\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2224273979663849, 0.96875],\n",
      "Time for 16230 iterations: 14114.033666610718\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16240 iterations: 14122.462728261948\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 79.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2586715519428253, 0.90625],\n",
      "Time for 16250 iterations: 14131.05513215065\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16260 iterations: 14139.559148788452\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13247749209403992, 1.0],\n",
      "Time for 16270 iterations: 14148.168357372284\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16280 iterations: 14156.674119472504\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21390092372894287, 0.96875],\n",
      "Time for 16290 iterations: 14165.409452915192\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16300 iterations: 14173.929268836975\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1961158663034439, 0.9375],\n",
      "Time for 16310 iterations: 14182.53005194664\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16320 iterations: 14191.043165445328\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2850475311279297, 0.9375],\n",
      "Time for 16330 iterations: 14199.61906170845\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16340 iterations: 14208.218811750412\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2212190479040146, 0.90625],\n",
      "Time for 16350 iterations: 14216.72142124176\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16360 iterations: 14225.29406619072\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22747181355953217, 0.9375],\n",
      "Time for 16370 iterations: 14233.77498626709\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16380 iterations: 14242.31589269638\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24115470051765442, 0.9375],\n",
      "Time for 16390 iterations: 14250.881379127502\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16400 iterations: 14259.560730457306\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1473407745361328, 1.0],\n",
      "Time for 16410 iterations: 14268.061609745026\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16420 iterations: 14276.674255371094\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15363824367523193, 0.96875],\n",
      "Time for 16430 iterations: 14285.223798274994\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16440 iterations: 14293.795513153076\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.25834277272224426, 0.9375],\n",
      "Time for 16450 iterations: 14302.384789466858\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16460 iterations: 14310.887647151947\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17415833473205566, 0.96875],\n",
      "Time for 16470 iterations: 14319.462602376938\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16480 iterations: 14327.947833299637\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2531384229660034, 0.90625],\n",
      "Time for 16490 iterations: 14336.553770542145\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16500 iterations: 14345.051450490952\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22097083926200867, 0.96875],\n",
      "Time for 16510 iterations: 14353.739995241165\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16520 iterations: 14362.2717294693\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2302374243736267, 0.96875],\n",
      "Time for 16530 iterations: 14370.861267328262\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16540 iterations: 14379.418496370316\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22043773531913757, 0.9375],\n",
      "Time for 16550 iterations: 14388.053643226624\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 78.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16560 iterations: 14396.62238574028\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.27595916390419006, 0.90625],\n",
      "Time for 16570 iterations: 14405.04156088829\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16580 iterations: 14413.649641513824\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21819934248924255, 0.96875],\n",
      "Time for 16590 iterations: 14422.185153007507\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16600 iterations: 14430.796852350235\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17786410450935364, 1.0],\n",
      "Time for 16610 iterations: 14439.27864575386\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16620 iterations: 14448.026059865952\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14054284989833832, 1.0],\n",
      "Time for 16630 iterations: 14456.577355146408\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16640 iterations: 14465.139873981476\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.27447670698165894, 0.9375],\n",
      "Time for 16650 iterations: 14473.676391839981\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16660 iterations: 14482.133244276047\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 77.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1925501823425293, 0.96875],\n",
      "Time for 16670 iterations: 14490.786716222763\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16680 iterations: 14499.220838308334\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18998314440250397, 0.9375],\n",
      "Time for 16690 iterations: 14507.792797088623\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16700 iterations: 14516.281298875809\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21889136731624603, 0.90625],\n",
      "Time for 16710 iterations: 14524.90504360199\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16720 iterations: 14533.406791687012\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18737278878688812, 0.96875],\n",
      "Time for 16730 iterations: 14542.100712299347\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16740 iterations: 14550.678922891617\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.12097667157649994, 1.0],\n",
      "Time for 16750 iterations: 14559.284205198288\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16760 iterations: 14567.837205648422\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3279382586479187, 0.90625],\n",
      "Time for 16770 iterations: 14576.317606925964\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16780 iterations: 14585.015643119812\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.5235229730606079, 0.84375],\n",
      "Time for 16790 iterations: 14593.448536396027\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16800 iterations: 14601.98354268074\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2716861069202423, 0.9375],\n",
      "Time for 16810 iterations: 14610.470689535141\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16820 iterations: 14619.05586194992\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21842265129089355, 0.96875],\n",
      "Time for 16830 iterations: 14627.570663690567\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16840 iterations: 14636.252850294113\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1595650613307953, 1.0],\n",
      "Time for 16850 iterations: 14644.791127204895\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16860 iterations: 14653.35177731514\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15671691298484802, 1.0],\n",
      "Time for 16870 iterations: 14661.875858306885\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16880 iterations: 14670.338579893112\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2762967646121979, 0.96875],\n",
      "Time for 16890 iterations: 14679.02461194992\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16900 iterations: 14687.499417304993\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.25912708044052124, 0.9375],\n",
      "Time for 16910 iterations: 14696.112129211426\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16920 iterations: 14704.611784219742\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17212697863578796, 1.0],\n",
      "Time for 16930 iterations: 14713.217661380768\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16940 iterations: 14721.700189352036\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24187061190605164, 0.9375],\n",
      "Time for 16950 iterations: 14730.412955522537\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16960 iterations: 14738.911546230316\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18697264790534973, 0.96875],\n",
      "Time for 16970 iterations: 14747.480249166489\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 16980 iterations: 14756.026347637177\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.25547006726264954, 0.9375],\n",
      "Time for 16990 iterations: 14764.500707626343\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17000 iterations: 14773.13041472435\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17615830898284912, 0.96875],\n",
      "Time for 17010 iterations: 14781.598263025284\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 77.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17020 iterations: 14790.223573923111\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2585051953792572, 0.96875],\n",
      "Time for 17030 iterations: 14798.72544002533\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17040 iterations: 14807.334099769592\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15364587306976318, 1.0],\n",
      "Time for 17050 iterations: 14815.836976766586\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17060 iterations: 14824.583515405655\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21243496239185333, 0.96875],\n",
      "Time for 17070 iterations: 14833.101459026337\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17080 iterations: 14841.678586006165\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22605130076408386, 0.9375],\n",
      "Time for 17090 iterations: 14850.26826620102\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17100 iterations: 14858.7165081501\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2830292284488678, 0.9375],\n",
      "Time for 17110 iterations: 14867.330565690994\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17120 iterations: 14875.83778476715\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1353224515914917, 1.0],\n",
      "Time for 17130 iterations: 14884.43477320671\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 77.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17140 iterations: 14892.943816184998\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15395498275756836, 1.0],\n",
      "Time for 17150 iterations: 14901.530147790909\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17160 iterations: 14910.022229909897\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23318743705749512, 0.9375],\n",
      "Time for 17170 iterations: 14918.775283813477\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17180 iterations: 14927.311922550201\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17223303020000458, 1.0],\n",
      "Time for 17190 iterations: 14935.920248270035\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17200 iterations: 14944.429301261902\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2474443018436432, 0.9375],\n",
      "Time for 17210 iterations: 14952.821852684021\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 78.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17220 iterations: 14961.475882291794\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2732468843460083, 0.9375],\n",
      "Time for 17230 iterations: 14970.01013302803\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17240 iterations: 14978.582910776138\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.12882830202579498, 1.0],\n",
      "Time for 17250 iterations: 14987.083300113678\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17260 iterations: 14995.690279960632\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18944118916988373, 0.96875],\n",
      "Time for 17270 iterations: 15004.167430400848\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17280 iterations: 15012.882885456085\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16632363200187683, 0.96875],\n",
      "Time for 17290 iterations: 15021.398311376572\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17300 iterations: 15029.886657238007\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1832694411277771, 1.0],\n",
      "Time for 17310 iterations: 15038.507330656052\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17320 iterations: 15046.931121587753\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2743322551250458, 0.9375],\n",
      "Time for 17330 iterations: 15055.623636007309\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17340 iterations: 15064.08913564682\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21759867668151855, 0.96875],\n",
      "Time for 17350 iterations: 15072.717019796371\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17360 iterations: 15081.20773100853\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2366926670074463, 0.96875],\n",
      "Time for 17370 iterations: 15089.804298400879\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17380 iterations: 15098.342118024826\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14162611961364746, 1.0],\n",
      "Time for 17390 iterations: 15107.052016973495\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17400 iterations: 15115.603254318237\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14352905750274658, 1.0],\n",
      "Time for 17410 iterations: 15124.066416025162\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17420 iterations: 15132.712362289429\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2376483678817749, 0.875],\n",
      "Time for 17430 iterations: 15141.12475991249\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17440 iterations: 15149.829767465591\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2340564727783203, 0.9375],\n",
      "Time for 17450 iterations: 15158.33124923706\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17460 iterations: 15166.943345785141\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.28874850273132324, 0.9375],\n",
      "Time for 17470 iterations: 15175.483222961426\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17480 iterations: 15184.079674959183\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2285807728767395, 0.96875],\n",
      "Time for 17490 iterations: 15192.60987496376\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17500 iterations: 15201.30031466484\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18930834531784058, 0.9375],\n",
      "Time for 17510 iterations: 15209.861187696457\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17520 iterations: 15218.283615589142\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18230131268501282, 0.9375],\n",
      "Time for 17530 iterations: 15226.883509635925\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17540 iterations: 15235.38445854187\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24237391352653503, 0.96875],\n",
      "Time for 17550 iterations: 15244.050596952438\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17560 iterations: 15252.554685115814\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23888729512691498, 0.9375],\n",
      "Time for 17570 iterations: 15261.106096029282\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17580 iterations: 15269.659149646759\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.26779574155807495, 0.90625],\n",
      "Time for 17590 iterations: 15278.229349374771\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17600 iterations: 15286.736145734787\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18556663393974304, 0.96875],\n",
      "Time for 17610 iterations: 15295.421040534973\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17620 iterations: 15303.956199169159\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.29402270913124084, 0.875],\n",
      "Time for 17630 iterations: 15312.449016571045\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17640 iterations: 15320.984892606735\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2158195525407791, 0.96875],\n",
      "Time for 17650 iterations: 15329.412523031235\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 79.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17660 iterations: 15338.027354001999\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2635768949985504, 0.96875],\n",
      "Time for 17670 iterations: 15346.532753229141\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17680 iterations: 15355.119511127472\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.266451895236969, 0.875],\n",
      "Time for 17690 iterations: 15363.651866197586\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17700 iterations: 15372.243377923965\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17670109868049622, 1.0],\n",
      "Time for 17710 iterations: 15380.749657869339\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 77.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17720 iterations: 15389.405915021896\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2354423999786377, 0.96875],\n",
      "Time for 17730 iterations: 15397.95567393303\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17740 iterations: 15406.43958067894\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3135150671005249, 0.96875],\n",
      "Time for 17750 iterations: 15415.043127298355\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17760 iterations: 15423.512408018112\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14476993680000305, 0.96875],\n",
      "Time for 17770 iterations: 15432.213898897171\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17780 iterations: 15440.690002441406\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 79.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14251576364040375, 1.0],\n",
      "Time for 17790 iterations: 15449.386694669724\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 79.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17800 iterations: 15457.899277448654\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16629347205162048, 0.96875],\n",
      "Time for 17810 iterations: 15466.515132665634\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17820 iterations: 15474.995498895645\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.198190838098526, 0.96875],\n",
      "Time for 17830 iterations: 15483.737502098083\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17840 iterations: 15492.236997127533\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2812875807285309, 0.9375],\n",
      "Time for 17850 iterations: 15500.697890043259\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17860 iterations: 15509.29482293129\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20399606227874756, 0.9375],\n",
      "Time for 17870 iterations: 15517.855622529984\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17880 iterations: 15526.588921546936\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1342497169971466, 1.0],\n",
      "Time for 17890 iterations: 15535.061003446579\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17900 iterations: 15543.678825616837\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16193494200706482, 1.0],\n",
      "Time for 17910 iterations: 15552.204620838165\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17920 iterations: 15560.788548707962\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1628178358078003, 0.96875],\n",
      "Time for 17930 iterations: 15569.3089261055\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17940 iterations: 15577.880893230438\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13638772070407867, 1.0],\n",
      "Time for 17950 iterations: 15586.465905189514\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17960 iterations: 15594.904809713364\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21071356534957886, 0.96875],\n",
      "Time for 17970 iterations: 15603.500837564468\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 17980 iterations: 15611.990297079086\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.27737024426460266, 0.9375],\n",
      "Time for 17990 iterations: 15620.681033611298\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18000 iterations: 15629.142882108688\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1613137423992157, 1.0],\n",
      "Time for 18010 iterations: 15637.774603843689\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18020 iterations: 15646.28108882904\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1747419536113739, 1.0],\n",
      "Time for 18030 iterations: 15654.823837280273\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18040 iterations: 15663.356981277466\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15726467967033386, 1.0],\n",
      "Time for 18050 iterations: 15671.983521699905\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18060 iterations: 15680.664580345154\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.21459227800369263, 0.9375],\n",
      "Time for 18070 iterations: 15689.091074466705\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18080 iterations: 15697.687287807465\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17275747656822205, 0.96875],\n",
      "Time for 18090 iterations: 15706.237469911575\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18100 iterations: 15714.91150856018\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17678236961364746, 0.96875],\n",
      "Time for 18110 iterations: 15723.44440126419\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18120 iterations: 15732.051153421402\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22413218021392822, 0.96875],\n",
      "Time for 18130 iterations: 15740.57530760765\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18140 iterations: 15749.181118488312\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2743639647960663, 0.9375],\n",
      "Time for 18150 iterations: 15757.730256080627\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18160 iterations: 15766.341195583344\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15547379851341248, 1.0],\n",
      "Time for 18170 iterations: 15774.956146001816\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18180 iterations: 15783.447723150253\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20196598768234253, 1.0],\n",
      "Time for 18190 iterations: 15792.04643034935\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18200 iterations: 15800.591593265533\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.210683211684227, 0.96875],\n",
      "Time for 18210 iterations: 15809.287067174911\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18220 iterations: 15817.780316114426\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14358536899089813, 1.0],\n",
      "Time for 18230 iterations: 15826.377362012863\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18240 iterations: 15834.93530368805\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17581653594970703, 0.96875],\n",
      "Time for 18250 iterations: 15843.523419857025\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 77.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18260 iterations: 15852.01967549324\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1376507431268692, 1.0],\n",
      "Time for 18270 iterations: 15860.63128232956\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18280 iterations: 15869.209281682968\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24759414792060852, 0.9375],\n",
      "Time for 18290 iterations: 15877.672227859497\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18300 iterations: 15886.273198366165\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23071372509002686, 0.9375],\n",
      "Time for 18310 iterations: 15894.761459350586\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18320 iterations: 15903.341037988663\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.12524205446243286, 1.0],\n",
      "Time for 18330 iterations: 15911.935908794403\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18340 iterations: 15920.561112880707\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20903414487838745, 0.9375],\n",
      "Time for 18350 iterations: 15929.094532728195\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18360 iterations: 15937.67211318016\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15518853068351746, 1.0],\n",
      "Time for 18370 iterations: 15946.236606836319\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18380 iterations: 15954.796273469925\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1800127625465393, 1.0],\n",
      "Time for 18390 iterations: 15963.383154630661\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18400 iterations: 15971.85971736908\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1673683077096939, 0.96875],\n",
      "Time for 18410 iterations: 15980.469876289368\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18420 iterations: 15988.954418420792\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2415761649608612, 0.9375],\n",
      "Time for 18430 iterations: 15997.641135931015\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18440 iterations: 16006.17897939682\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.175308495759964, 0.96875],\n",
      "Time for 18450 iterations: 16014.753258943558\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18460 iterations: 16023.269117116928\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2423933893442154, 0.90625],\n",
      "Time for 18470 iterations: 16031.863472223282\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18480 iterations: 16040.357766866684\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3810173273086548, 0.875],\n",
      "Time for 18490 iterations: 16048.942551374435\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18500 iterations: 16057.54484796524\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17291158437728882, 1.0],\n",
      "Time for 18510 iterations: 16066.039559602737\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18520 iterations: 16074.66035604477\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1906665861606598, 0.96875],\n",
      "Time for 18530 iterations: 16083.153898000717\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18540 iterations: 16091.781449317932\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2829127311706543, 0.96875],\n",
      "Time for 18550 iterations: 16100.349647521973\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18560 iterations: 16108.936489582062\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15252260863780975, 1.0],\n",
      "Time for 18570 iterations: 16117.468112707138\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18580 iterations: 16125.92740392685\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.245004802942276, 0.96875],\n",
      "Time for 18590 iterations: 16134.561932086945\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 78.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18600 iterations: 16143.016956090927\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13996733725070953, 1.0],\n",
      "Time for 18610 iterations: 16151.550139904022\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18620 iterations: 16160.017828702927\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23448866605758667, 0.9375],\n",
      "Time for 18630 iterations: 16168.632026672363\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18640 iterations: 16177.117658138275\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15100190043449402, 1.0],\n",
      "Time for 18650 iterations: 16185.862425327301\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18660 iterations: 16194.413985490799\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17200037837028503, 1.0],\n",
      "Time for 18670 iterations: 16202.992019891739\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18680 iterations: 16211.535200357437\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.27822422981262207, 0.9375],\n",
      "Time for 18690 iterations: 16220.010746240616\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18700 iterations: 16228.757078409195\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.3427584767341614, 0.9375],\n",
      "Time for 18710 iterations: 16237.2122964859\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 78.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18720 iterations: 16245.807092428207\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2617284655570984, 0.9375],\n",
      "Time for 18730 iterations: 16254.281639099121\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18740 iterations: 16262.905155181885\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24295680224895477, 0.96875],\n",
      "Time for 18750 iterations: 16271.443759202957\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18760 iterations: 16280.131319999695\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17306244373321533, 1.0],\n",
      "Time for 18770 iterations: 16288.672493457794\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18780 iterations: 16297.3285009861\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20976462960243225, 0.96875],\n",
      "Time for 18790 iterations: 16305.848050117493\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18800 iterations: 16314.323135137558\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22354674339294434, 0.96875],\n",
      "Time for 18810 iterations: 16323.000628471375\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18820 iterations: 16331.50223851204\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14300015568733215, 1.0],\n",
      "Time for 18830 iterations: 16340.084179639816\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18840 iterations: 16348.632618188858\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17610818147659302, 1.0],\n",
      "Time for 18850 iterations: 16357.189471006393\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18860 iterations: 16365.7443857193\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2745320796966553, 0.9375],\n",
      "Time for 18870 iterations: 16374.463002443314\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18880 iterations: 16383.004888057709\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1447085738182068, 1.0],\n",
      "Time for 18890 iterations: 16391.58797287941\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18900 iterations: 16400.122965335846\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2363031804561615, 0.96875],\n",
      "Time for 18910 iterations: 16408.597157239914\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18920 iterations: 16417.262834072113\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 77.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1733590066432953, 0.96875],\n",
      "Time for 18930 iterations: 16425.723049402237\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18940 iterations: 16434.360829353333\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17309735715389252, 0.96875],\n",
      "Time for 18950 iterations: 16442.875919818878\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18960 iterations: 16451.49184370041\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 77.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13004563748836517, 1.0],\n",
      "Time for 18970 iterations: 16459.99704003334\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 18980 iterations: 16468.69913816452\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22548407316207886, 0.96875],\n",
      "Time for 18990 iterations: 16477.237100601196\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19000 iterations: 16485.837792158127\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17769759893417358, 0.96875],\n",
      "Time for 19010 iterations: 16494.43753504753\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19020 iterations: 16502.875987052917\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1958674192428589, 1.0],\n",
      "Time for 19030 iterations: 16511.558827877045\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19040 iterations: 16520.046028375626\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15948787331581116, 1.0],\n",
      "Time for 19050 iterations: 16528.679184436798\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19060 iterations: 16537.138710975647\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14307424426078796, 1.0],\n",
      "Time for 19070 iterations: 16545.754174232483\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19080 iterations: 16554.257237672806\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.408021479845047, 0.90625],\n",
      "Time for 19090 iterations: 16562.989574432373\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 78.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19100 iterations: 16571.547449588776\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16759052872657776, 0.96875],\n",
      "Time for 19110 iterations: 16580.176443338394\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19120 iterations: 16588.68017601967\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20915362238883972, 0.96875],\n",
      "Time for 19130 iterations: 16597.135059595108\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19140 iterations: 16605.771770477295\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2806967496871948, 0.9375],\n",
      "Time for 19150 iterations: 16614.256945371628\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19160 iterations: 16622.851248264313\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2118951678276062, 0.96875],\n",
      "Time for 19170 iterations: 16631.354048252106\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19180 iterations: 16639.938176870346\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.350643128156662, 0.90625],\n",
      "Time for 19190 iterations: 16648.472744464874\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19200 iterations: 16657.17678117752\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13863974809646606, 1.0],\n",
      "Time for 19210 iterations: 16665.740553855896\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19220 iterations: 16674.251944303513\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.17825675010681152, 0.96875],\n",
      "Time for 19230 iterations: 16682.862052440643\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19240 iterations: 16691.2601416111\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1657504439353943, 1.0],\n",
      "Time for 19250 iterations: 16699.846596479416\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19260 iterations: 16708.475205898285\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1953338086605072, 0.96875],\n",
      "Time for 19270 iterations: 16717.02739930153\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19280 iterations: 16725.579503297806\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20158591866493225, 0.96875],\n",
      "Time for 19290 iterations: 16734.17720389366\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19300 iterations: 16742.713785648346\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.24231016635894775, 0.96875],\n",
      "Time for 19310 iterations: 16751.343488931656\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19320 iterations: 16759.92096376419\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1553073227405548, 0.96875],\n",
      "Time for 19330 iterations: 16768.399383306503\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19340 iterations: 16777.10783481598\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.19404032826423645, 0.9375],\n",
      "Time for 19350 iterations: 16785.560775995255\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19360 iterations: 16794.24555826187\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18469685316085815, 0.96875],\n",
      "Time for 19370 iterations: 16802.747064352036\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19380 iterations: 16811.342209339142\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.34269052743911743, 0.90625],\n",
      "Time for 19390 iterations: 16819.88112258911\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19400 iterations: 16828.500846862793\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 78.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13299442827701569, 1.0],\n",
      "Time for 19410 iterations: 16837.29603457451\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19420 iterations: 16845.90860915184\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 78.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.15407489240169525, 1.0],\n",
      "Time for 19430 iterations: 16854.41998553276\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 77.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19440 iterations: 16862.900666475296\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13400326669216156, 1.0],\n",
      "Time for 19450 iterations: 16871.505712032318\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19460 iterations: 16880.000153779984\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16744059324264526, 0.96875],\n",
      "Time for 19470 iterations: 16888.70676445961\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19480 iterations: 16897.206525087357\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.36978060007095337, 0.90625],\n",
      "Time for 19490 iterations: 16905.899563789368\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19500 iterations: 16914.373688459396\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14501389861106873, 1.0],\n",
      "Time for 19510 iterations: 16922.9696559906\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19520 iterations: 16931.5667822361\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.289124071598053, 0.96875],\n",
      "Time for 19530 iterations: 16940.18225002289\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19540 iterations: 16948.916704654694\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.40695685148239136, 0.90625],\n",
      "Time for 19550 iterations: 16957.359612226486\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19560 iterations: 16965.994809627533\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 78.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1363191455602646, 1.0],\n",
      "Time for 19570 iterations: 16974.474816560745\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19580 iterations: 16983.23571705818\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.31654292345046997, 0.9375],\n",
      "Time for 19590 iterations: 16991.741250038147\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19600 iterations: 17000.372386932373\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 77.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.22254355251789093, 0.90625],\n",
      "Time for 19610 iterations: 17008.885591506958\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19620 iterations: 17017.49160838127\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23792363703250885, 0.96875],\n",
      "Time for 19630 iterations: 17026.04935693741\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19640 iterations: 17034.598472833633\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.23727422952651978, 0.96875],\n",
      "Time for 19650 iterations: 17043.161016702652\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19660 iterations: 17051.65885233879\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20906487107276917, 0.9375],\n",
      "Time for 19670 iterations: 17060.28461575508\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 66.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19680 iterations: 17068.758249282837\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18213236331939697, 0.96875],\n",
      "Time for 19690 iterations: 17077.485680818558\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19700 iterations: 17085.982499837875\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2012726366519928, 0.96875],\n",
      "Time for 19710 iterations: 17094.572138786316\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19720 iterations: 17103.076248168945\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.20776957273483276, 0.96875],\n",
      "Time for 19730 iterations: 17111.697947740555\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19740 iterations: 17120.359872579575\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1566988229751587, 0.96875],\n",
      "Time for 19750 iterations: 17129.00387239456\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19760 iterations: 17137.493367433548\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2276158630847931, 0.96875],\n",
      "Time for 19770 iterations: 17145.93448114395\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19780 iterations: 17154.549060106277\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13916820287704468, 1.0],\n",
      "Time for 19790 iterations: 17163.05475616455\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19800 iterations: 17171.77470946312\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1825219988822937, 0.96875],\n",
      "Time for 19810 iterations: 17180.287285089493\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19820 iterations: 17188.912471055984\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 77.2% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1824733167886734, 0.96875],\n",
      "Time for 19830 iterations: 17197.427714586258\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19840 iterations: 17206.03512454033\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.13885441422462463, 1.0],\n",
      "Time for 19850 iterations: 17214.643707752228\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19860 iterations: 17223.08787536621\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.2750234007835388, 0.90625],\n",
      "Time for 19870 iterations: 17231.69458580017\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19880 iterations: 17240.115785598755\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1451081931591034, 1.0],\n",
      "Time for 19890 iterations: 17248.695204496384\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 75.6% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19900 iterations: 17257.16078352928\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.6% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.16956296563148499, 0.96875],\n",
      "Time for 19910 iterations: 17265.889211654663\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19920 iterations: 17274.413805007935\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 64.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.14820607006549835, 1.0],\n",
      "Time for 19930 iterations: 17283.02295732498\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 79.2% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19940 iterations: 17291.541446447372\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.18452981114387512, 0.96875],\n",
      "Time for 19950 iterations: 17300.117316246033\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19960 iterations: 17308.753908634186\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 70.8% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.38570746779441833, 0.90625],\n",
      "Time for 19970 iterations: 17317.226904153824\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 76.0% 20 way one-shot learning accuracy \n",
      "\n",
      "Time for 19980 iterations: 17325.93214392662\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.4% 20 way one-shot learning accuracy \n",
      "\n",
      "training loss: [0.1914965659379959, 0.96875],\n",
      "Time for 19990 iterations: 17334.407027959824\n",
      "Evaluating model on 250 random 20 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 72.4% 20 way one-shot learning accuracy \n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_every = 10 # interval for evaluating on one-shot tasks\n",
    "loss_every = 20 # interval for printing loss (iterations)\n",
    "batch_size = 32\n",
    "n_iter = 20000\n",
    "N_way = 20 # how many classes for testing one-shot tasks>\n",
    "n_val = 250 # how many one-shot tasks to validate on?\n",
    "best = -1\n",
    "weights_path_2 = os.path.join(data_path, \"model_omniglot_weights.h5\")\n",
    "\n",
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "with tf.device('/gpu:1'):\n",
    "    t_start = time.time()\n",
    "    for i in range(1, n_iter):\n",
    "        (inputs,targets)=loader.get_batch(batch_size)\n",
    "        loss=model.train_on_batch(inputs,targets)\n",
    "        #print(\"\\n ------------- \\n\")\n",
    "        #print(\"Loss: {0}\".format(loss)) \n",
    "        if i % evaluate_every == 0:\n",
    "            print(\"Time for {0} iterations: {1}\".format(i, time.time()-t_start))\n",
    "            val_acc = loader.test_oneshot(model,N_way,n_val,verbose=True)\n",
    "            if val_acc >= best:\n",
    "                print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "                print(\"Saving weights to: {0} \\n\".format(weights_path_2))\n",
    "                model.save_weights(weights_path_2)\n",
    "                best=val_acc\n",
    "        \n",
    "        if i % loss_every == 0:\n",
    "            print(\"training loss: {0},\".format(loss))\n",
    "model.load_weights(weights_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''evaluate_every = 1 # interval for evaluating on one-shot tasks\n",
    "loss_every = 50 # interval for printing loss (iterations)\n",
    "batch_size = 32\n",
    "n_iter = 9000\n",
    "N_way = 20 # how many classes for testing one-shot tasks>\n",
    "n_val = 550 # how many one-shot tasks to validate on?\n",
    "best = -1\n",
    "weights_path_2 = os.path.join(data_path, \"model_weights_color.h5\")\n",
    "\n",
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "#from keras_input_pipeline import *\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "model.summary()\n",
    "with tf.device('/gpu:1'):\n",
    "    t_start = time.time()\n",
    "    for i in range(1, n_iter):\n",
    "        (inputs,targets)=loader.get_batch(batch_size)\n",
    "        loss=model.train_on_batch(inputs,targets)\n",
    "        #print(\"\\n ------------- \\n\")\n",
    "        #print(\"Loss: {0}\".format(loss)) \n",
    "        if i % evaluate_every == 0:\n",
    "            print(\"Time for {0} iterations: {1}\".format(i, time.time()-t_start))\n",
    "            val_acc = loader.test_oneshot(model,N_way,n_val,verbose=True)\n",
    "            if val_acc >= best:\n",
    "                print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "                #print(\"Saving weights to: {0} \\n\".format(weights_path))\n",
    "                model.save_weights(weights_path_2)\n",
    "                best=val_acc\n",
    "        \n",
    "        if i % loss_every == 0:\n",
    "            print(\"iteration {}, training loss: {},\".format(i,loss))\n",
    "            print(\"\\n ---------------------------------------------------- \\n\")\n",
    "        \n",
    "model.load_weights(weights_path_2)"
   ]
  },
  {
   "source": [
    "### Testing pipeline "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluating model on 450 random 1 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 100.0% 1 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 450 random 1 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 100.0% 1 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 450 unique 1 way one-shot learning tasks ...\n",
      "Evaluating model on 450 random 3 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.44444444444444% 3 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 450 random 3 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.66666666666667% 3 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 450 unique 3 way one-shot learning tasks ...\n",
      "Evaluating model on 450 random 5 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 90.0% 5 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 450 random 5 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.22222222222223% 5 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 450 unique 5 way one-shot learning tasks ...\n",
      "Evaluating model on 450 random 7 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 86.0% 7 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 450 random 7 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 94.66666666666667% 7 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 450 unique 7 way one-shot learning tasks ...\n",
      "Evaluating model on 450 random 9 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 84.88888888888889% 9 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 450 random 9 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 94.66666666666667% 9 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 450 unique 9 way one-shot learning tasks ...\n",
      "Evaluating model on 450 random 11 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 78.66666666666667% 11 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 450 random 11 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 94.0% 11 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 450 unique 11 way one-shot learning tasks ...\n",
      "Evaluating model on 450 random 13 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 82.66666666666667% 13 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 450 random 13 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 93.77777777777777% 13 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 450 unique 13 way one-shot learning tasks ...\n",
      "Evaluating model on 450 random 15 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 78.44444444444444% 15 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 450 random 15 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 92.66666666666667% 15 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 450 unique 15 way one-shot learning tasks ...\n",
      "Evaluating model on 450 random 17 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 74.0% 17 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 450 random 17 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 93.33333333333333% 17 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 450 unique 17 way one-shot learning tasks ...\n",
      "Evaluating model on 450 random 19 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 68.66666666666667% 19 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 450 random 19 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 90.66666666666667% 19 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 450 unique 19 way one-shot learning tasks ...\n",
      "Evaluating model on 450 random 21 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 73.11111111111111% 21 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 450 random 21 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 90.22222222222223% 21 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 450 unique 21 way one-shot learning tasks ...\n",
      "Evaluating model on 450 random 23 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 69.55555555555556% 23 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 450 random 23 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 89.55555555555556% 23 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 450 unique 23 way one-shot learning tasks ...\n",
      "Evaluating model on 450 random 25 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.11111111111111% 25 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 450 random 25 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 88.44444444444444% 25 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 450 unique 25 way one-shot learning tasks ...\n",
      "Evaluating model on 450 random 27 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.55555555555556% 27 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 450 random 27 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 86.66666666666667% 27 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 450 unique 27 way one-shot learning tasks ...\n",
      "Evaluating model on 450 random 29 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 65.33333333333333% 29 way one-shot learning accuracy \n",
      "\n",
      "Evaluating model on 450 random 29 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 86.22222222222223% 29 way one-shot learning accuracy \n",
      "\n",
      "Evaluating nearest neighbour on 450 unique 29 way one-shot learning tasks ...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 377.301948 248.518125\" width=\"377.301948pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-01-24T15:34:52.772433</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 377.301948 248.518125 \nL 377.301948 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 224.64 \nL 368.0875 224.64 \nL 368.0875 7.2 \nL 33.2875 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m98cdc9230b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"37.635552\" xlink:href=\"#m98cdc9230b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(34.454302 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"91.986201\" xlink:href=\"#m98cdc9230b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(88.804951 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"146.336851\" xlink:href=\"#m98cdc9230b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(139.974351 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"200.6875\" xlink:href=\"#m98cdc9230b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(194.325 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"255.038149\" xlink:href=\"#m98cdc9230b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(248.675649 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"309.388799\" xlink:href=\"#m98cdc9230b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(303.026299 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.739448\" xlink:href=\"#m98cdc9230b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <g transform=\"translate(357.376948 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"ma086c25c30\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma086c25c30\" y=\"219.709439\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 223.508658)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma086c25c30\" y=\"179.184278\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 20 -->\n      <g transform=\"translate(13.5625 182.983497)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma086c25c30\" y=\"138.659118\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 40 -->\n      <g transform=\"translate(13.5625 142.458337)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma086c25c30\" y=\"98.133957\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 60 -->\n      <g transform=\"translate(13.5625 101.933176)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma086c25c30\" y=\"57.608797\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 80 -->\n      <g transform=\"translate(13.5625 61.408016)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#ma086c25c30\" y=\"17.083636\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 20.882855)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p4b1f31d4ef)\" d=\"M 48.505682 17.083636 \nL 70.245942 32.393141 \nL 91.986201 37.346217 \nL 113.726461 45.451249 \nL 135.466721 47.702647 \nL 157.206981 60.310474 \nL 178.94724 52.205442 \nL 200.6875 60.760754 \nL 222.42776 69.766345 \nL 244.168019 80.573054 \nL 265.908279 71.567463 \nL 287.648539 78.771936 \nL 309.388799 75.619979 \nL 331.129058 86.876968 \nL 352.869318 87.327248 \n\" style=\"fill:none;stroke:#bf00bf;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p4b1f31d4ef)\" d=\"M 48.505682 17.083636 \nL 70.245942 19.785314 \nL 91.986201 20.685873 \nL 113.726461 27.890346 \nL 135.466721 27.890346 \nL 157.206981 29.241185 \nL 178.94724 29.691464 \nL 200.6875 31.942862 \nL 222.42776 30.592023 \nL 244.168019 35.995378 \nL 265.908279 36.895937 \nL 287.648539 38.246776 \nL 309.388799 40.498174 \nL 331.129058 44.10041 \nL 352.869318 45.000969 \n\" style=\"fill:none;stroke:#bfbf00;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p4b1f31d4ef)\" d=\"M 48.505682 17.083636 \nL 70.245942 146.31387 \nL 91.986201 179.184278 \nL 113.726461 185.488192 \nL 135.466721 199.446859 \nL 157.206981 198.546299 \nL 178.94724 202.598815 \nL 200.6875 206.201052 \nL 222.42776 208.00217 \nL 244.168019 212.955245 \nL 265.908279 210.253568 \nL 287.648539 208.902729 \nL 309.388799 212.054686 \nL 331.129058 213.405525 \nL 352.869318 214.756364 \n\" style=\"fill:none;stroke:#00bfbf;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p4b1f31d4ef)\" d=\"M 48.505682 17.083636 \nL 70.245942 152.167505 \nL 91.986201 179.184278 \nL 113.726461 190.762896 \nL 135.466721 197.195461 \nL 157.206981 201.288911 \nL 178.94724 204.122839 \nL 200.6875 206.201052 \nL 222.42776 207.790274 \nL 244.168019 209.044923 \nL 265.908279 210.060591 \nL 287.648539 210.899621 \nL 309.388799 211.604407 \nL 331.129058 212.204779 \nL 352.869318 212.722342 \n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 224.64 \nL 33.2875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 368.0875 224.64 \nL 368.0875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 224.64 \nL 368.0875 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 7.2 \nL 368.0875 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4b1f31d4ef\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "def nearest_neighbour_correct(pairs,targets):\n",
    "    \"\"\"returns 1 if nearest neighbour gets the correct answer for a one-shot task\n",
    "        given by (pairs, targets)\"\"\"\n",
    "    L2_distances = np.zeros_like(targets)\n",
    "    for i in range(len(targets)):\n",
    "        L2_distances[i] = np.sum(np.sqrt(pairs[0][i]**2 - pairs[1][i]**2))\n",
    "    if np.argmin(L2_distances) == np.argmax(targets):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test_nn_accuracy(N_ways,n_trials,loader):\n",
    "    \"\"\"Returns accuracy of one shot \"\"\"\n",
    "    print(\"Evaluating nearest neighbour on {} unique {} way one-shot learning tasks ...\".format(n_trials,N_ways))\n",
    "\n",
    "    n_right = 0\n",
    "    \n",
    "    for i in range(n_trials):\n",
    "        pairs,targets = loader.make_oneshot_task(N_ways,\"val\")\n",
    "        correct = nearest_neighbour_correct(pairs,targets)\n",
    "        n_right += correct\n",
    "    return 100.0 * n_right / n_trials\n",
    "\n",
    "\n",
    "ways = np.arange(1, 30, 2)\n",
    "resume =  False\n",
    "val_accs, train_accs,nn_accs = [], [], []\n",
    "trials = 450\n",
    "for N in ways:\n",
    "    val_accs.append(loader.test_oneshot(model, N,trials, \"val\", verbose=True))\n",
    "    train_accs.append(loader.test_oneshot(model, N,trials, \"train\", verbose=True))\n",
    "    nn_accs.append(test_nn_accuracy(N,trials, loader))\n",
    "    \n",
    "#plot the accuracy vs num categories for each\n",
    "plt.plot(ways, val_accs, \"m\")\n",
    "plt.plot(ways, train_accs, \"y\")\n",
    "plt.plot(ways, nn_accs, \"c\")\n",
    "\n",
    "plt.plot(ways,100.0/ways,\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 471.485312 277.314375\" width=\"471.485312pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-01-24T15:35:40.696413</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 471.485312 277.314375 \nL 471.485312 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 62.465625 239.758125 \nL 330.305625 239.758125 \nL 330.305625 22.318125 \nL 62.465625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mdf2953036a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.944067\" xlink:href=\"#mdf2953036a\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(62.762817 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"109.424586\" xlink:href=\"#mdf2953036a\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(106.243336 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"152.905106\" xlink:href=\"#mdf2953036a\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(146.542606 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"196.385625\" xlink:href=\"#mdf2953036a\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(190.023125 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"239.866144\" xlink:href=\"#mdf2953036a\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(233.503644 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"283.346664\" xlink:href=\"#mdf2953036a\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(276.984164 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"326.827183\" xlink:href=\"#mdf2953036a\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <g transform=\"translate(320.464683 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- Number of possible classes in one-shot tasks -->\n     <g transform=\"translate(83.792656 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 23.09375 72.90625 \nL 55.421875 11.921875 \nL 55.421875 72.90625 \nL 64.984375 72.90625 \nL 64.984375 0 \nL 51.703125 0 \nL 19.390625 60.984375 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-78\"/>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n       <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n       <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path id=\"DejaVuSans-32\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n       <path d=\"M 4.890625 31.390625 \nL 31.203125 31.390625 \nL 31.203125 23.390625 \nL 4.890625 23.390625 \nz\n\" id=\"DejaVuSans-45\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path d=\"M 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 31.109375 \nL 44.921875 54.6875 \nL 56.390625 54.6875 \nL 27.390625 29.109375 \nL 57.625 0 \nL 45.90625 0 \nL 18.109375 26.703125 \nL 18.109375 0 \nL 9.078125 0 \nz\n\" id=\"DejaVuSans-107\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-78\"/>\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"138.183594\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"235.595703\" xlink:href=\"#DejaVuSans-98\"/>\n      <use x=\"299.072266\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"360.595703\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"401.708984\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"433.496094\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"494.677734\" xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"529.882812\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"561.669922\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"625.146484\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"686.328125\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"738.427734\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"790.527344\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"818.310547\" xlink:href=\"#DejaVuSans-98\"/>\n      <use x=\"881.787109\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"909.570312\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"971.09375\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1002.880859\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"1057.861328\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"1085.644531\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"1146.923828\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"1199.023438\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"1251.123047\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"1312.646484\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"1364.746094\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1396.533203\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"1424.316406\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"1487.695312\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1519.482422\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"1580.664062\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"1644.042969\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"1705.566406\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"1741.650391\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"1793.75\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"1857.128906\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"1918.310547\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"1957.519531\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"1989.306641\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"2028.515625\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"2089.794922\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"2141.894531\" xlink:href=\"#DejaVuSans-107\"/>\n      <use x=\"2199.804688\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m7b42a87604\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.465625\" xlink:href=\"#m7b42a87604\" y=\"234.827564\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0 -->\n      <g transform=\"translate(49.103125 238.626783)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.465625\" xlink:href=\"#m7b42a87604\" y=\"194.302403\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 20 -->\n      <g transform=\"translate(42.740625 198.101622)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.465625\" xlink:href=\"#m7b42a87604\" y=\"153.777243\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 40 -->\n      <g transform=\"translate(42.740625 157.576462)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.465625\" xlink:href=\"#m7b42a87604\" y=\"113.252082\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 60 -->\n      <g transform=\"translate(42.740625 117.051301)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.465625\" xlink:href=\"#m7b42a87604\" y=\"72.726922\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 80 -->\n      <g transform=\"translate(42.740625 76.526141)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.465625\" xlink:href=\"#m7b42a87604\" y=\"32.201761\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 100 -->\n      <g transform=\"translate(36.378125 36.00098)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- % Accuracy -->\n     <g transform=\"translate(30.298437 160.206094)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 72.703125 32.078125 \nQ 68.453125 32.078125 66.03125 28.46875 \nQ 63.625 24.859375 63.625 18.40625 \nQ 63.625 12.0625 66.03125 8.421875 \nQ 68.453125 4.78125 72.703125 4.78125 \nQ 76.859375 4.78125 79.265625 8.421875 \nQ 81.6875 12.0625 81.6875 18.40625 \nQ 81.6875 24.8125 79.265625 28.4375 \nQ 76.859375 32.078125 72.703125 32.078125 \nz\nM 72.703125 38.28125 \nQ 80.421875 38.28125 84.953125 32.90625 \nQ 89.5 27.546875 89.5 18.40625 \nQ 89.5 9.28125 84.9375 3.921875 \nQ 80.375 -1.421875 72.703125 -1.421875 \nQ 64.890625 -1.421875 60.34375 3.921875 \nQ 55.8125 9.28125 55.8125 18.40625 \nQ 55.8125 27.59375 60.375 32.9375 \nQ 64.9375 38.28125 72.703125 38.28125 \nz\nM 22.3125 68.015625 \nQ 18.109375 68.015625 15.6875 64.375 \nQ 13.28125 60.75 13.28125 54.390625 \nQ 13.28125 47.953125 15.671875 44.328125 \nQ 18.0625 40.71875 22.3125 40.71875 \nQ 26.5625 40.71875 28.96875 44.328125 \nQ 31.390625 47.953125 31.390625 54.390625 \nQ 31.390625 60.6875 28.953125 64.34375 \nQ 26.515625 68.015625 22.3125 68.015625 \nz\nM 66.40625 74.21875 \nL 74.21875 74.21875 \nL 28.609375 -1.421875 \nL 20.796875 -1.421875 \nz\nM 22.3125 74.21875 \nQ 30.03125 74.21875 34.609375 68.875 \nQ 39.203125 63.53125 39.203125 54.390625 \nQ 39.203125 45.171875 34.640625 39.84375 \nQ 30.078125 34.515625 22.3125 34.515625 \nQ 14.546875 34.515625 10.03125 39.859375 \nQ 5.515625 45.21875 5.515625 54.390625 \nQ 5.515625 63.484375 10.046875 68.84375 \nQ 14.59375 74.21875 22.3125 74.21875 \nz\n\" id=\"DejaVuSans-37\"/>\n       <path d=\"M 34.1875 63.1875 \nL 20.796875 26.90625 \nL 47.609375 26.90625 \nz\nM 28.609375 72.90625 \nL 39.796875 72.90625 \nL 67.578125 0 \nL 57.328125 0 \nL 50.6875 18.703125 \nL 17.828125 18.703125 \nL 11.1875 0 \nL 0.78125 0 \nz\n\" id=\"DejaVuSans-65\"/>\n       <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-37\"/>\n      <use x=\"95.019531\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"126.806641\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"193.464844\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"248.445312\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"303.425781\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"366.804688\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"407.917969\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"469.197266\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"524.177734\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p7806768fa3)\" d=\"M 74.64017 32.201761 \nL 92.032378 47.511266 \nL 109.424586 52.464342 \nL 126.816794 60.569374 \nL 144.209002 62.820772 \nL 161.601209 75.428599 \nL 178.993417 67.323567 \nL 196.385625 75.878879 \nL 213.777833 84.88447 \nL 231.170041 95.691179 \nL 248.562248 86.685588 \nL 265.954456 93.890061 \nL 283.346664 90.738104 \nL 300.738872 101.995093 \nL 318.13108 102.445373 \n\" style=\"fill:none;stroke:#bf00bf;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p7806768fa3)\" d=\"M 74.64017 32.201761 \nL 92.032378 34.903439 \nL 109.424586 35.803998 \nL 126.816794 43.008471 \nL 144.209002 43.008471 \nL 161.601209 44.35931 \nL 178.993417 44.809589 \nL 196.385625 47.060987 \nL 213.777833 45.710148 \nL 231.170041 51.113503 \nL 248.562248 52.014062 \nL 265.954456 53.364901 \nL 283.346664 55.616299 \nL 300.738872 59.218535 \nL 318.13108 60.119094 \n\" style=\"fill:none;stroke:#bfbf00;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p7806768fa3)\" d=\"M 74.64017 32.201761 \nL 92.032378 161.431995 \nL 109.424586 194.302403 \nL 126.816794 200.606317 \nL 144.209002 214.564984 \nL 161.601209 213.664424 \nL 178.993417 217.71694 \nL 196.385625 221.319177 \nL 213.777833 223.120295 \nL 231.170041 228.07337 \nL 248.562248 225.371693 \nL 265.954456 224.020854 \nL 283.346664 227.172811 \nL 300.738872 228.52365 \nL 318.13108 229.874489 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p7806768fa3)\" d=\"M 74.64017 32.201761 \nL 92.032378 167.28563 \nL 109.424586 194.302403 \nL 126.816794 205.881021 \nL 144.209002 212.313586 \nL 161.601209 216.407036 \nL 178.993417 219.240964 \nL 196.385625 221.319177 \nL 213.777833 222.908399 \nL 231.170041 224.163048 \nL 248.562248 225.178716 \nL 265.954456 226.017746 \nL 283.346664 226.722532 \nL 300.738872 227.322904 \nL 318.13108 227.840467 \n\" style=\"fill:none;stroke:#008000;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 62.465625 239.758125 \nL 62.465625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 330.305625 239.758125 \nL 330.305625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 62.465625 239.758125 \nL 330.305625 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 62.465625 22.318125 \nL 330.305625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_16\">\n    <!-- Omiglot One-Shot Learning Performance of a Siamese Network -->\n    <g transform=\"translate(7.2 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 39.40625 66.21875 \nQ 28.65625 66.21875 22.328125 58.203125 \nQ 16.015625 50.203125 16.015625 36.375 \nQ 16.015625 22.609375 22.328125 14.59375 \nQ 28.65625 6.59375 39.40625 6.59375 \nQ 50.140625 6.59375 56.421875 14.59375 \nQ 62.703125 22.609375 62.703125 36.375 \nQ 62.703125 50.203125 56.421875 58.203125 \nQ 50.140625 66.21875 39.40625 66.21875 \nz\nM 39.40625 74.21875 \nQ 54.734375 74.21875 63.90625 63.9375 \nQ 73.09375 53.65625 73.09375 36.375 \nQ 73.09375 19.140625 63.90625 8.859375 \nQ 54.734375 -1.421875 39.40625 -1.421875 \nQ 24.03125 -1.421875 14.8125 8.828125 \nQ 5.609375 19.09375 5.609375 36.375 \nQ 5.609375 53.65625 14.8125 63.9375 \nQ 24.03125 74.21875 39.40625 74.21875 \nz\n\" id=\"DejaVuSans-79\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      <path d=\"M 4.203125 54.6875 \nL 13.1875 54.6875 \nL 24.421875 12.015625 \nL 35.59375 54.6875 \nL 46.1875 54.6875 \nL 57.421875 12.015625 \nL 68.609375 54.6875 \nL 77.59375 54.6875 \nL 63.28125 0 \nL 52.6875 0 \nL 40.921875 44.828125 \nL 29.109375 0 \nL 18.5 0 \nz\n\" id=\"DejaVuSans-119\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-79\"/>\n     <use x=\"78.710938\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"176.123047\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"203.90625\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"267.382812\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"295.166016\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"356.347656\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"395.556641\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"427.34375\" xlink:href=\"#DejaVuSans-79\"/>\n     <use x=\"506.054688\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"569.433594\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"630.957031\" xlink:href=\"#DejaVuSans-45\"/>\n     <use x=\"667.041016\" xlink:href=\"#DejaVuSans-83\"/>\n     <use x=\"730.517578\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"793.896484\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"855.078125\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"894.287109\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"926.074219\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"980.037109\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1041.560547\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1102.839844\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1142.203125\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1205.582031\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1233.365234\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1296.744141\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"1360.220703\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1392.007812\" xlink:href=\"#DejaVuSans-80\"/>\n     <use x=\"1448.685547\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1510.208984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1551.322266\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"1586.527344\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1647.708984\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1687.072266\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"1784.484375\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1845.763672\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1909.142578\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1964.123047\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"2025.646484\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2057.433594\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"2118.615234\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"2153.820312\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2185.607422\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2246.886719\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2278.673828\" xlink:href=\"#DejaVuSans-83\"/>\n     <use x=\"2342.150391\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"2369.933594\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"2431.212891\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"2528.625\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"2590.148438\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"2642.248047\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"2703.771484\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"2735.558594\" xlink:href=\"#DejaVuSans-78\"/>\n     <use x=\"2810.363281\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"2871.886719\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"2911.095703\" xlink:href=\"#DejaVuSans-119\"/>\n     <use x=\"2992.882812\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"3054.064453\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"3095.177734\" xlink:href=\"#DejaVuSans-107\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 337.305625 161.894375 \nL 462.285313 161.894375 \nQ 464.285313 161.894375 464.285313 159.894375 \nL 464.285313 102.181875 \nQ 464.285313 100.181875 462.285313 100.181875 \nL 337.305625 100.181875 \nQ 335.305625 100.181875 335.305625 102.181875 \nL 335.305625 159.894375 \nQ 335.305625 161.894375 337.305625 161.894375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_18\">\n     <path d=\"M 339.305625 108.280312 \nL 359.305625 108.280312 \n\" style=\"fill:none;stroke:#bf00bf;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_19\"/>\n    <g id=\"text_17\">\n     <!-- Siamese(val set) -->\n     <g transform=\"translate(367.305625 111.780312)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 31 75.875 \nQ 24.46875 64.65625 21.28125 53.65625 \nQ 18.109375 42.671875 18.109375 31.390625 \nQ 18.109375 20.125 21.3125 9.0625 \nQ 24.515625 -2 31 -13.1875 \nL 23.1875 -13.1875 \nQ 15.875 -1.703125 12.234375 9.375 \nQ 8.59375 20.453125 8.59375 31.390625 \nQ 8.59375 42.28125 12.203125 53.3125 \nQ 15.828125 64.359375 23.1875 75.875 \nz\n\" id=\"DejaVuSans-40\"/>\n       <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n       <path d=\"M 8.015625 75.875 \nL 15.828125 75.875 \nQ 23.140625 64.359375 26.78125 53.3125 \nQ 30.421875 42.28125 30.421875 31.390625 \nQ 30.421875 20.453125 26.78125 9.375 \nQ 23.140625 -1.703125 15.828125 -13.1875 \nL 8.015625 -13.1875 \nQ 14.5 -2 17.703125 9.0625 \nQ 20.90625 20.125 20.90625 31.390625 \nQ 20.90625 42.671875 17.703125 53.65625 \nQ 14.5 64.65625 8.015625 75.875 \nz\n\" id=\"DejaVuSans-41\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-83\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"91.259766\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"152.539062\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"249.951172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"311.474609\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"363.574219\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"425.097656\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"464.111328\" xlink:href=\"#DejaVuSans-118\"/>\n      <use x=\"523.291016\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"584.570312\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"612.353516\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"644.140625\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"696.240234\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"757.763672\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"796.972656\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n    <g id=\"line2d_20\">\n     <path d=\"M 339.305625 122.958437 \nL 359.305625 122.958437 \n\" style=\"fill:none;stroke:#bfbf00;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_21\"/>\n    <g id=\"text_18\">\n     <!-- Siamese(train set) -->\n     <g transform=\"translate(367.305625 126.458437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-83\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"91.259766\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"152.539062\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"249.951172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"311.474609\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"363.574219\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"425.097656\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"464.111328\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"503.320312\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"544.433594\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"605.712891\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"633.496094\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"696.875\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"728.662109\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"780.761719\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"842.285156\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"881.494141\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n    <g id=\"line2d_22\">\n     <path d=\"M 339.305625 137.636562 \nL 359.305625 137.636562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_23\"/>\n    <g id=\"text_19\">\n     <!-- Nearest neighbour -->\n     <g transform=\"translate(367.305625 141.136562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-78\"/>\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"136.328125\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"197.607422\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"236.470703\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"297.994141\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"350.09375\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"389.302734\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"421.089844\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"484.46875\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"545.992188\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"573.775391\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"637.251953\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"700.630859\" xlink:href=\"#DejaVuSans-98\"/>\n      <use x=\"764.107422\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"825.289062\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"888.667969\" xlink:href=\"#DejaVuSans-114\"/>\n     </g>\n    </g>\n    <g id=\"line2d_24\">\n     <path d=\"M 339.305625 152.314687 \nL 359.305625 152.314687 \n\" style=\"fill:none;stroke:#008000;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_25\"/>\n    <g id=\"text_20\">\n     <!-- Random guessing -->\n     <g transform=\"translate(367.305625 155.814687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 44.390625 34.1875 \nQ 47.5625 33.109375 50.5625 29.59375 \nQ 53.5625 26.078125 56.59375 19.921875 \nL 66.609375 0 \nL 56 0 \nL 46.6875 18.703125 \nQ 43.0625 26.03125 39.671875 28.421875 \nQ 36.28125 30.8125 30.421875 30.8125 \nL 19.671875 30.8125 \nL 19.671875 0 \nL 9.8125 0 \nL 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.578125 72.90625 50.734375 67.671875 \nQ 56.890625 62.453125 56.890625 51.90625 \nQ 56.890625 45.015625 53.6875 40.46875 \nQ 50.484375 35.9375 44.390625 34.1875 \nz\nM 19.671875 64.796875 \nL 19.671875 38.921875 \nL 32.078125 38.921875 \nQ 39.203125 38.921875 42.84375 42.21875 \nQ 46.484375 45.515625 46.484375 51.90625 \nQ 46.484375 58.296875 42.84375 61.546875 \nQ 39.203125 64.796875 32.078125 64.796875 \nz\n\" id=\"DejaVuSans-82\"/>\n       <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-82\"/>\n      <use x=\"67.232422\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"128.511719\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"191.890625\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"255.367188\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"316.548828\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"413.960938\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"445.748047\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"509.224609\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"572.603516\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"634.126953\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"686.226562\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"738.326172\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"766.109375\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"829.488281\" xlink:href=\"#DejaVuSans-103\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p7806768fa3\">\n   <rect height=\"217.44\" width=\"267.84\" x=\"62.465625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(20, 105, 105, 1)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"238.84pt\" version=\"1.1\" viewBox=\"0 0 116.736364 238.84\" width=\"116.736364pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-01-24T15:35:40.928173</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 238.84 \nL 116.736364 238.84 \nL 116.736364 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 10.7 109.536364 \nL 109.536364 109.536364 \nL 109.536364 10.7 \nL 10.7 10.7 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pc25741e13b)\">\n    <image height=\"99\" id=\"imageb5c34a0b7b\" transform=\"scale(1 -1)translate(0 -99)\" width=\"99\" x=\"10.7\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAGMAAABjCAYAAACPO76VAAACHUlEQVR4nO3c3Y6CMBBA4elm3/+Vu1dE40qkCMyZcs6l+xPTj6kS0NZ772GIfrKfgD0SA5QYoMQAJQYoMUCJAUoMUL/ZT2BLrbV/j814rupkgBIDlBigxAAlBigxQIkBSgxQYoASA5QYoMQAJQYoMUCJAQp3PePdtYtvfm+pwvUPDMbo4s5YKsaVAK01/HSkvWY4Cf9LmYwRiN778DXwtf9Pn47LJ2MUYk+9d/Sir3UpxhaIZSE/LebebY68PaLOM44+mqtNBwaj2sKdUTrGli3pG6hKyOkY9kgMUGKAEgOUGKDEAJWOcfYZMfmM+7V0DHuEwKh09J7ZpRhXnw1X+/gZYjIinI6IBIxPF4WOQqmIi5mM575dyLW/J29REUkYWxblyCmpUtpkbD1KR0GqTkVE8jZ19AJVn6T014wzr3dXKx1j6cxtpMIWFQHCiDjnFpsqEBEwjKU9CzjDVobEiKh1RB8VFiPiM8gyDTNMRQQcI+I+t+lEFMCIWF/U5fHXn1e917Zlfv121vZChSoxGUdHfY25JQY1MUDdEoP6mpH6AcuRRal2PXtPt5wMamKAKo1BfYu6t9IYsyUGqDIYs71zelcZjDskBigxQIkBSgxQ5TFmOvErjzFTYoAqhTH7iV8pjNkrh/HutpxZwnyv7UgzATxXbjJmTgxQYoASA5QYoMQAJQYoMUCJAUoMUGKAEgOUGKDEACUGKDFA/QFtqqeh6JGAqgAAAABJRU5ErkJggg==\" y=\"-10.536364\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 10.7 109.536364 \nL 10.7 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 109.536364 109.536364 \nL 109.536364 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 10.7 109.536364 \nL 109.536364 109.536364 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 10.7 10.7 \nL 109.536364 10.7 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 10.7 228.14 \nL 109.536364 228.14 \nL 109.536364 129.303636 \nL 10.7 129.303636 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p7737f7506b)\">\n    <image height=\"99\" id=\"image8c6de8027c\" transform=\"scale(1 -1)translate(0 -99)\" width=\"99\" x=\"10.7\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAGMAAABjCAYAAACPO76VAAADuUlEQVR4nO2c2W4sIQxEIcr///LchwhdQlhssHH1TB0pyqIJbSjMZtM5pfRKBIKvaAPIfygGEBQDCIoBBMUAgmIAQTGAoBhAUAwgKAYQFAMIigEExQCCYgBBMYCgGEBQDCAoBhAUA4jv14shcBToGUBQDCAoxoKc87VnUYzLzMQNFeNmr3sCXTFyzt2GQmy81iZEG6VMPaOuGGol32lp/h1twA45Z3MRep3tttBdMVojSuV3vcPSq1pbLBuslOUhtoRtz5A2xqxiJyIVQSwbri7LowOtmE7go8qWv0kecOJRkrKLPVZllZ/L7ztC79oz9IxZbz4x0Nr9o4eWETv2qPYZUm+w+j/tM1BWfLt1Fc8ZXj0vukev9ileHt215TV4Sm3UiSEe5WiIEFpia8+uoRjkPjwoBIJiAEExgKAYQFAMICgGEGoxRoEnco761FZz7NB+jluaOarjkJT0DTo7WT05Cll1iCcKLxJjN/zq1SAjEaPOuayO8MWe4RF0scZCiJ2htf7MSYcYBpfK97Zwj2ALisB10Kx8wSSxWbn8qhyE8R2hQ/waphAMimC0OLFoD03WyZ85o3VNS+/wSrE5GbNHMf6UbOquEViUqmOFlyC71HNj4SQRoS1XS0gS22nP85zLTjvLkYhtpE9iDFLvPmU3ROpBN+w621SlhLH68Saiww3njNEq4FNXXDdQJ7F9ChGLDWaHAMHgEhAUAwiKAQSMGFylAYmBRFTHgBDjnXb0hR1B/+wztIW8UyP2wss366e6YNkSOc5/REKCdOeJMLR4PL+u/+3O1p0zVoYgCLHiicl203jGKEUnSgjNqyl27x72Ak4WSOag6UFh7170qRCRwZuoZ0nbTpzEFhGGjKD1jpNopLYDL8WwmB9a79od8p7mFdoylmJYNkBvyfwkr/HmShJbTesViKuyKJuuH4d4JIq9C2Hvm/qk5AYp1z0DYb+CCmPgQEAcoZMfKAYQFAMIigFE+NK25ZPXE1fE6F1aRL2rIY1yenBFDLSGr1ltPjWRzxNUV49PQRZkZFMbXNu5KKqpc1cMz13y086ibt5/H66mvI670U5qZ7bs2lnaShvCXS5tkXuyt23aF9bULzGoL2qWIXrVoadzRu8askUI1so7dkKbWqzu/LXXo3v/N32v7SgZYUeQNhfJM3CFMgyu2qm1dzpneGSESFYuu9ycbC0Qe8avD3VWVxrvkIiKuuy9iWif0WskbVbHKkORXD6b6s1DKeGM8dFcjfRRgDlXPYMCzGE8A4h/wxmY3J4b7p4AAAAASUVORK5CYII=\" y=\"-129.14\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\"/>\n   <g id=\"matplotlib.axis_2\"/>\n   <g id=\"patch_8\">\n    <path d=\"M 10.7 228.14 \nL 10.7 129.303636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 109.536364 228.14 \nL 109.536364 129.303636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 10.7 228.14 \nL 109.536364 228.14 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 10.7 129.303636 \nL 109.536364 129.303636 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pc25741e13b\">\n   <rect height=\"98.836364\" width=\"98.836364\" x=\"10.7\" y=\"10.7\"/>\n  </clipPath>\n  <clipPath id=\"p7737f7506b\">\n   <rect height=\"98.836364\" width=\"98.836364\" x=\"10.7\" y=\"129.303636\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 62.489187 248.518125\" width=\"62.489187pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-01-24T15:35:41.073268</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 62.489187 248.518125 \nL 62.489187 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 224.64 \nL 47.337625 224.64 \nL 47.337625 7.2 \nL 36.465625 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pd4663d35d9)\">\n    <image height=\"218\" id=\"image6c53ff618a\" transform=\"scale(1 -1)translate(0 -218)\" width=\"11\" x=\"36.465625\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAAAsAAADaCAYAAABwzrisAAAAjElEQVR4nO3YsQ2AIBBGYcArmMHaARjA0sQFHNkB3MIZKABXuBAll/j++iteeTm/+aM55YIWgsFgMBgM/gpLXZOBDPAYLOG8DGSAu7G4MBnIAI/B4moxkAEGg8FgMFg5X+6Fv+hvsOxzMpABBoNfxxJiNJABHoOl5mwgA9yNxTX18WWlGQwGg8HgP+MHfwYQ8wlEo10AAAAASUVORK5CYII=\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"md18d4010b8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#md18d4010b8\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −0.5 -->\n      <g transform=\"translate(24.324219 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"41.901625\" xlink:href=\"#md18d4010b8\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.0 -->\n      <g transform=\"translate(33.950063 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"47.337625\" xlink:href=\"#md18d4010b8\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.5 -->\n      <g transform=\"translate(39.386063 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_4\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mf90581aedb\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mf90581aedb\" y=\"12.636\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.0 -->\n      <g transform=\"translate(13.5625 16.435219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mf90581aedb\" y=\"39.816\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 2.5 -->\n      <g transform=\"translate(13.5625 43.615219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mf90581aedb\" y=\"66.996\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 5.0 -->\n      <g transform=\"translate(13.5625 70.795219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mf90581aedb\" y=\"94.176\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 7.5 -->\n      <g transform=\"translate(13.5625 97.975219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mf90581aedb\" y=\"121.356\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 10.0 -->\n      <g transform=\"translate(7.2 125.155219)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mf90581aedb\" y=\"148.536\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 12.5 -->\n      <g transform=\"translate(7.2 152.335219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mf90581aedb\" y=\"175.716\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15.0 -->\n      <g transform=\"translate(7.2 179.515219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mf90581aedb\" y=\"202.896\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 17.5 -->\n      <g transform=\"translate(7.2 206.695219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 224.64 \nL 36.465625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 47.337625 224.64 \nL 47.337625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 224.64 \nL 47.337625 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 7.2 \nL 47.337625 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pd4663d35d9\">\n   <rect height=\"217.44\" width=\"10.872\" x=\"36.465625\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.plot(ways, val_accs, \"m\", label=\"Siamese(val set)\")\n",
    "ax.plot(ways, train_accs, \"y\", label=\"Siamese(train set)\")\n",
    "plt.plot(ways, nn_accs, label=\"Nearest neighbour\")\n",
    "\n",
    "ax.plot(ways, 100.0/ways, \"g\", label=\"Random guessing\")\n",
    "plt.xlabel(\"Number of possible classes in one-shot tasks\")\n",
    "plt.ylabel(\"% Accuracy\")\n",
    "plt.title(\"Omiglot One-Shot Learning Performance of a Siamese Network\")\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "inputs,targets = loader.make_oneshot_task(20, \"val\")\n",
    "plt.show()\n",
    "\n",
    "print(inputs[0].shape)\n",
    "\n",
    "plot_oneshot_task(inputs)\n",
    "p=model.predict(inputs)\n",
    "img = plt.imshow(p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tf-gpu",
   "display_name": "GPU",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}