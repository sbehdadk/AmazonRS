{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device mapping:\n/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8) #0.333\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True, gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x7f50f83b0be0>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from tensorflow.compat.v1.keras import backend as K\n",
    "import tensorflow as tf\n",
    "K.clear_session()\n",
    "tf.compat.v1.reset_default_graph()\n",
    "K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D,Activation, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras import optimizers\n",
    "#from skimage.io import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from keras import backend as k \n",
    "from tensorflow.python.keras.backend import set_session\n",
    "#from tf.compat.v1.keras.backend import set_session\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "from keras.applications import resnet50, vgg16, vgg19, xception, densenet, inception_v3, mobilenet, nasnet, inception_resnet_v2\n",
    "from tensorflow.compat.v1.keras.applications import MobileNetV2\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger, EarlyStopping\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "#from keras.applications.xception import preprocess_input\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and pre_process all Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "from random import shuffle\n",
    "dog_path = '/home/sina/Desktop/ACAdapters/*'\n",
    "cat_path = '/home/sina/Desktop/Adapters/*'\n",
    "addrsd = glob.glob(dog_path)\n",
    "addrsc = glob.glob(cat_path)\n",
    "    \n",
    "labelsd = [1 for addr in addrsd]  # 1 = dog, 0 =  cat\n",
    "labelsc = [0 for addr in addrsc]\n",
    "# loop over the input images\n",
    "datad = []\n",
    "for imagePath in addrsd:\n",
    "# load the image, pre-process it, and store it in the data list\n",
    "    img = cv2.imread(imagePath)\n",
    "    \n",
    "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    datad.append(img)\n",
    "datac = []\n",
    "for imagePath in addrsc:\n",
    "# load the image, pre-process it, and store it in the data list\n",
    "    img = cv2.imread(imagePath)\n",
    "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    datac.append(img)\n",
    "# to shuffle data\n",
    "shuffle_data = True\n",
    "if shuffle_data:\n",
    "    d = list(zip(datad, labelsd))\n",
    "    c = list(zip(datac, labelsc))\n",
    "    e = d + c\n",
    "    shuffle(e)\n",
    "    data, labels = zip(*e)\n",
    "del datad\n",
    "del datac\n",
    "del addrsd\n",
    "del addrsc\n",
    "    \n",
    "Y_train = np.array(labels)\n",
    "X_train = np.array(data, dtype=\"int8\")\n",
    "#preprocess for Resnet- 50\n",
    "X_train =  preprocess_input(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the architecture of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shape, dtype=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bias(shape, dtype=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two inputs one each - left and right image\n",
    "left_input = Input((224,224,3))\n",
    "right_input = Input((224,224,3))\n",
    "#Import Resnetarchitecture from keras application and initializing each layer with pretrained imagenet weights.\n",
    "'''Please note that it’s usually better to intialize the layers with imagenet initializations than random. While training I will be updating the weights for each layer in each epoch. we don’t want to confuse this activity with transfer learning as I am not freezing any layer but initilializing each layer with imagenet weights'''\n",
    "convnet = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "convnet.trainable = False\n",
    "\n",
    "# Add the final fully connected layers\n",
    "x = convnet.output\n",
    "x = Flatten()(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "preds = Dense(2, activation='sigmoid')(x) # Apply sigmoid\n",
    "convnet = Model(inputs=convnet.input, outputs=preds)\n",
    "#Applying above model for both the left and right images\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)\n",
    "# Euclidian Distance between the two images or encodings through the Resnet-50 architecture\n",
    "Euc_layer = Lambda(lambda tensor:K.abs(tensor[0] - tensor[1]))\n",
    "# use and add the distance function\n",
    "Euc_distance = Euc_layer([encoded_l, encoded_r])\n",
    "#identify the prediction\n",
    "prediction = Dense(1,activation='sigmoid')(Euc_distance)\n",
    "#Define the network with the left and right inputs and the ouput prediction\n",
    "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "#define the optimizer. Here I have used SGD with nesterov momentum\n",
    "for layer in convnet.layers[:100]:\n",
    "    layer.trainable = False\n",
    "for layer in convnet.layers[100:]:\n",
    "    layer.trainable=True\n",
    "optim = optimizers.SGD(lr=0.001, decay=.01, momentum=0.9, nesterov=True)\n",
    "#compile the network using binary cross entropy loss and the above optimizer\n",
    "\n",
    "siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optim,metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image pairs are created with 2 labels as 0 & 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test & train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D,Activation, Dropout, GlobalAveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.applications import resnet50, vgg16, vgg19, xception, densenet, inception_v3, mobilenet, nasnet, inception_resnet_v2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.compat.v1.keras.applications import MobileNetV2\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger, EarlyStopping\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras import optimizers\n",
    "\n",
    "def get_siamese_model(input_shape):\n",
    "    \"\"\"\n",
    "        Model architecture based on the one provided in: http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    '''left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)'''\n",
    "    \n",
    "    left_input = Input((224,224,3))\n",
    "    right_input = Input((224,224,3))\n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
    "                   kernel_initializer=initialize_weights, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation='relu',\n",
    "                     kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer=initialize_weights,\n",
    "                     bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
    "    \n",
    "    #model.add(GlobalAveragePooling2D())\n",
    "    #model.add(Activation('sigmoid', name='predictions'))\n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    optim = optimizers.SGD(lr=0.001, decay=.01, momentum=0.9, nesterov=True)\n",
    "    #compile the network using binary cross entropy loss and the above optimizer\n",
    "\n",
    "    siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optim,metrics=[\"accuracy\"])\n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_9 (InputLayer)            [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\ninput_10 (InputLayer)           [(None, 224, 224, 3) 0                                            \n__________________________________________________________________________________________________\nsequential_4 (Sequential)       (None, 1024)         106066240   input_9[0][0]                    \n                                                                 input_10[0][0]                   \n__________________________________________________________________________________________________\nlambda_2 (Lambda)               (None, 1024)         0           sequential_4[0][0]               \n                                                                 sequential_4[1][0]               \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 1)            1025        lambda_2[0][0]                   \n==================================================================================================\nTotal params: 106,067,265\nTrainable params: 106,067,265\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_net = get_siamese_model((224,224,3))\n",
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_list = X_train[:180]\n",
    "label_list = Y_train[:180]\n",
    "left_input = []\n",
    "right_input = []\n",
    "targets = []\n",
    "#Number of pairs per image\n",
    "pairs = 8\n",
    "#create the dataset to train on\n",
    "for i in range(len(label_list)):\n",
    "    for j in range(pairs):\n",
    "# we need to make sure that we are not comparing with the same image\n",
    "        compare_to = i\n",
    "        while compare_to == i: \n",
    "            compare_to = random.randint(0,179)\n",
    "        left_input.append(image_list[i])\n",
    "        right_input.append(image_list[compare_to])\n",
    "        if label_list[i] == label_list[compare_to]:\n",
    "            # if the images are same then label - 1\n",
    "            targets.append(1.)\n",
    "        else:\n",
    "            # if the images are different then label - 0\n",
    "            targets.append(0.)\n",
    "            \n",
    "#remove single-dimensional entries from the shape of the arrays and making them ready to create the train & datasets \n",
    " \n",
    "#the train data - left right images arrays and target label\n",
    "left_input = np.squeeze(np.array(left_input))\n",
    "right_input = np.squeeze(np.array(right_input))\n",
    "targets = np.squeeze(np.array(targets))\n",
    "# Creating test datasets - left, right images and target label\n",
    "dog_image = X_train[4] #dog_image = 1, cat_image = 0\n",
    "test_left = []\n",
    "test_right = []\n",
    "test_targets = []\n",
    "for i in range(len(Y_train)-180):\n",
    "    test_left.append(dog_image)\n",
    "    test_right.append(X_train[i+180])\n",
    "    test_targets.append(Y_train[i+180])\n",
    "test_left = np.squeeze(np.array(test_left))\n",
    "test_right = np.squeeze(np.array(test_right))\n",
    "test_targets = np.squeeze(np.array(test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the code using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'siamese_net' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d985e6f0bb34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#from keras_input_pipeline import *\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CUDA_VISIBLE_DEVICES'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msiamese_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gpu:1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     siamese_net.fit([left_input,right_input], targets,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'siamese_net' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "#from keras_input_pipeline import *\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "siamese_net.summary()\n",
    "with tf.device('/gpu:1'):\n",
    "    siamese_net.fit([left_input,right_input], targets,\n",
    "          batch_size=16,\n",
    "          epochs=30,\n",
    "          verbose=1,\n",
    "          validation_data=([test_left,test_right],test_targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sys import platform as sys_pf\n",
    "import matplotlib\n",
    "if sys_pf == 'darwin':\n",
    "\tmatplotlib.use(\"TkAgg\")\n",
    "from matplotlib import pyplot as plt\n",
    "# ---\n",
    "# Demo for how to load image and stroke data for a character\n",
    "# ---\n",
    "\n",
    "# Plot the motor trajectory over an image\n",
    "#\n",
    "# Input\n",
    "#  I [105 x 105 nump] grayscale image\n",
    "#  drawings: [ns list] of strokes (numpy arrays) in motor space\n",
    "#  lw : line width\n",
    "def plot_motor_to_image(I,drawing,lw=2):\n",
    "\tdrawing = [d[:,0:2] for d in drawing] # strip off the timing data (third column)\n",
    "\tdrawing = [space_motor_to_img(d) for d in drawing] # convert to image space\n",
    "\tplt.imshow(I,cmap='gray')\n",
    "\tns = len(drawing)\n",
    "\tfor sid in range(ns): # for each stroke\n",
    "\t\tplot_traj(drawing[sid],get_color(sid),lw)\n",
    "\tplt.xticks([])\n",
    "\tplt.yticks([])\n",
    "\n",
    "# Plot individual stroke\n",
    "#\n",
    "# Input\n",
    "#  stk: [n x 2] individual stroke\n",
    "#  color: stroke color\n",
    "#  lw: line width\n",
    "def plot_traj(stk,color,lw):\n",
    "\tn = stk.shape[0]\n",
    "\tif n > 1:\n",
    "\t\tplt.plot(stk[:,0],stk[:,1],color=color,linewidth=lw)\n",
    "\telse:\n",
    "\t\tplt.plot(stk[0,0],stk[0,1],color=color,linewidth=lw,marker='.')\n",
    "\n",
    "# Color map for the stroke of index k\n",
    "def get_color(k):\t\n",
    "    scol = ['r','g','b','m','c']\n",
    "    ncol = len(scol)\n",
    "    if k < ncol:\n",
    "       out = scol[k]\n",
    "    else:\n",
    "       out = scol[-1]\n",
    "    return out\n",
    "\n",
    "# convert to str and add leading zero to single digit numbers\n",
    "def num2str(idx):\n",
    "\tif idx < 10:\n",
    "\t\treturn '0'+str(idx)\n",
    "\treturn str(idx)\n",
    "\n",
    "# Load binary image for a character\n",
    "#\n",
    "# fn : filename\n",
    "def load_img(fn):\n",
    "\tI = plt.imread(fn)\n",
    "\tI = np.array(I,dtype=bool)\n",
    "\treturn I\n",
    "\n",
    "# Load stroke data for a character from text file\n",
    "#\n",
    "# Input\n",
    "#   fn : filename\n",
    "#\n",
    "# Output\n",
    "#   motor : list of strokes (each is a [n x 3] numpy array)\n",
    "#      first two columns are coordinates\n",
    "#\t   the last column is the timing data (in milliseconds)\n",
    "def load_motor(fn):\n",
    "\tmotor = []\n",
    "\twith open(fn,'r') as fid:\n",
    "\t\tlines = fid.readlines()\n",
    "\tlines = [l.strip() for l in lines]\n",
    "\tfor myline in lines:\n",
    "\t\tif myline =='START': # beginning of character\n",
    "\t\t\tstk = []\n",
    "\t\telif myline =='BREAK': # break between strokes\n",
    "\t\t\tstk = np.array(stk)\n",
    "\t\t\tmotor.append(stk) # add to list of strokes\n",
    "\t\t\tstk = [] \n",
    "\t\telse:\n",
    "\t\t\tarr = np.fromstring(myline,dtype=float,sep=',')\n",
    "\t\t\tstk.append(arr)\n",
    "\treturn motor\n",
    "\n",
    "#\n",
    "# Map from motor space to image space (or vice versa)\n",
    "#\n",
    "# Input\n",
    "#   pt: [n x 2] points (rows) in motor coordinates\n",
    "#\n",
    "# Output\n",
    "#  new_pt: [n x 2] points (rows) in image coordinates\n",
    "def space_motor_to_img(pt):\n",
    "\tpt[:,1] = -pt[:,1]\n",
    "\treturn pt\n",
    "def space_img_to_motor(pt):\n",
    "\tpt[:,1] = -pt[:,1]\n",
    "\treturn\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\timg_dir = 'images_background'\n",
    "\tstroke_dir = 'strokes_background'\n",
    "\tnreps = 20 # number of renditions for each character\n",
    "\tnalpha = 5 # number of alphabets to show\n",
    "\n",
    "\talphabet_names = [a for a in os.listdir(img_dir) if a[0] != '.'] # get folder names\n",
    "\talphabet_names = random.sample(alphabet_names,nalpha) # choose random alphabets\n",
    "\n",
    "\tfor a in range(nalpha): # for each alphabet\n",
    "\t\tprint('generating figure ' + str(a+1) + ' of ' + str(nalpha))\n",
    "\t\talpha_name = alphabet_names[a]\n",
    "\t\t\n",
    "\t\t# choose a random character from the alphabet\n",
    "\t\tcharacter_id = random.randint(1,len(os.listdir(os.path.join(img_dir,alpha_name))))\n",
    "\n",
    "\t\t# get image and stroke directories for this character\n",
    "\t\timg_char_dir = os.path.join(img_dir,alpha_name,'character'+num2str(character_id))\n",
    "\t\tstroke_char_dir = os.path.join(stroke_dir,alpha_name,'character'+num2str(character_id))\n",
    "\n",
    "\t\t# get base file name for this character\n",
    "\t\tfn_example = os.listdir(img_char_dir)[0]\n",
    "\t\tfn_base = fn_example[:fn_example.find('_')] \n",
    "\n",
    "\t\tplt.figure(a,figsize=(10,8))\n",
    "\t\tplt.clf()\n",
    "\t\tfor r in range(1,nreps+1): # for each rendition\n",
    "\t\t\tplt.subplot(4,5,r)\n",
    "\t\t\tfn_stk = stroke_char_dir + '/' + fn_base + '_' + num2str(r) + '.txt'\n",
    "\t\t\tfn_img = img_char_dir + '/' + fn_base + '_' + num2str(r) + '.png'\t\t\t\n",
    "\t\t\tmotor = load_motor(fn_stk)\n",
    "\t\t\tI = load_img(fn_img)\n",
    "\t\t\tplot_motor_to_image(I,motor)\n",
    "\t\t\tif r==1:\n",
    "\t\t\t\tplt.title(alpha_name[:15] + '\\n character ' + str(character_id))\n",
    "\t\tplt.tight_layout()\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tf-gpu",
   "display_name": "GPU",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}