{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, Lambda, merge, Dense, Flatten,MaxPooling2D,Activation, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras import optimizers\n",
    "#from skimage.io import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from keras import backend as k \n",
    "from tensorflow.python.keras.backend import set_session\n",
    "#from tf.compat.v1.keras.backend import set_session\n",
    "#from keras.backend.tensorflow_backend import set_session\n",
    "from keras.applications import resnet50, vgg16, vgg19, xception, densenet, inception_v3, mobilenet, nasnet, inception_resnet_v2\n",
    "from tensorflow.compat.v1.keras.applications import MobileNetV2\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger, EarlyStopping\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "#from keras.applications.xception import preprocess_input\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and pre_process all Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "from random import shuffle\n",
    "dog_path = '/home/sina/Desktop/Dataset/Electronics,Accessories&Supplies,Audio&VideoAccessories,Cables&Interconnects,VideoCables/Audio&VideoAccessories/DVICables/*'\n",
    "cat_path = '/home/sina/Desktop/Dataset/Electronics,Accessories&Supplies,Audio&VideoAccessories,Cables&Interconnects,VideoCables/Audio&VideoAccessories/CompositeVideo/*'\n",
    "addrsd = glob.glob(dog_path)\n",
    "addrsc = glob.glob(cat_path)\n",
    "    \n",
    "labelsd = [1 for addr in addrsd]  # 1 = dog, 0 =  cat\n",
    "labelsc = [0 for addr in addrsc]\n",
    "# loop over the input images\n",
    "datad = []\n",
    "for imagePath in addrsd:\n",
    "# load the image, pre-process it, and store it in the data list\n",
    "    img = cv2.imread(imagePath)\n",
    "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    datad.append(img)\n",
    "datac = []\n",
    "for imagePath in addrsc:\n",
    "# load the image, pre-process it, and store it in the data list\n",
    "    img = cv2.imread(imagePath)\n",
    "    img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    datac.append(img)\n",
    "# to shuffle data\n",
    "shuffle_data = True\n",
    "if shuffle_data:\n",
    "    d = list(zip(datad, labelsd))\n",
    "    c = list(zip(datac, labelsc))\n",
    "    e = d + c\n",
    "    shuffle(e)\n",
    "    data, labels = zip(*e)\n",
    "del datad\n",
    "del datac\n",
    "del addrsd\n",
    "del addrsc\n",
    "    \n",
    "Y_train = np.array(labels)\n",
    "X_train = np.array(data, dtype=\"int8\")\n",
    "#preprocess for Resnet- 50\n",
    "X_train =  preprocess_input(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the architecture of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two inputs one each - left and right image\n",
    "left_input = Input((224,224,3))\n",
    "right_input = Input((224,224,3))\n",
    "#Import Resnetarchitecture from keras application and initializing each layer with pretrained imagenet weights.\n",
    "'’'\n",
    "Please note that it’s usually better to intialize the layers with imagenet initializations than random. While training I will be updating the weights for each layer in each epoch. we don’t want to confuse this activity with transfer learning as I am not freezing any layer but initilializing each layer with imagenet weights\n",
    "'’'\n",
    "convnet = resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "# Add the final fully connected layers\n",
    "x = convnet.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "preds = Dense(18, activation='sigmoid')(x) # Apply sigmoid\n",
    "convnet = Model(inputs=convnet.input, outputs=preds)\n",
    "#Applying above model for both the left and right images\n",
    "encoded_l = convnet(left_input)\n",
    "encoded_r = convnet(right_input)\n",
    "# Euclidian Distance between the two images or encodings through the Resnet-50 architecture\n",
    "Euc_layer = Lambda(lambda tensor:K.abs(tensor[0] - tensor[1]))\n",
    "# use and add the distance function\n",
    "Euc_distance = Euc_layer([encoded_l, encoded_r])\n",
    "#identify the prediction\n",
    "prediction = Dense(1,activation='sigmoid')(Euc_distance)\n",
    "#Define the network with the left and right inputs and the ouput prediction\n",
    "siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "#define the optimizer. Here I have used SGD with nesterov momentum\n",
    "\n",
    "optim = optimizers.SGD(lr=0.001, decay=.01, momentum=0.9, nesterov=True)\n",
    "#compile the network using binary cross entropy loss and the above optimizer\n",
    "\n",
    "siamese_net.compile(loss=\"binary_crossentropy\",optimizer=optim,metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image pairs are created with 2 labels as 0 & 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test & train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_list = X_train[:180]\n",
    "label_list = Y_train[:180]\n",
    "left_input = []\n",
    "right_input = []\n",
    "targets = []\n",
    "#Number of pairs per image\n",
    "pairs = 8\n",
    "#create the dataset to train on\n",
    "for i in range(len(label_list)):\n",
    "    for j in range(pairs):\n",
    "# we need to make sure that we are not comparing with the same image\n",
    "        compare_to = i\n",
    "        while compare_to == i: \n",
    "            compare_to = random.randint(0,179)\n",
    "        left_input.append(image_list[i])\n",
    "        right_input.append(image_list[compare_to])\n",
    "        if label_list[i] == label_list[compare_to]:\n",
    "            # if the images are same then label - 1\n",
    "            targets.append(1.)\n",
    "        else:\n",
    "            # if the images are different then label - 0\n",
    "            targets.append(0.)\n",
    "            \n",
    "#remove single-dimensional entries from the shape of the arrays and making them ready to create the train & datasets \n",
    " \n",
    "#the train data - left right images arrays and target label\n",
    "left_input = np.squeeze(np.array(left_input))\n",
    "right_input = np.squeeze(np.array(right_input))\n",
    "targets = np.squeeze(np.array(targets))\n",
    "# Creating test datasets - left, right images and target label\n",
    "dog_image = X_train[4] #dog_image = 1, cat_image = 0\n",
    "test_left = []\n",
    "test_right = []\n",
    "test_targets = []\n",
    "for i in range(len(Y_train)-180):\n",
    "    test_left.append(dog_image)\n",
    "    test_right.append(X_train[i+180])\n",
    "    test_targets.append(Y_train[i+180])\n",
    "test_left = np.squeeze(np.array(test_left))\n",
    "test_right = np.squeeze(np.array(test_right))\n",
    "test_targets = np.squeeze(np.array(test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the code using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 18)           126367634   input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 18)           0           model_2[1][0]                    \n",
      "                                                                 model_2[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            19          lambda_1[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 126,367,653\n",
      "Trainable params: 126,314,533\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "90/90 [==============================] - 715s 8s/step - loss: 0.6970 - accuracy: 0.5625 - val_loss: 0.6661 - val_accuracy: 0.5910\n",
      "Epoch 2/30\n",
      "90/90 [==============================] - 694s 8s/step - loss: 0.5697 - accuracy: 0.7611 - val_loss: 0.6062 - val_accuracy: 0.7395\n",
      "Epoch 3/30\n",
      "90/90 [==============================] - 670s 7s/step - loss: 0.4780 - accuracy: 0.8861 - val_loss: 0.5420 - val_accuracy: 0.8263\n",
      "Epoch 4/30\n",
      "90/90 [==============================] - 665s 7s/step - loss: 0.4096 - accuracy: 0.9500 - val_loss: 0.5336 - val_accuracy: 0.8389\n",
      "Epoch 5/30\n",
      "90/90 [==============================] - 669s 7s/step - loss: 0.3888 - accuracy: 0.9639 - val_loss: 0.5262 - val_accuracy: 0.8417\n",
      "Epoch 6/30\n",
      "90/90 [==============================] - 671s 7s/step - loss: 0.3781 - accuracy: 0.9625 - val_loss: 0.5194 - val_accuracy: 0.8431\n",
      "Epoch 7/30\n",
      "90/90 [==============================] - 673s 7s/step - loss: 0.3666 - accuracy: 0.9646 - val_loss: 0.5145 - val_accuracy: 0.8389\n",
      "Epoch 8/30\n",
      "90/90 [==============================] - 673s 7s/step - loss: 0.3556 - accuracy: 0.9674 - val_loss: 0.5112 - val_accuracy: 0.8543\n",
      "Epoch 9/30\n",
      "90/90 [==============================] - 671s 7s/step - loss: 0.3481 - accuracy: 0.9694 - val_loss: 0.5094 - val_accuracy: 0.8473\n",
      "Epoch 10/30\n",
      "90/90 [==============================] - 669s 7s/step - loss: 0.3408 - accuracy: 0.9701 - val_loss: 0.5091 - val_accuracy: 0.8445\n",
      "Epoch 11/30\n",
      "90/90 [==============================] - 675s 8s/step - loss: 0.3359 - accuracy: 0.9736 - val_loss: 0.5066 - val_accuracy: 0.8473\n",
      "Epoch 12/30\n",
      "90/90 [==============================] - 729s 8s/step - loss: 0.3289 - accuracy: 0.9764 - val_loss: 0.5089 - val_accuracy: 0.8417\n",
      "Epoch 13/30\n",
      "90/90 [==============================] - 714s 8s/step - loss: 0.3238 - accuracy: 0.9750 - val_loss: 0.5125 - val_accuracy: 0.8333\n",
      "Epoch 14/30\n",
      "90/90 [==============================] - 735s 8s/step - loss: 0.3167 - accuracy: 0.9750 - val_loss: 0.5178 - val_accuracy: 0.8277\n",
      "Epoch 15/30\n",
      "90/90 [==============================] - 711s 8s/step - loss: 0.3064 - accuracy: 0.9764 - val_loss: 0.4783 - val_accuracy: 0.8263\n",
      "Epoch 16/30\n",
      "90/90 [==============================] - 721s 8s/step - loss: 0.2937 - accuracy: 0.9792 - val_loss: 0.4695 - val_accuracy: 0.8361\n",
      "Epoch 17/30\n",
      "90/90 [==============================] - 731s 8s/step - loss: 0.2834 - accuracy: 0.9819 - val_loss: 0.4640 - val_accuracy: 0.8403\n",
      "Epoch 18/30\n",
      "90/90 [==============================] - 707s 8s/step - loss: 0.2765 - accuracy: 0.9868 - val_loss: 0.4598 - val_accuracy: 0.8403\n",
      "Epoch 19/30\n",
      "90/90 [==============================] - 669s 7s/step - loss: 0.2703 - accuracy: 0.9854 - val_loss: 0.4563 - val_accuracy: 0.8403\n",
      "Epoch 20/30\n",
      "90/90 [==============================] - 681s 8s/step - loss: 0.2669 - accuracy: 0.9875 - val_loss: 0.4548 - val_accuracy: 0.8375\n",
      "Epoch 21/30\n",
      "90/90 [==============================] - 767s 9s/step - loss: 0.2635 - accuracy: 0.9882 - val_loss: 0.4539 - val_accuracy: 0.8389\n",
      "Epoch 22/30\n",
      "90/90 [==============================] - 756s 8s/step - loss: 0.2606 - accuracy: 0.9875 - val_loss: 0.4538 - val_accuracy: 0.8403\n",
      "Epoch 23/30\n",
      "90/90 [==============================] - 767s 9s/step - loss: 0.2566 - accuracy: 0.9903 - val_loss: 0.4534 - val_accuracy: 0.8445\n",
      "Epoch 24/30\n",
      "90/90 [==============================] - 724s 8s/step - loss: 0.2552 - accuracy: 0.9903 - val_loss: 0.4517 - val_accuracy: 0.8445\n",
      "Epoch 25/30\n",
      "90/90 [==============================] - 767s 9s/step - loss: 0.2529 - accuracy: 0.9910 - val_loss: 0.4499 - val_accuracy: 0.8417\n",
      "Epoch 26/30\n",
      "90/90 [==============================] - 731s 8s/step - loss: 0.2520 - accuracy: 0.9917 - val_loss: 0.4499 - val_accuracy: 0.8431\n",
      "Epoch 27/30\n",
      "90/90 [==============================] - 704s 8s/step - loss: 0.2494 - accuracy: 0.9903 - val_loss: 0.4489 - val_accuracy: 0.8431\n",
      "Epoch 28/30\n",
      "90/90 [==============================] - 703s 8s/step - loss: 0.2483 - accuracy: 0.9903 - val_loss: 0.4487 - val_accuracy: 0.8417\n",
      "Epoch 29/30\n",
      "90/90 [==============================] - 706s 8s/step - loss: 0.2470 - accuracy: 0.9889 - val_loss: 0.4482 - val_accuracy: 0.8417\n",
      "Epoch 30/30\n",
      "90/90 [==============================] - 689s 8s/step - loss: 0.2465 - accuracy: 0.9889 - val_loss: 0.4473 - val_accuracy: 0.8431\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "#from keras_input_pipeline import *\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "siamese_net.summary()\n",
    "with tf.device('/gpu:1'):\n",
    "    siamese_net.fit([left_input,right_input], targets,\n",
    "          batch_size=16,\n",
    "          epochs=30,\n",
    "          verbose=1,\n",
    "          validation_data=([test_left,test_right],test_targets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sys import platform as sys_pf\n",
    "import matplotlib\n",
    "if sys_pf == 'darwin':\n",
    "\tmatplotlib.use(\"TkAgg\")\n",
    "from matplotlib import pyplot as plt\n",
    "# ---\n",
    "# Demo for how to load image and stroke data for a character\n",
    "# ---\n",
    "\n",
    "# Plot the motor trajectory over an image\n",
    "#\n",
    "# Input\n",
    "#  I [105 x 105 nump] grayscale image\n",
    "#  drawings: [ns list] of strokes (numpy arrays) in motor space\n",
    "#  lw : line width\n",
    "def plot_motor_to_image(I,drawing,lw=2):\n",
    "\tdrawing = [d[:,0:2] for d in drawing] # strip off the timing data (third column)\n",
    "\tdrawing = [space_motor_to_img(d) for d in drawing] # convert to image space\n",
    "\tplt.imshow(I,cmap='gray')\n",
    "\tns = len(drawing)\n",
    "\tfor sid in range(ns): # for each stroke\n",
    "\t\tplot_traj(drawing[sid],get_color(sid),lw)\n",
    "\tplt.xticks([])\n",
    "\tplt.yticks([])\n",
    "\n",
    "# Plot individual stroke\n",
    "#\n",
    "# Input\n",
    "#  stk: [n x 2] individual stroke\n",
    "#  color: stroke color\n",
    "#  lw: line width\n",
    "def plot_traj(stk,color,lw):\n",
    "\tn = stk.shape[0]\n",
    "\tif n > 1:\n",
    "\t\tplt.plot(stk[:,0],stk[:,1],color=color,linewidth=lw)\n",
    "\telse:\n",
    "\t\tplt.plot(stk[0,0],stk[0,1],color=color,linewidth=lw,marker='.')\n",
    "\n",
    "# Color map for the stroke of index k\n",
    "def get_color(k):\t\n",
    "    scol = ['r','g','b','m','c']\n",
    "    ncol = len(scol)\n",
    "    if k < ncol:\n",
    "       out = scol[k]\n",
    "    else:\n",
    "       out = scol[-1]\n",
    "    return out\n",
    "\n",
    "# convert to str and add leading zero to single digit numbers\n",
    "def num2str(idx):\n",
    "\tif idx < 10:\n",
    "\t\treturn '0'+str(idx)\n",
    "\treturn str(idx)\n",
    "\n",
    "# Load binary image for a character\n",
    "#\n",
    "# fn : filename\n",
    "def load_img(fn):\n",
    "\tI = plt.imread(fn)\n",
    "\tI = np.array(I,dtype=bool)\n",
    "\treturn I\n",
    "\n",
    "# Load stroke data for a character from text file\n",
    "#\n",
    "# Input\n",
    "#   fn : filename\n",
    "#\n",
    "# Output\n",
    "#   motor : list of strokes (each is a [n x 3] numpy array)\n",
    "#      first two columns are coordinates\n",
    "#\t   the last column is the timing data (in milliseconds)\n",
    "def load_motor(fn):\n",
    "\tmotor = []\n",
    "\twith open(fn,'r') as fid:\n",
    "\t\tlines = fid.readlines()\n",
    "\tlines = [l.strip() for l in lines]\n",
    "\tfor myline in lines:\n",
    "\t\tif myline =='START': # beginning of character\n",
    "\t\t\tstk = []\n",
    "\t\telif myline =='BREAK': # break between strokes\n",
    "\t\t\tstk = np.array(stk)\n",
    "\t\t\tmotor.append(stk) # add to list of strokes\n",
    "\t\t\tstk = [] \n",
    "\t\telse:\n",
    "\t\t\tarr = np.fromstring(myline,dtype=float,sep=',')\n",
    "\t\t\tstk.append(arr)\n",
    "\treturn motor\n",
    "\n",
    "#\n",
    "# Map from motor space to image space (or vice versa)\n",
    "#\n",
    "# Input\n",
    "#   pt: [n x 2] points (rows) in motor coordinates\n",
    "#\n",
    "# Output\n",
    "#  new_pt: [n x 2] points (rows) in image coordinates\n",
    "def space_motor_to_img(pt):\n",
    "\tpt[:,1] = -pt[:,1]\n",
    "\treturn pt\n",
    "def space_img_to_motor(pt):\n",
    "\tpt[:,1] = -pt[:,1]\n",
    "\treturn\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\timg_dir = 'images_background'\n",
    "\tstroke_dir = 'strokes_background'\n",
    "\tnreps = 20 # number of renditions for each character\n",
    "\tnalpha = 5 # number of alphabets to show\n",
    "\n",
    "\talphabet_names = [a for a in os.listdir(img_dir) if a[0] != '.'] # get folder names\n",
    "\talphabet_names = random.sample(alphabet_names,nalpha) # choose random alphabets\n",
    "\n",
    "\tfor a in range(nalpha): # for each alphabet\n",
    "\t\tprint('generating figure ' + str(a+1) + ' of ' + str(nalpha))\n",
    "\t\talpha_name = alphabet_names[a]\n",
    "\t\t\n",
    "\t\t# choose a random character from the alphabet\n",
    "\t\tcharacter_id = random.randint(1,len(os.listdir(os.path.join(img_dir,alpha_name))))\n",
    "\n",
    "\t\t# get image and stroke directories for this character\n",
    "\t\timg_char_dir = os.path.join(img_dir,alpha_name,'character'+num2str(character_id))\n",
    "\t\tstroke_char_dir = os.path.join(stroke_dir,alpha_name,'character'+num2str(character_id))\n",
    "\n",
    "\t\t# get base file name for this character\n",
    "\t\tfn_example = os.listdir(img_char_dir)[0]\n",
    "\t\tfn_base = fn_example[:fn_example.find('_')] \n",
    "\n",
    "\t\tplt.figure(a,figsize=(10,8))\n",
    "\t\tplt.clf()\n",
    "\t\tfor r in range(1,nreps+1): # for each rendition\n",
    "\t\t\tplt.subplot(4,5,r)\n",
    "\t\t\tfn_stk = stroke_char_dir + '/' + fn_base + '_' + num2str(r) + '.txt'\n",
    "\t\t\tfn_img = img_char_dir + '/' + fn_base + '_' + num2str(r) + '.png'\t\t\t\n",
    "\t\t\tmotor = load_motor(fn_stk)\n",
    "\t\t\tI = load_img(fn_img)\n",
    "\t\t\tplot_motor_to_image(I,motor)\n",
    "\t\t\tif r==1:\n",
    "\t\t\t\tplt.title(alpha_name[:15] + '\\n character ' + str(character_id))\n",
    "\t\tplt.tight_layout()\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "rs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
